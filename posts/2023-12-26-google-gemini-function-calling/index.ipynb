{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Google Gemini and Function Calling\n",
        "author: Lawrence Wu\n",
        "date: '2023-12-26'\n",
        "date-modified: '2023-12-26'\n",
        "categories:\n",
        "  - LLMs\n",
        "  - Google\n",
        "  - Gemini\n",
        "  - Function Calling\n",
        "format:\n",
        "  html:\n",
        "    code-overflow: wrap\n",
        "execute:\n",
        "  freeze: true\n",
        "---"
      ],
      "id": "331fe3fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Google's latest LLM called Gemini was released in December. Google trained three model sizes: Nano, Gemini and Ultra (small, medium and large respectively). Ultra hasn't been released publicly yet. There are reports that Gemini has similar performance to `gpt-3.5-turbo`. Based on lmsys' [Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard), Gemini Pro's Elo is `1111` while `gpt-3.5-turbo-0613` is a tick higher at `1117` (as of this writing 2023-12-26).\n",
        "\n",
        "![Chatbot Arena Leaderboard as of 2023-12-26](./chatbot_arena_leaderboard_20231226.png)\n",
        "\n",
        "# How to Run Gemini Pro with langchain\n",
        "\n",
        "You'll need two Python libraries:\n",
        "```bash\n",
        "google-cloud-aiplatform>=1.38.0\n",
        "langchain>=0.0.350\n",
        "```\n",
        "\n",
        "With those installed, you can run:\n",
        "\n",
        "```python\n",
        "from langchain.llms import VertexAI\n",
        " \n",
        "llm_gemini_pro = VertexAI(\n",
        "    model_name=\"gemini-pro\",\n",
        "    temperature=0.7,\n",
        "    max_output_tokens=1024,\n",
        ")\n",
        " \n",
        "llm_gemini_pro.invoke(\"What's the difference between data science and data engineering?\")\n",
        "\n",
        "# the answer is pretty good\n",
        "# Data science and data engineering are two closely related but distinct fields that play crucial roles in extracting insights from data. While both involve working with data, they have different objectives, skill sets, and responsibilities. Here are the key differences between data science and data engineering:\n",
        "\n",
        "# 1. **Objective**:\n",
        "#    - Data Science: The primary objective of data science is to extract insights from data to make informed decisions. Data scientists use statistical and machine learning techniques to uncover patterns, predict outcomes, and develop models to solve business problems.\n",
        "#    - Data Engineering: Data engineers focus on building and maintaining the infrastructure that enables data scientists to access, process, and analyze data effectively. They ensure that data is collected, cleaned, transformed, and stored in a way that makes it accessible and usable for data analysis.\n",
        "\n",
        "# 2. **Skill Sets**:\n",
        "#    - Data Science: Data scientists typically have a strong background in mathematics, statistics, computer science, and programming languages like Python and R. They are proficient in data mining, machine learning algorithms, and statistical modeling.\n",
        "#    - Data Engineering: Data engineers possess expertise in database management systems, data integration tools, cloud computing platforms, and distributed systems. They are skilled in data modeling, data warehousing, and data pipelines.\n",
        "\n",
        "# 3. **Responsibilities**:\n",
        "#    - Data Science:\n",
        "#      - Develop and implement machine learning models for predictive analytics, classification, and regression.\n",
        "#      - Analyze large datasets to identify patterns, trends, and anomalies.\n",
        "#      - Build data visualization dashboards and reports to communicate insights to stakeholders.\n",
        "#      - Collaborate with business teams to understand their needs and translate them into actionable insights.\n",
        "#    - Data Engineering:\n",
        "#      - Design and implement data pipelines for data ingestion, transformation, and storage.\n",
        "#      - Ensure data quality and consistency by cleaning, validating, and normalizing data.\n",
        "#      - Develop and maintain data warehouses and data lakes for efficient data storage and retrieval.\n",
        "#      - Collaborate with data scientists to provide them with the necessary data and infrastructure for analysis.\n",
        "\n",
        "# 4. **Tools and Technologies**:\n",
        "#    - Data Science: Data scientists commonly use tools like Python, R, Jupyter Notebooks, Pandas, NumPy, Scikit-learn, and TensorFlow for data analysis and machine learning.\n",
        "#    - Data Engineering: Data engineers work with tools such as Apache Hadoop, Apache Spark, Apache Hive, Apache Kafka, SQL databases, NoSQL databases, and cloud-based data storage services like Amazon S3 and Google Cloud Storage.\n",
        "# ...\n",
        "#    - Data Science: Data scientists can progress in their careers by becoming senior data scientists, data science managers, or even chief data scientists. They may also transition into roles such as machine learning engineers or research scientists.\n",
        "#    - Data Engineering: Data engineers can advance to become senior data engineers, data architects, or big data engineers. They may also move into roles in cloud engineering, data governance, or data security.\n",
        "\n",
        "# In summary, data science focuses on extracting insights from data to make informed decisions, while data engineering focuses on building and maintaining the infrastructure that enables data analysis. Both fields are crucial for organizations to effectively manage and utilize their data, and they often work together to deliver data-driven solutions for businesses.\n",
        "```\n",
        "\n",
        "# Gemini Pro and Function Calling\n",
        "\n",
        "- My learnings are only from text based use cases as I haven't really tried the multi-modal capabilities yet (e.g. vision)\n",
        "- My \"vibe check\" of Gemini Pro is it is a much improved model compared to `text-bison` and `text-unicorn`. It is able to follow instructions much more reliably than the previous models that were based on PaLM2. \n",
        "- One feature that I was particularly excited to see Google add is [function calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling).\n",
        "- Here are some [function calling examples](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling#function-calling-sdk-samples) from Google's documentation. \n",
        "- This [intro to function calling Jupyter notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb) was very helpful to get started with function calling.\n",
        "- As of this writing (2023-12-26), langchain doesn't support Google Gemini's function calling. I opened an [issue](https://github.com/langchain-ai/langchain/issues/15073) and it looks like there's already a [PR](https://github.com/langchain-ai/langchain/pull/15146) to add this feature.\n",
        "\n",
        "# Function Calling Basic Usage\n",
        "\n",
        "To use function calling with Gemini, you can first define a function:\n",
        "```python\n",
        "from vertexai.preview import generative_models\n",
        "from vertexai.preview.generative_models import GenerativeModel\n",
        "model = GenerativeModel(\"gemini-pro\")\n",
        "\n",
        "get_current_weather_func = generative_models.FunctionDeclaration(\n",
        "  name=\"get_current_weather\",\n",
        "  description=\"Get the current weather in a given location\",\n",
        "  parameters={\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "          \"location\": {\n",
        "              \"type\": \"string\",\n",
        "              \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "          },\n",
        "          \"unit\": {\n",
        "              \"type\": \"string\",\n",
        "              \"enum\": [\n",
        "                  \"celsius\",\n",
        "                  \"fahrenheit\",\n",
        "              ]\n",
        "          }\n",
        "      },\n",
        "      \"required\": [\n",
        "          \"location\"\n",
        "      ]\n",
        "  },\n",
        ")\n",
        "```\n",
        "\n",
        "Then you create a tool with that function\n",
        "```python\n",
        "weather_tool = generative_models.Tool(\n",
        "  function_declarations=[get_current_weather_func]\n",
        ")\n",
        "```\n",
        "\n",
        "Then when you call the LLM, you pass the tool to the call:\n",
        "```python\n",
        "model_response = model.generate_content(\n",
        "    \"What is the weather like in Boston?\",\n",
        "    generation_config={\"temperature\": 0},\n",
        "    tools=[weather_tool],\n",
        ")\n",
        "\n",
        "print(\"model_response\\n\", model_response)\n",
        "```\n",
        "\n",
        "The model will output something like this:\n",
        "```python\n",
        "candidates {\n",
        "  content {\n",
        "    role: \"model\"\n",
        "    parts {\n",
        "      function_call {\n",
        "        name: \"get_current_weather\"\n",
        "        args {\n",
        "          fields {\n",
        "            key: \"location\"\n",
        "            value {\n",
        "              string_value: \"Boston, MA\"\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  finish_reason: STOP\n",
        "  safety_ratings {\n",
        "    category: HARM_CATEGORY_HARASSMENT\n",
        "    probability: NEGLIGIBLE\n",
        "  }\n",
        "  safety_ratings {\n",
        "    category: HARM_CATEGORY_HATE_SPEECH\n",
        "    probability: NEGLIGIBLE\n",
        "  }\n",
        "  safety_ratings {\n",
        "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
        "    probability: NEGLIGIBLE\n",
        "  }\n",
        "  safety_ratings {\n",
        "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
        "    probability: NEGLIGIBLE\n",
        "  }\n",
        "}\n",
        "usage_metadata {\n",
        "  prompt_token_count: 8\n",
        "  total_token_count: 8\n",
        "}\n",
        "```\n",
        "\n",
        "# Function Calling & Data Extraction\n",
        "\n",
        "One very useful use-case for function calling is robust semantic data extraction. Without LLM's that support function calling, you can already do semantic data extraction. With a prompt like \"extract name, age and hair color from this block of text and return the data in JSON\" and a large enough LLM, this will already get you pretty far. This is an [example](https://python.langchain.com/docs/modules/model_io/output_parsers/) with gpt-3.5, langchain and one of langchain's output parsers `PydanticOutputParser`. However, some issues are:\n",
        "\n",
        "- the LLM may not always return valid JSON\n",
        "- the LLM may return keys that are named differently\n",
        "= the LLM may not return all the keys\n",
        "\n",
        "The reason I say function calling supports **robust** semantic data extraction is the LLM will always return valid JSON and a schema that matches the schema you have defined.\n",
        "\n",
        "# Data Extraction Example\n",
        "I modified [langchain's Function Calling Extraction use case](https://python.langchain.com/docs/use_cases/extraction) which uses OpenAI to work with Gemini Pro.\n",
        "\n",
        "First do the necessary imports:\n",
        "```python\n",
        "from vertexai.preview.generative_models import (\n",
        "    FunctionDeclaration,\n",
        "    GenerativeModel,\n",
        "    Tool,\n",
        ")\n",
        "```\n",
        "\n",
        "Then define your function and tool:\n",
        "```python\n",
        "func_person_extractor = FunctionDeclaration(\n",
        "    name=\"person_extractor\",\n",
        "    description=\"Extract data about a person from the text\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"name\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Name of a person\",\n",
        "            },\n",
        "            \"height\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"Height of a person\",\n",
        "            },\n",
        "            \"hair_color\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Hair Color\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"name\", \"height\"]\n",
        "    },\n",
        "    \n",
        ")\n",
        "tool_person_extractor = Tool(\n",
        "    function_declarations=[func_person_extractor],\n",
        ")\n",
        "```\n",
        "\n",
        "Instantiate the gemini-pro model, pass the tool to the model and call the model with a prompt:\n",
        "```python\n",
        "\n",
        "# have to use the model directly from vertexai since function calling not available through langchain\n",
        "model = GenerativeModel(\"gemini-pro\")\n",
        "person_example_1 = model.generate_content(\n",
        "    \"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\",\n",
        "    generation_config={\"temperature\": 0},\n",
        "    tools=[tool_person_extractor],\n",
        ")\n",
        "print(person_example_1)\n",
        "\n",
        "# candidates {\n",
        "#   content {\n",
        "#     role: \"model\"\n",
        "#     parts {\n",
        "#       function_call {\n",
        "#         name: \"person_extractor\"\n",
        "#         args {\n",
        "#           fields {\n",
        "#             key: \"name\"\n",
        "#             value {\n",
        "#               string_value: \"Alex\"\n",
        "#             }\n",
        "#           }\n",
        "#           fields {\n",
        "#             key: \"height\"\n",
        "#             value {\n",
        "#               number_value: 5\n",
        "#             }\n",
        "#           }\n",
        "#           fields {\n",
        "#             key: \"hair_color\"\n",
        "#             value {\n",
        "#               string_value: \"blonde\"\n",
        "#             }\n",
        "#           }\n",
        "# ...\n",
        "#   prompt_token_count: 29\n",
        "#   total_token_count: 29\n",
        "# }\n",
        "```"
      ],
      "id": "3ddd87ba"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}