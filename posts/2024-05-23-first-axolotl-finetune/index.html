<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lawrence Wu">
<meta name="dcterms.date" content="2024-05-23">

<title>Finetuning LLMs with Axolotl – Lawrence Wu</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-8647a4a42273f773479d27c00df3f9ed.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-LN4GM4FVCJ"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-LN4GM4FVCJ', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  
<script>
  // Log script loading attempt
  console.log('Loading GitHub stars script');
  
  // Simple script loading for main website
  const scriptTag = document.createElement('script');
  scriptTag.src = '/js/github-stars.js';
  scriptTag.async = false;
  scriptTag.defer = true;
  scriptTag.onload = () => console.log('GitHub stars script loaded successfully');
  scriptTag.onerror = (err) => console.error('Error loading GitHub stars script:', err);
  
  // Append to document head
  document.head.appendChild(scriptTag);
</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Finetuning LLMs with Axolotl – Lawrence Wu">
<meta property="og:description" content="This is Lawrence Wu’s personal website">
<meta property="og:site_name" content="Lawrence Wu">
<meta name="twitter:title" content="Finetuning LLMs with Axolotl – Lawrence Wu">
<meta name="twitter:description" content="This is Lawrence Wu’s personal website">
<meta name="twitter:creator" content="@law_wu">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Lawrence Wu</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../conferences.html"> 
<span class="menu-text">Conferences</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://lawwu.github.io/til/"> 
<span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/lawwu"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text">GitHub</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://twitter.com/law_wu"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text">Twitter</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:lawrencewu1+blog@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#fine-tuning-with-a-smaller-sample" id="toc-fine-tuning-with-a-smaller-sample" class="nav-link active" data-scroll-target="#fine-tuning-with-a-smaller-sample">Fine-Tuning with a Smaller Sample</a></li>
  <li><a href="#some-things-i-learned" id="toc-some-things-i-learned" class="nav-link" data-scroll-target="#some-things-i-learned">Some things I learned</a>
  <ul class="collapse">
  <li><a href="#runtimeerror-_amp_foreach_non_finite_check_and_unscale_cuda-not-implemented-for-bfloat16" id="toc-runtimeerror-_amp_foreach_non_finite_check_and_unscale_cuda-not-implemented-for-bfloat16" class="nav-link" data-scroll-target="#runtimeerror-_amp_foreach_non_finite_check_and_unscale_cuda-not-implemented-for-bfloat16">RuntimeError: “_amp_foreach_non_finite_check_and_unscale_cuda” not implemented for ‘BFloat16’</a></li>
  <li><a href="#running-out-of-gpu-memory" id="toc-running-out-of-gpu-memory" class="nav-link" data-scroll-target="#running-out-of-gpu-memory">Running out of GPU memory</a></li>
  </ul></li>
  <li><a href="#fine-tuning-with-the-full-dataset" id="toc-fine-tuning-with-the-full-dataset" class="nav-link" data-scroll-target="#fine-tuning-with-the-full-dataset">Fine-Tuning with the full dataset</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Finetuning LLMs with Axolotl</h1>
  <div class="quarto-categories">
    <div class="quarto-category">LLMs</div>
    <div class="quarto-category">Fine-tune</div>
    <div class="quarto-category">Axolotl</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Lawrence Wu </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 23, 2024</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">May 23, 2024</p>
    </div>
  </div>
    
  </div>
  


</header>


<p>I started Hamel Husain’s <del>fine-tuning LLM course</del> Mastering LLM course last week. I don’t have a ton of experience fine-tuning LLMs so I thought this would be a good way to learn.</p>
<p>One of the examples he is using throughout the course is fine-tuning an LLM to generate Honeycomb queries. So you can turn natural language into a domain specific language. My goal was to reproduce the model he trained <a href="https://huggingface.co/parlance-labs/hc-mistral-alpaca">here</a>. Here are the steps I took to reproduce what Hamel did:</p>
<p>The class gave us $200 of Jarvislabs credits so I spun up a VM using the <a href="https://jarvislabs.ai/templates/axolotl">Axolotl template</a>. I picked an <del>RTX5000 with 16GB VRAM</del> 1x A100 with 100GB of disk space. The default 20GB of disk space is not enough as the base models take 5-10GB of space each.</p>
<p>I cloned the repo:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> lfs install</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://huggingface.co/parlance-labs/hc-mistral-alpaca</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I logged into Weights and Biases:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install wandb</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb</span> login</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># paste your api key from https://wandb.ai/home</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I logged into Huggingface. Make sure your token has <code>WRITE</code> access:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-U</span> <span class="st">"huggingface_hub[cli]"</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">huggingface-cli</span> login</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># paste your huggingface token from https://huggingface.co/settings/tokens</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="fine-tuning-with-a-smaller-sample" class="level1">
<h1>Fine-Tuning with a Smaller Sample</h1>
<p>I sampled 100 rows of his training data to make the first fine-tune go faster. The uploaded model to huggingface is <a href="https://huggingface.co/lawrencewu/hc-mistral-7B-v0.3-alpaca-first-100">here</a>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> read_jsonl(file_path):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> []</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> line <span class="kw">in</span> <span class="bu">file</span>:</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            data.append(json.loads(line.strip()))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> write_jsonl(data, file_path):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'w'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> entry <span class="kw">in</span> data:</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>            <span class="bu">file</span>.write(json.dumps(entry) <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Path to the input JSONL file</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>input_file_path <span class="op">=</span> <span class="st">'./data/alpaca_synth_queries_healed.jsonl'</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Path to the output JSONL file</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>output_file_path <span class="op">=</span> <span class="st">'./data/output_first_100.jsonl'</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the data from the input file</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> read_jsonl(input_file_path)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the first 100 rows</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>first_100_rows <span class="op">=</span> data[:<span class="dv">100</span>]</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Write the first 100 rows to the output file</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>write_jsonl(first_100_rows, output_file_path)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First 100 rows have been written to </span><span class="sc">{</span>output_file_path<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is the Axolotl config file I wound up with below. Some changes I made: - updated the base model to <code>mistralai/Mistral-7B-v0.3</code> - used a smaller dataset <code>data/output_first_100.jsonl</code> - updated <code>hub_model_id</code> and <code>wandb_project</code> and <code>wandb_entity</code></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">base_model</span><span class="kw">:</span><span class="at"> mistralai/Mistral-7B-v0.3</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">model_type</span><span class="kw">:</span><span class="at"> MistralForCausalLM</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">tokenizer_type</span><span class="kw">:</span><span class="at"> LlamaTokenizer</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">is_mistral_derived_model</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="fu">load_in_8bit</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="fu">load_in_4bit</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="fu">strict</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_fan_in_fan_out</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="fu">data_seed</span><span class="kw">:</span><span class="at"> </span><span class="dv">49</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="fu">seed</span><span class="kw">:</span><span class="at"> </span><span class="dv">49</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="fu">datasets</span><span class="kw">:</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> data/output_first_100.jsonl</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> sharegpt</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">conversation</span><span class="kw">:</span><span class="at"> alpaca</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="fu">dataset_prepared_path</span><span class="kw">:</span><span class="at"> last_run_prepared</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="fu">val_set_size</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.1</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="fu">output_dir</span><span class="kw">:</span><span class="at"> ./qlora-alpaca-out</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="fu">hub_model_id</span><span class="kw">:</span><span class="at"> lawrencewu/hc-mistral-7B-v0.3-alpaca-first-100</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="fu">adapter</span><span class="kw">:</span><span class="at"> qlora</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_model_dir</span><span class="kw">:</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="fu">sequence_len</span><span class="kw">:</span><span class="at"> </span><span class="dv">896</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="fu">sample_packing</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="fu">pad_to_sequence_len</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_r</span><span class="kw">:</span><span class="at"> </span><span class="dv">32</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_alpha</span><span class="kw">:</span><span class="at"> </span><span class="dv">16</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_dropout</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.05</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_target_linear</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_fan_in_fan_out</span><span class="kw">:</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_target_modules</span><span class="kw">:</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> gate_proj</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> down_proj</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> up_proj</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> q_proj</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> v_proj</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> k_proj</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> o_proj</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="fu">wandb_project</span><span class="kw">:</span><span class="at"> hc-axolotl-mistral</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="fu">wandb_entity</span><span class="kw">:</span><span class="at"> law</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="fu">gradient_accumulation_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">4</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="fu">micro_batch_size</span><span class="kw">:</span><span class="at"> </span><span class="dv">16</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a><span class="fu">eval_batch_size</span><span class="kw">:</span><span class="at"> </span><span class="dv">16</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a><span class="fu">num_epochs</span><span class="kw">:</span><span class="at"> </span><span class="dv">3</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="fu">optimizer</span><span class="kw">:</span><span class="at"> adamw_bnb_8bit</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a><span class="fu">lr_scheduler</span><span class="kw">:</span><span class="at"> cosine</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a><span class="fu">learning_rate</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0002</span></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a><span class="fu">max_grad_norm</span><span class="kw">:</span><span class="at"> </span><span class="fl">1.0</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a><span class="fu">adam_beta2</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.95</span></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a><span class="fu">adam_epsilon</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.00001</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a><span class="fu">save_total_limit</span><span class="kw">:</span><span class="at"> </span><span class="dv">12</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a><span class="fu">train_on_inputs</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a><span class="fu">group_by_length</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a><span class="fu">bf16</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="fu">fp16</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a><span class="fu">tf32</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a><span class="fu">gradient_checkpointing</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a><span class="fu">early_stopping_patience</span><span class="kw">:</span></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a><span class="fu">resume_from_checkpoint</span><span class="kw">:</span></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a><span class="fu">local_rank</span><span class="kw">:</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a><span class="fu">logging_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a><span class="fu">xformers_attention</span><span class="kw">:</span></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a><span class="fu">flash_attention</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a><span class="fu">loss_watchdog_threshold</span><span class="kw">:</span><span class="at"> </span><span class="fl">5.0</span></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a><span class="fu">loss_watchdog_patience</span><span class="kw">:</span><span class="at"> </span><span class="dv">3</span></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a><span class="fu">warmup_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">20</span></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a><span class="fu">evals_per_epoch</span><span class="kw">:</span><span class="at"> </span><span class="dv">4</span></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a><span class="fu">eval_table_size</span><span class="kw">:</span></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a><span class="fu">eval_table_max_new_tokens</span><span class="kw">:</span><span class="at"> </span><span class="dv">128</span></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a><span class="fu">saves_per_epoch</span><span class="kw">:</span><span class="at"> </span><span class="dv">6</span></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a><span class="fu">debug</span><span class="kw">:</span></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a><span class="fu">weight_decay</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0</span></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a><span class="fu">fsdp</span><span class="kw">:</span></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a><span class="fu">fsdp_config</span><span class="kw">:</span></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a><span class="fu">special_tokens</span><span class="kw">:</span></span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">bos_token</span><span class="kw">:</span><span class="at"> </span><span class="st">"&lt;s&gt;"</span></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">eos_token</span><span class="kw">:</span><span class="at"> </span><span class="st">"&lt;/s&gt;"</span></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">unk_token</span><span class="kw">:</span><span class="at"> </span><span class="st">"&lt;unk&gt;"</span></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a><span class="fu">save_safetensors</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I launched the training script:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">accelerate</span> launch <span class="at">-m</span> axolotl.cli.train hc-first-100.yml </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Weighs and biases provides a nice summary of the run too:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> / 0.123 MB of 0.123 MB uploaded</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> Run history:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>               eval/loss █▇▁</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>            eval/runtime ▁▅█</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> eval/samples_per_second █▄▁</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>   eval/steps_per_second █▄▁</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>             train/epoch ▁▁▅▅███</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>       train/global_step ▁▁▅▅███</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>         train/grad_norm ██▁</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>     train/learning_rate ▁▅█</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>              train/loss █▁▅</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> Run summary:</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>                eval/loss 1.08833</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>             eval/runtime 1.0702</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>  eval/samples_per_second 9.344</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>    eval/steps_per_second 0.934</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>               total_flos 6965062501662720.0</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>              train/epoch 2.0</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>        train/global_step 3</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>          train/grad_norm 2.29688</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>      train/learning_rate 3e-05</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>               train/loss 1.2203</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>               train_loss 1.22012</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>            train_runtime 70.8206</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> train_samples_per_second 3.812</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span>   train_steps_per_second 0.042</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> </span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> 🚀 View run scarlet-lake-4 at: https://wandb.ai/law/hc-axolotl-mistral/runs/wrnox7vk</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> ⭐️ View project at: https://wandb.ai/law/hc-axolotl-mistral</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> Synced 6 W<span class="kw">&amp;</span><span class="ex">B</span> file<span class="er">(</span><span class="ex">s</span><span class="kw">)</span><span class="ex">,</span> 0 media file<span class="er">(</span><span class="ex">s</span><span class="kw">)</span><span class="ex">,</span> 1 artifact file<span class="er">(</span><span class="ex">s</span><span class="kw">)</span> <span class="ex">and</span> 1 other file<span class="er">(</span><span class="ex">s</span><span class="kw">)</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> Find logs at: ./wandb/run-20240523_235927-wrnox7vk/logs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="some-things-i-learned" class="level1">
<h1>Some things I learned</h1>
<section id="runtimeerror-_amp_foreach_non_finite_check_and_unscale_cuda-not-implemented-for-bfloat16" class="level2">
<h2 class="anchored" data-anchor-id="runtimeerror-_amp_foreach_non_finite_check_and_unscale_cuda-not-implemented-for-bfloat16">RuntimeError: “_amp_foreach_non_finite_check_and_unscale_cuda” not implemented for ‘BFloat16’</h2>
<p>For one run I got this error:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">iciency_estimate:</span> 0.96 total_num_tokens per device: 414041</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Traceback</span> <span class="er">(</span><span class="ex">most</span> recent call last<span class="kw">)</span><span class="bu">:</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/runpy.py"</span>, line 196, in _run_module_as_main</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ex">_run_code</span><span class="er">(</span><span class="ex">code,</span> main_globals, None,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/runpy.py"</span>, line 86, in _run_code</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">exec</span><span class="er">(</span><span class="ex">code,</span> run_globals<span class="kw">)</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/workspace/axolotl/src/axolotl/cli/train.py"</span>, line 70, in <span class="op">&lt;</span>module<span class="op">&gt;</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="ex">fire.Fire</span><span class="er">(</span><span class="ex">do_cli</span><span class="kw">)</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/fire/core.py"</span>, line 143, in Fire</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="ex">component_trace</span> = _Fire<span class="er">(</span><span class="ex">component,</span> args, parsed_flag_args, context, name<span class="kw">)</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/fire/core.py"</span>, line 477, in _Fire</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="ex">component,</span> remaining_args = _CallAndUpdateTrace<span class="er">(</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/fire/core.py"</span>, line 693, in _CallAndUpdateTrace</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="ex">component</span> = fn<span class="er">(</span><span class="ex">*varargs,</span> <span class="pp">**</span>kwargs<span class="kw">)</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/workspace/axolotl/src/axolotl/cli/train.py"</span>, line 38, in do_cli</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ex">do_train</span><span class="er">(</span><span class="ex">parsed_cfg,</span> parsed_cli_args<span class="kw">)</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/workspace/axolotl/src/axolotl/cli/train.py"</span>, line 66, in do_train</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ex">train</span><span class="er">(</span><span class="va">cfg</span><span class="op">=</span>cfg, <span class="va">cli_args</span><span class="op">=</span>cli_args, <span class="va">dataset_meta</span><span class="op">=</span>dataset_meta<span class="kw">)</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/workspace/axolotl/src/axolotl/train.py"</span>, line 170, in train</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="ex">trainer.train</span><span class="er">(</span><span class="va">resume_from_checkpoint</span><span class="op">=</span>resume_from_checkpoint<span class="kw">)</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py"</span>, line 1859, in train</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ex">inner_training_loop</span><span class="er">(</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/trainer.py"</span>, line 2249, in _inner_training_loop</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="ex">_grad_norm</span> = self.accelerator.clip_grad_norm_<span class="er">(</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/accelerator.py"</span>, line 2269, in clip_grad_norm_</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">self.unscale_gradients()</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/accelerator.py"</span>, line 2219, in unscale_gradients</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="ex">self.scaler.unscale_</span><span class="er">(</span><span class="ex">opt</span><span class="kw">)</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py"</span>, line 307, in unscale_</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="va">optimizer_state</span><span class="op">[</span><span class="st">"found_inf_per_device"</span><span class="op">]</span> <span class="ex">=</span> self._unscale_grads_<span class="er">(</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py"</span>, line 248, in _unscale_grads_</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="ex">torch._amp_foreach_non_finite_check_and_unscale_</span><span class="er">(</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="ex">RuntimeError:</span> <span class="st">"_amp_foreach_non_finite_check_and_unscale_cuda"</span> not implemented for <span class="st">'BFloat16'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Setting the parameter <code>bf16: false</code> resolved this issue. Although switching from an RTX5000 GPU to a 1x A100 GPU also resolved the issue.</p>
</section>
<section id="running-out-of-gpu-memory" class="level2">
<h2 class="anchored" data-anchor-id="running-out-of-gpu-memory">Running out of GPU memory</h2>
<p>I had a run where the GPU ran out of memory.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">torch.cuda.OutOfMemoryError:</span> CUDA out of memory. Tried to allocate 784.00 MiB. GPU 0 has a total capacty of 15.74 GiB of which 58.62 MiB is free. Process 1065967 has 15.67 GiB memory in use. Of the allocated memory 13.22 GiB is allocated by PyTorch, and 2.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> 🚀 View run crimson-aardvark-1 at: https://wandb.ai/law/hc-axolotl-mistral/runs/itak6glk</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> ⭐️ View project at: https://wandb.ai/law/hc-axolotl-mistral</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> Synced 6 W<span class="kw">&amp;</span><span class="ex">B</span> file<span class="er">(</span><span class="ex">s</span><span class="kw">)</span><span class="ex">,</span> 0 media file<span class="er">(</span><span class="ex">s</span><span class="kw">)</span><span class="ex">,</span> 1 artifact file<span class="er">(</span><span class="ex">s</span><span class="kw">)</span> <span class="ex">and</span> 1 other file<span class="er">(</span><span class="ex">s</span><span class="kw">)</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="ex">wandb:</span> Find logs at: ./wandb/run-20240523_233643-itak6glk/logs</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Traceback</span> <span class="er">(</span><span class="ex">most</span> recent call last<span class="kw">)</span><span class="bu">:</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/bin/accelerate"</span>, line 8, in <span class="op">&lt;</span>module<span class="op">&gt;</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="ex">sys.exit</span><span class="er">(</span><span class="fu">main()</span><span class="kw">)</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py"</span>, line 46, in main</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="ex">args.func</span><span class="er">(</span><span class="ex">args</span><span class="kw">)</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/commands/launch.py"</span>, line 1082, in launch_command</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="ex">simple_launcher</span><span class="er">(</span><span class="ex">args</span><span class="kw">)</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>  <span class="ex">File</span> <span class="st">"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/commands/launch.py"</span>, line 688, in simple_launcher</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="ex">raise</span> subprocess.CalledProcessError<span class="er">(</span><span class="va">returncode</span><span class="op">=</span>process.returncode, <span class="va">cmd</span><span class="op">=</span>cmd<span class="kw">)</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="ex">subprocess.CalledProcessError:</span> Command <span class="st">'['</span>/root/miniconda3/envs/py3.10/bin/python<span class="st">', '</span>-m<span class="st">', '</span>axolotl.cli.train<span class="st">', '</span>hc-first-100.yml<span class="st">']'</span> returned non-zero exit status 1.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I wound up needing to use a larger GPU to finetune <code>mistralai/Mistral-7B-v0.3</code>.</p>
</section>
</section>
<section id="fine-tuning-with-the-full-dataset" class="level1">
<h1>Fine-Tuning with the full dataset</h1>
<p>The config file I used is below:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">base_model</span><span class="kw">:</span><span class="at"> mistralai/Mistral-7B-v0.3</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">model_type</span><span class="kw">:</span><span class="at"> MistralForCausalLM</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">tokenizer_type</span><span class="kw">:</span><span class="at"> LlamaTokenizer</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">is_mistral_derived_model</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fu">load_in_8bit</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="fu">load_in_4bit</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="fu">strict</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_fan_in_fan_out</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="fu">data_seed</span><span class="kw">:</span><span class="at"> </span><span class="dv">49</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="fu">seed</span><span class="kw">:</span><span class="at"> </span><span class="dv">49</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="fu">datasets</span><span class="kw">:</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> data/alpaca_synth_queries_healed.jsonl</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> sharegpt</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">conversation</span><span class="kw">:</span><span class="at"> alpaca</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="fu">dataset_prepared_path</span><span class="kw">:</span><span class="at"> last_run_prepared</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="fu">val_set_size</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.1</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="fu">output_dir</span><span class="kw">:</span><span class="at"> ./qlora-alpaca-out</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="fu">hub_model_id</span><span class="kw">:</span><span class="at"> lawrencewu/hc-mistral-7B-v0.3-alpaca</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="fu">adapter</span><span class="kw">:</span><span class="at"> qlora</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_model_dir</span><span class="kw">:</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="fu">sequence_len</span><span class="kw">:</span><span class="at"> </span><span class="dv">896</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="fu">sample_packing</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="fu">pad_to_sequence_len</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_r</span><span class="kw">:</span><span class="at"> </span><span class="dv">32</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_alpha</span><span class="kw">:</span><span class="at"> </span><span class="dv">16</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_dropout</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.05</span></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_target_linear</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_fan_in_fan_out</span><span class="kw">:</span></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="fu">lora_target_modules</span><span class="kw">:</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> gate_proj</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> down_proj</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> up_proj</span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> q_proj</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> v_proj</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> k_proj</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> o_proj</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="fu">wandb_project</span><span class="kw">:</span><span class="at"> hc-axolotl-mistral</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="fu">wandb_entity</span><span class="kw">:</span><span class="at"> law</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="fu">gradient_accumulation_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">4</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a><span class="fu">micro_batch_size</span><span class="kw">:</span><span class="at"> </span><span class="dv">16</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a><span class="fu">eval_batch_size</span><span class="kw">:</span><span class="at"> </span><span class="dv">16</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a><span class="fu">num_epochs</span><span class="kw">:</span><span class="at"> </span><span class="dv">3</span></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a><span class="fu">optimizer</span><span class="kw">:</span><span class="at"> adamw_bnb_8bit</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a><span class="fu">lr_scheduler</span><span class="kw">:</span><span class="at"> cosine</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a><span class="fu">learning_rate</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0002</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="fu">max_grad_norm</span><span class="kw">:</span><span class="at"> </span><span class="fl">1.0</span></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a><span class="fu">adam_beta2</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.95</span></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a><span class="fu">adam_epsilon</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.00001</span></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a><span class="fu">save_total_limit</span><span class="kw">:</span><span class="at"> </span><span class="dv">12</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a><span class="fu">train_on_inputs</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a><span class="fu">group_by_length</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a><span class="fu">bf16</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a><span class="fu">fp16</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a><span class="fu">tf32</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a><span class="fu">gradient_checkpointing</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a><span class="fu">early_stopping_patience</span><span class="kw">:</span></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a><span class="fu">resume_from_checkpoint</span><span class="kw">:</span></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a><span class="fu">local_rank</span><span class="kw">:</span></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a><span class="fu">logging_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a><span class="fu">xformers_attention</span><span class="kw">:</span></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a><span class="fu">flash_attention</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a><span class="fu">loss_watchdog_threshold</span><span class="kw">:</span><span class="at"> </span><span class="fl">5.0</span></span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a><span class="fu">loss_watchdog_patience</span><span class="kw">:</span><span class="at"> </span><span class="dv">3</span></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a><span class="fu">warmup_steps</span><span class="kw">:</span><span class="at"> </span><span class="dv">20</span></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a><span class="fu">evals_per_epoch</span><span class="kw">:</span><span class="at"> </span><span class="dv">4</span></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a><span class="fu">eval_table_size</span><span class="kw">:</span></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a><span class="fu">eval_table_max_new_tokens</span><span class="kw">:</span><span class="at"> </span><span class="dv">128</span></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a><span class="fu">saves_per_epoch</span><span class="kw">:</span><span class="at"> </span><span class="dv">6</span></span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a><span class="fu">debug</span><span class="kw">:</span></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a><span class="fu">weight_decay</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.0</span></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a><span class="fu">fsdp</span><span class="kw">:</span></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a><span class="fu">fsdp_config</span><span class="kw">:</span></span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a><span class="fu">special_tokens</span><span class="kw">:</span></span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">bos_token</span><span class="kw">:</span><span class="at"> </span><span class="st">"&lt;s&gt;"</span></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">eos_token</span><span class="kw">:</span><span class="at"> </span><span class="st">"&lt;/s&gt;"</span></span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">unk_token</span><span class="kw">:</span><span class="at"> </span><span class="st">"&lt;unk&gt;"</span></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a><span class="fu">save_safetensors</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I launched a run with:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">accelerate</span> launch <span class="at">-m</span> axolotl.cli.train hc.yml</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I didn’t finish this run because it was going to take ~30 hours.</p>
<p>The logs are here:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">root@6df7cfbf0d81:~/axolotl/hc-mistral-alpaca#</span> accelerate launch <span class="at">-m</span> axolotl.cli.train hc.yml</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> following values were not passed to <span class="kw">`</span><span class="ex">accelerate</span> launch<span class="kw">`</span> and had defaults used instead:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--num_processes</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="ex">1</span><span class="kw">`</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--num_machines</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="ex">1</span><span class="kw">`</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--mixed_precision</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="st">'no'</span><span class="kw">`</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        <span class="kw">`</span><span class="ex">--dynamo_backend</span><span class="kw">`</span> was set to a value of <span class="kw">`</span><span class="st">'no'</span><span class="kw">`</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="ex">To</span> avoid this warning pass in values for each of the problematic parameters or run <span class="kw">`</span><span class="ex">accelerate</span> config<span class="kw">`</span>.</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="ex">WARNING:</span> BNB_CUDA_VERSION=118 environment variable detected<span class="kw">;</span> <span class="ex">loading</span> libbitsandbytes_cuda118.so.</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="ex">This</span> can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="ex">If</span> this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="ex">If</span> you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="ex">For</span> example by adding the following to your .bashrc: export LD_LIBRARY_PATH=<span class="va">$LD_LIBRARY_PATH</span>:<span class="op">&lt;</span>path_to_cuda_dir/lib64</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-05-24</span> 00:04:01,268] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> [datasets.<span class="op">&lt;</span>module<span class="op">&gt;</span>:58] <span class="pp">[</span><span class="ss">PID:4902</span><span class="pp">]</span> PyTorch version 2.1.2+cu118 available.</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-05-24</span> 00:04:02,171] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">real_accelerator.py:203:get_accelerator</span><span class="pp">]</span> Setting ds_accelerator to cuda <span class="er">(</span><span class="ex">auto</span> detect<span class="kw">)</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-05-24</span> 00:04:02,240] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">root.spawn:38</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:4902</span><span class="pp">]</span> gcc <span class="at">-pthread</span> <span class="at">-B</span> /root/miniconda3/envs/py3.10/compiler_compat <span class="at">-Wno-unused-result</span> <span class="at">-Wsign-compare</span> <span class="at">-DNDEBUG</span> <span class="at">-fwrapv</span> <span class="at">-O2</span> <span class="at">-Wall</span> <span class="at">-fPIC</span> <span class="at">-O2</span> <span class="at">-isystem</span> /root/miniconda3/envs/py3.10/include <span class="at">-fPIC</span> <span class="at">-O2</span> <span class="at">-isystem</span> /root/miniconda3/envs/py3.10/include <span class="at">-fPIC</span> <span class="at">-c</span> /tmp/tmp63g3s38_/test.c <span class="at">-o</span> /tmp/tmp63g3s38_/test.o</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-05-24</span> 00:04:02,258] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">root.spawn:38</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:4902</span><span class="pp">]</span> gcc <span class="at">-pthread</span> <span class="at">-B</span> /root/miniconda3/envs/py3.10/compiler_compat /tmp/tmp63g3s38_/test.o <span class="at">-laio</span> <span class="at">-o</span> /tmp/tmp63g3s38_/a.out</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a> <span class="ex">[WARNING]</span>  Please specify the CUTLASS repo directory as environment variable <span class="va">$CUTLASS_PATH</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a> <span class="ex">[WARNING]</span>  sparse_attn requires a torch version <span class="op">&gt;</span>= 1.5 and <span class="op">&lt;</span> 2.0 but detected 2.1</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a> <span class="ex">[WARNING]</span>  using untested triton version <span class="er">(</span><span class="ex">2.1.0</span><span class="kw">)</span><span class="ex">,</span> only 1.0.0 is known to be compatible</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="ex">/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132:</span> FutureWarning: <span class="kw">`</span><span class="ex">resume_download</span><span class="kw">`</span> is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use <span class="kw">`</span><span class="va">force_download</span><span class="op">=</span>True<span class="kw">`</span><span class="bu">.</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>  <span class="ex">warnings.warn</span><span class="er">(</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="ex">[2024-05-24</span> 00:04:04,037] <span class="pp">[</span><span class="ss">INFO</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">axolotl.normalize_config:182</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">PID:4902</span><span class="pp">]</span> <span class="pp">[</span><span class="ss">RANK:0</span><span class="pp">]</span> GPU memory usage baseline: 0.000GB <span class="er">(</span><span class="ex">+0.627GB</span> misc<span class="kw">)</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>                                 <span class="ex">dP</span>            dP   dP </span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>                                 <span class="ex">88</span>            88   88 </span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>      <span class="ex">.d8888b.</span> dP.  .dP .d8888b. 88 .d8888b. d8888P 88 </span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>      <span class="ex">88</span><span class="st">'  `88  `8bd8'</span>  88<span class="st">'  `88 88 88'</span>  <span class="kw">`</span><span class="ex">88</span>   88   88 </span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>      <span class="ex">88.</span>  .88  .d88b.  88.  .88 88 88.  .88   88   88 </span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>      <span class="kw">`</span>88888P8 dP<span class="st">'  `dP `88888P'</span> dP <span class="kw">`</span><span class="ex">88888P</span><span class="st">'   dP   dP </span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="st">                                                       </span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="st">                                                       </span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="st">****************************************</span></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="st">**** Axolotl Dependency Versions *****</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a><span class="st">  accelerate: 0.30.1         </span></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="st">        peft: 0.10.0         </span></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="st">transformers: 4.40.2         </span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="st">         trl: 0.8.5          </span></span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="st">       torch: 2.1.2+cu118    </span></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span class="st">bitsandbytes: 0.43.1         </span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a><span class="st">****************************************</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span class="st">/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.</span></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a><span class="st">  warnings.warn(</span></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:04:05,053] [DEBUG] [axolotl.load_tokenizer:280] [PID:4902] [RANK:0] EOS: 2 / &lt;/s&gt;</span></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:04:05,053] [DEBUG] [axolotl.load_tokenizer:281] [PID:4902] [RANK:0] BOS: 1 / &lt;s&gt;</span></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:04:05,053] [DEBUG] [axolotl.load_tokenizer:282] [PID:4902] [RANK:0] PAD: 2 / &lt;/s&gt;</span></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:04:05,053] [DEBUG] [axolotl.load_tokenizer:283] [PID:4902] [RANK:0] UNK: 0 / &lt;unk&gt;</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:04:05,053] [INFO] [axolotl.load_tokenizer:294] [PID:4902] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.</span></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:04:05,053] [INFO] [axolotl.load_tokenized_prepared_datasets:183] [PID:4902] [RANK:0] Unable to find prepared dataset in last_run_prepared/a1079e1609d0b7bf952979250cf0f7f4</span></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:04:05,054] [INFO] [axolotl.load_tokenized_prepared_datasets:184] [PID:4902] [RANK:0] Loading raw datasets...</span></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:04:05,054] [WARNING] [axolotl.load_tokenized_prepared_datasets:186] [PID:4902] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.</span></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a><span class="st">Generating train split: 133501 examples [00:01, 75757.77 examples/s]</span></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a><span class="st">Tokenizing Prompts (num_proc=64): 100%|███████████████████████████████████████████| 133501/133501 [01:21&lt;00:00, 1635.33 examples/s]</span></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:31,099] [INFO] [axolotl.load_tokenized_prepared_datasets:410] [PID:4902] [RANK:0] merging datasets</span></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a><span class="st">Dropping Long Sequences (num_proc=64): 100%|█████████████████████████████████████| 133501/133501 [00:10&lt;00:00, 12220.82 examples/s]</span></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:43,227] [INFO] [axolotl.load_tokenized_prepared_datasets:423] [PID:4902] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/a1079e1609d0b7bf952979250cf0f7f4</span></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a><span class="st">Saving the dataset (2/2 shards): 100%|███████████████████████████████████████████| 127998/127998 [00:01&lt;00:00, 93288.97 examples/s]</span></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:44,812] [DEBUG] [axolotl.calculate_total_num_steps:299] [PID:4902] [RANK:0] total_num_tokens: 70_440_026</span></span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:46,240] [DEBUG] [axolotl.calculate_total_num_steps:312] [PID:4902] [RANK:0] `total_supervised_tokens: 14_142_350`</span></span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:46,240] [DEBUG] [axolotl.calculate_total_num_steps:391] [PID:4902] [RANK:0] total_num_steps: 5400</span></span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:46,247] [DEBUG] [axolotl.train.train:56] [PID:4902] [RANK:0] loading tokenizer... mistralai/Mistral-7B-v0.3</span></span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:46,967] [DEBUG] [axolotl.load_tokenizer:280] [PID:4902] [RANK:0] EOS: 2 / &lt;/s&gt;</span></span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:46,967] [DEBUG] [axolotl.load_tokenizer:281] [PID:4902] [RANK:0] BOS: 1 / &lt;s&gt;</span></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:46,967] [DEBUG] [axolotl.load_tokenizer:282] [PID:4902] [RANK:0] PAD: 2 / &lt;/s&gt;</span></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:46,967] [DEBUG] [axolotl.load_tokenizer:283] [PID:4902] [RANK:0] UNK: 0 / &lt;unk&gt;</span></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:46,967] [INFO] [axolotl.load_tokenizer:294] [PID:4902] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.</span></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:46,967] [DEBUG] [axolotl.train.train:85] [PID:4902] [RANK:0] loading model and peft_config...</span></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a><span class="st">`low_cpu_mem_usage` was None, now set to True since model is quantized.</span></span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a><span class="st">Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [00:03&lt;00:00,  1.19s/it]</span></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:53,315] [INFO] [axolotl.load_model:734] [PID:4902] [RANK:0] GPU memory usage after model load: 4.354GB (+0.146GB cache, +1.111GB misc)</span></span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:53,326] [INFO] [axolotl.load_model:785] [PID:4902] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training</span></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:53,330] [INFO] [axolotl.load_model:794] [PID:4902] [RANK:0] converting modules to torch.bfloat16 for flash attention</span></span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:53,334] [INFO] [axolotl.load_lora:951] [PID:4902] [RANK:0] found linear modules: ['</span><span class="ex">v_proj</span><span class="st">', '</span><span class="ex">up_proj</span><span class="st">', '</span><span class="ex">q_proj</span><span class="st">', '</span><span class="ex">k_proj</span><span class="st">', '</span><span class="ex">down_proj</span><span class="st">', '</span><span class="ex">gate_proj</span><span class="st">', '</span><span class="ex">o_proj</span><span class="st">']</span></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a><span class="st">trainable params: 83,886,080 || all params: 7,331,909,632 || trainable%: 1.1441232122376492</span></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:54,299] [INFO] [axolotl.load_model:843] [PID:4902] [RANK:0] GPU memory usage after adapters: 4.511GB (+1.146GB cache, +1.111GB misc)</span></span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:54,787] [INFO] [axolotl.train.train:119] [PID:4902] [RANK:0] Pre-saving adapter config to ./qlora-alpaca-out</span></span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:54,807] [INFO] [axolotl.train.train:156] [PID:4902] [RANK:0] Starting trainer...</span></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: Currently logged in as: law. Use `wandb login --relogin` to force relogin</span></span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: Tracking run with wandb version 0.17.0</span></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: Run data is saved locally in /home/axolotl/hc-mistral-alpaca/wandb/run-20240524_000556-iewv47f2</span></span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: Run `wandb offline` to turn off syncing.</span></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: Syncing run lyric-wildflower-5</span></span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: ⭐️ View project at https://wandb.ai/law/hc-axolotl-mistral</span></span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: 🚀 View run at https://wandb.ai/law/hc-axolotl-mistral/runs/iewv47f2</span></span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")</span></span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a><span class="st">[2024-05-24 00:05:58,369] [INFO] [axolotl.callbacks.on_train_begin:771] [PID:4902] [RANK:0] The Axolotl config has been saved to the WandB run under files.</span></span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 1.154, '</span><span class="ex">grad_norm</span><span class="st">': 2.078125, '</span><span class="ex">learning_rate</span><span class="st">': 1e-05, '</span><span class="ex">epoch</span><span class="st">': 0.0}                                                       </span></span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a><span class="st">  0%|                                                                                          | 1/5400 [00:21&lt;32:33:13, 21.71s</span></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a><span class="st"> 49%|█████████████████████████████████████████████▍                      </span></span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a><span class="st"> 50%|███████████████████████████████████████████▌                        </span></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a><span class="st">                                                                         {'</span><span class="ex">eval_loss</span><span class="st">': 1.1900806427001953, '</span><span class="ex">eval_runtime</span><span class="st">': 1342.7584, '</span><span class="ex">eval_samples_per_second</span><span class="st">': 9.533, '</span><span class="ex">eval_steps_per_second</span><span class="st">': 0.596, '</span><span class="ex">epoch</span><span class="st">': 0.0}      </span></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a><span class="st">  0%|                                | 1/5400 [22:44&lt;32:33:13, 21.71s/it[2024-05-24 00:29:04,813] [INFO] [axolotl.callbacks.on_step_end:126] [PID:4902] [RANK:0] GPU memory usage while training: 4.684GB (+12.633GB cache, +1.136GB misc)</span></span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 1.1821, '</span><span class="ex">grad_norm</span><span class="st">': 2.125, '</span><span class="ex">learning_rate</span><span class="st">': 2e-05, '</span><span class="ex">epoch</span><span class="st">': 0.0}</span></span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 1.1561, '</span><span class="ex">grad_norm</span><span class="st">': 1.9609375, '</span><span class="ex">learning_rate</span><span class="st">': 3e-05, '</span><span class="ex">epoch</span><span class="st">': 0.0}</span></span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 1.1569, '</span><span class="ex">grad_norm</span><span class="st">': 1.3671875, '</span><span class="ex">learning_rate</span><span class="st">': 4e-05, '</span><span class="ex">epoch</span><span class="st">': 0.0}</span></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 1.1285, '</span><span class="ex">grad_norm</span><span class="st">': 1.1640625, '</span><span class="ex">learning_rate</span><span class="st">': 5e-05, '</span><span class="ex">epoch</span><span class="st">': 0.0}</span></span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 1.0089, '</span><span class="ex">grad_norm</span><span class="st">': 1.0234375, '</span><span class="ex">learning_rate</span><span class="st">': 6e-05, '</span><span class="ex">epoch</span><span class="st">': 0.0}</span></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.874, '</span><span class="ex">grad_norm</span><span class="st">': 1.0390625, '</span><span class="ex">learning_rate</span><span class="st">': 7e-05, '</span><span class="ex">epoch</span><span class="st">': 0.0}</span></span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.7215, '</span><span class="ex">grad_norm</span><span class="st">': 1.0234375, '</span><span class="ex">learning_rate</span><span class="st">': 8e-05, '</span><span class="ex">epoch</span><span class="st">': 0.0}</span></span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.632, '</span><span class="ex">grad_norm</span><span class="st">': 1.0625, '</span><span class="ex">learning_rate</span><span class="st">': 9e-05, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.4603, '</span><span class="ex">grad_norm</span><span class="st">': 0.8984375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.3983, '</span><span class="ex">grad_norm</span><span class="st">': 0.6796875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00011000000000000002, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.363, '</span><span class="ex">grad_norm</span><span class="st">': 0.796875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00012, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.3174, '</span><span class="ex">grad_norm</span><span class="st">': 0.7421875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00013000000000000002, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.244, '</span><span class="ex">grad_norm</span><span class="st">': 0.73046875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00014, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.2493, '</span><span class="ex">grad_norm</span><span class="st">': 0.478515625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00015000000000000001, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.2496, '</span><span class="ex">grad_norm</span><span class="st">': 0.373046875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00016, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.2267, '</span><span class="ex">grad_norm</span><span class="st">': 0.400390625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00017, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.2481, '</span><span class="ex">grad_norm</span><span class="st">': 0.3671875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00018, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.2055, '</span><span class="ex">grad_norm</span><span class="st">': 0.3359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.2, '</span><span class="ex">grad_norm</span><span class="st">': 0.283203125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0002, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1825, '</span><span class="ex">grad_norm</span><span class="st">': 0.28515625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999998295075366, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.2323, '</span><span class="ex">grad_norm</span><span class="st">': 0.27734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999993180302042, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1805, '</span><span class="ex">grad_norm</span><span class="st">': 0.37109375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999984655681775, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1738, '</span><span class="ex">grad_norm</span><span class="st">': 0.283203125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999997272121747, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1843, '</span><span class="ex">grad_norm</span><span class="st">': 0.2333984375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999957376913195, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1804, '</span><span class="ex">grad_norm</span><span class="st">': 0.25, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999938622774187, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1682, '</span><span class="ex">grad_norm</span><span class="st">': 0.2216796875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999916458806832, '</span><span class="ex">epoch</span><span class="st">': 0.01}</span></span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1838, '</span><span class="ex">grad_norm</span><span class="st">': 0.1982421875, '</span><span class="ex">learning_rate</span><span class="st">': 0.000199998908850187, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.149, '</span><span class="ex">grad_norm</span><span class="st">': 0.1962890625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999861901418502, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1628, '</span><span class="ex">grad_norm</span><span class="st">': 0.25390625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999829508016124, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1699, '</span><span class="ex">grad_norm</span><span class="st">': 0.2265625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999979370482261, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1719, '</span><span class="ex">grad_norm</span><span class="st">': 0.1943359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999754491850172, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1624, '</span><span class="ex">grad_norm</span><span class="st">': 0.2001953125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999711869112178, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1532, '</span><span class="ex">grad_norm</span><span class="st">': 0.1982421875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999665836623162, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1503, '</span><span class="ex">grad_norm</span><span class="st">': 0.19921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999616394398821, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1893, '</span><span class="ex">grad_norm</span><span class="st">': 0.1591796875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999563542456015, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1594, '</span><span class="ex">grad_norm</span><span class="st">': 0.1826171875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999507280812765, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1636, '</span><span class="ex">grad_norm</span><span class="st">': 0.1943359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999944760948825, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1473, '</span><span class="ex">grad_norm</span><span class="st">': 0.2470703125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999384528502826, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1527, '</span><span class="ex">grad_norm</span><span class="st">': 0.25390625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999318037877995, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1473, '</span><span class="ex">grad_norm</span><span class="st">': 0.1552734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999248137636438, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1606, '</span><span class="ex">grad_norm</span><span class="st">': 0.1826171875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999174827801984, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1549, '</span><span class="ex">grad_norm</span><span class="st">': 0.158203125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999909810839963, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1742, '</span><span class="ex">grad_norm</span><span class="st">': 0.1953125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019999017979455537, '</span><span class="ex">epoch</span><span class="st">': 0.02}</span></span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.148, '</span><span class="ex">grad_norm</span><span class="st">': 0.1748046875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999893444099703, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1534, '</span><span class="ex">grad_norm</span><span class="st">': 0.1865234375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999884749305259, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1225, '</span><span class="ex">grad_norm</span><span class="st">': 0.1552734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999875713565187, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1484, '</span><span class="ex">grad_norm</span><span class="st">': 0.181640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999866336882568, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1731, '</span><span class="ex">grad_norm</span><span class="st">': 0.2119140625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019998566192605988, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1738, '</span><span class="ex">grad_norm</span><span class="st">': 0.1640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019998465607025935, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1364, '</span><span class="ex">grad_norm</span><span class="st">': 0.1396484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019998361612119813, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1443, '</span><span class="ex">grad_norm</span><span class="st">': 0.1416015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999825420792309, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1725, '</span><span class="ex">grad_norm</span><span class="st">': 0.2080078125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019998143394472386, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1547, '</span><span class="ex">grad_norm</span><span class="st">': 0.1572265625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019998029171805487, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1499, '</span><span class="ex">grad_norm</span><span class="st">': 0.1708984375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019997911539961337, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1617, '</span><span class="ex">grad_norm</span><span class="st">': 0.162109375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019997790498980055, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1443, '</span><span class="ex">grad_norm</span><span class="st">': 0.142578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999766604890291, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1668, '</span><span class="ex">grad_norm</span><span class="st">': 0.1552734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019997538189772335, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1624, '</span><span class="ex">grad_norm</span><span class="st">': 0.138671875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999740692163193, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1459, '</span><span class="ex">grad_norm</span><span class="st">': 0.146484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019997272244526456, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1433, '</span><span class="ex">grad_norm</span><span class="st">': 0.158203125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019997134158501837, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1284, '</span><span class="ex">grad_norm</span><span class="st">': 0.1640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019996992663605156, '</span><span class="ex">epoch</span><span class="st">': 0.03}</span></span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1618, '</span><span class="ex">grad_norm</span><span class="st">': 0.166015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019996847759884661, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1454, '</span><span class="ex">grad_norm</span><span class="st">': 0.162109375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019996699447389764, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1416, '</span><span class="ex">grad_norm</span><span class="st">': 0.1591796875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019996547726171032, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1387, '</span><span class="ex">grad_norm</span><span class="st">': 0.134765625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019996392596280206, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1362, '</span><span class="ex">grad_norm</span><span class="st">': 0.14453125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019996234057770184, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1324, '</span><span class="ex">grad_norm</span><span class="st">': 0.1640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019996072110695017, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1306, '</span><span class="ex">grad_norm</span><span class="st">': 0.169921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019995906755109933, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1395, '</span><span class="ex">grad_norm</span><span class="st">': 0.1591796875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019995737991071314, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1264, '</span><span class="ex">grad_norm</span><span class="st">': 0.1591796875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019995565818636707, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.121, '</span><span class="ex">grad_norm</span><span class="st">': 0.1630859375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019995390237864818, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1376, '</span><span class="ex">grad_norm</span><span class="st">': 0.142578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019995211248815517, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1344, '</span><span class="ex">grad_norm</span><span class="st">': 0.1611328125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999502885154984, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.154, '</span><span class="ex">grad_norm</span><span class="st">': 0.14453125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019994843046129977, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1627, '</span><span class="ex">grad_norm</span><span class="st">': 0.15234375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019994653832619292, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1353, '</span><span class="ex">grad_norm</span><span class="st">': 0.16796875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019994461211082296, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.132, '</span><span class="ex">grad_norm</span><span class="st">': 0.1845703125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019994265181584676, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1356, '</span><span class="ex">grad_norm</span><span class="st">': 0.1630859375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019994065744193272, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1466, '</span><span class="ex">grad_norm</span><span class="st">': 0.1552734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999386289897609, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1259, '</span><span class="ex">grad_norm</span><span class="st">': 0.140625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019993656646002296, '</span><span class="ex">epoch</span><span class="st">': 0.04}</span></span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1346, '</span><span class="ex">grad_norm</span><span class="st">': 0.146484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019993446985342223, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1388, '</span><span class="ex">grad_norm</span><span class="st">': 0.1767578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019993233917067358, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1427, '</span><span class="ex">grad_norm</span><span class="st">': 0.1435546875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019993017441250356, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1246, '</span><span class="ex">grad_norm</span><span class="st">': 0.146484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999279755796503, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1381, '</span><span class="ex">grad_norm</span><span class="st">': 0.162109375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019992574267286358, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1184, '</span><span class="ex">grad_norm</span><span class="st">': 0.1435546875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999234756929048, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.133, '</span><span class="ex">grad_norm</span><span class="st">': 0.1787109375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019992117464054696, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1297, '</span><span class="ex">grad_norm</span><span class="st">': 0.17578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019991883951657466, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1329, '</span><span class="ex">grad_norm</span><span class="st">': 0.142578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999164703217842, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1249, '</span><span class="ex">grad_norm</span><span class="st">': 0.1640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019991406705698338, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1215, '</span><span class="ex">grad_norm</span><span class="st">': 0.154296875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001999116297229917, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1354, '</span><span class="ex">grad_norm</span><span class="st">': 0.1708984375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019990915832064025, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1266, '</span><span class="ex">grad_norm</span><span class="st">': 0.1728515625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019990665285077178, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1155, '</span><span class="ex">grad_norm</span><span class="st">': 0.1513671875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019990411331424052, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1454, '</span><span class="ex">grad_norm</span><span class="st">': 0.1787109375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019990153971191253, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1322, '</span><span class="ex">grad_norm</span><span class="st">': 0.1416015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019989893204466527, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.125, '</span><span class="ex">grad_norm</span><span class="st">': 0.162109375, '</span><span class="ex">learning_rate</span><span class="st">': 0.000199896290313388, '</span><span class="ex">epoch</span><span class="st">': 0.05}</span></span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1085, '</span><span class="ex">grad_norm</span><span class="st">': 0.1806640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019989361451898144, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1441, '</span><span class="ex">grad_norm</span><span class="st">': 0.146484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019989090466235806, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.114, '</span><span class="ex">grad_norm</span><span class="st">': 0.134765625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019988816074444183, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1252, '</span><span class="ex">grad_norm</span><span class="st">': 0.1455078125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001998853827661684, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1251, '</span><span class="ex">grad_norm</span><span class="st">': 0.162109375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019988257072848503, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1133, '</span><span class="ex">grad_norm</span><span class="st">': 0.16796875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019987972463235057, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-196"><a href="#cb12-196" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1249, '</span><span class="ex">grad_norm</span><span class="st">': 0.1689453125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019987684447873548, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-197"><a href="#cb12-197" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1352, '</span><span class="ex">grad_norm</span><span class="st">': 0.2158203125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001998739302686219, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1248, '</span><span class="ex">grad_norm</span><span class="st">': 0.1484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019987098200300349, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.117, '</span><span class="ex">grad_norm</span><span class="st">': 0.1962890625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019986799968288557, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1239, '</span><span class="ex">grad_norm</span><span class="st">': 0.1533203125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019986498330928508, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1643, '</span><span class="ex">grad_norm</span><span class="st">': 0.1474609375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001998619328832305, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1051, '</span><span class="ex">grad_norm</span><span class="st">': 0.134765625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001998588484057621, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1148, '</span><span class="ex">grad_norm</span><span class="st">': 0.1484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001998557298779315, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1135, '</span><span class="ex">grad_norm</span><span class="st">': 0.162109375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019985257730080217, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1121, '</span><span class="ex">grad_norm</span><span class="st">': 0.1513671875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019984939067544907, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1646, '</span><span class="ex">grad_norm</span><span class="st">': 0.205078125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019984617000295876, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-207"><a href="#cb12-207" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1103, '</span><span class="ex">grad_norm</span><span class="st">': 0.1416015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019984291528442945, '</span><span class="ex">epoch</span><span class="st">': 0.06}</span></span>
<span id="cb12-208"><a href="#cb12-208" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1076, '</span><span class="ex">grad_norm</span><span class="st">': 0.1474609375, '</span><span class="ex">learning_rate</span><span class="st">': 0.000199839626520971, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.123, '</span><span class="ex">grad_norm</span><span class="st">': 0.1650390625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019983630371370477, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1194, '</span><span class="ex">grad_norm</span><span class="st">': 0.1728515625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019983294686376382, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1164, '</span><span class="ex">grad_norm</span><span class="st">': 0.1396484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019982955597229275, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1089, '</span><span class="ex">grad_norm</span><span class="st">': 0.1328125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019982613104044784, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1328, '</span><span class="ex">grad_norm</span><span class="st">': 0.1865234375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019982267206939693, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1297, '</span><span class="ex">grad_norm</span><span class="st">': 0.1484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019981917906031947, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1077, '</span><span class="ex">grad_norm</span><span class="st">': 0.15234375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019981565201440652, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1324, '</span><span class="ex">grad_norm</span><span class="st">': 0.1728515625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019981209093286077, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1138, '</span><span class="ex">grad_norm</span><span class="st">': 0.1494140625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019980849581689646, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1062, '</span><span class="ex">grad_norm</span><span class="st">': 0.1767578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001998048666677395, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1464, '</span><span class="ex">grad_norm</span><span class="st">': 0.1923828125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019980120348662736, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1184, '</span><span class="ex">grad_norm</span><span class="st">': 0.1650390625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019979750627480914, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-221"><a href="#cb12-221" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.113, '</span><span class="ex">grad_norm</span><span class="st">': 0.1767578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019979377503354554, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-222"><a href="#cb12-222" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1406, '</span><span class="ex">grad_norm</span><span class="st">': 0.1875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019979000976410886, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1111, '</span><span class="ex">grad_norm</span><span class="st">': 0.166015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019978621046778296, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1007, '</span><span class="ex">grad_norm</span><span class="st">': 0.1845703125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001997823771458634, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-225"><a href="#cb12-225" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1047, '</span><span class="ex">grad_norm</span><span class="st">': 0.16015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019977850979965723, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-226"><a href="#cb12-226" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1217, '</span><span class="ex">grad_norm</span><span class="st">': 0.1455078125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019977460843048316, '</span><span class="ex">epoch</span><span class="st">': 0.07}</span></span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1105, '</span><span class="ex">grad_norm</span><span class="st">': 0.1552734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019977067303967154, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1111, '</span><span class="ex">grad_norm</span><span class="st">': 0.169921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019976670362856428, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1297, '</span><span class="ex">grad_norm</span><span class="st">': 0.142578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019976270019851484, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-230"><a href="#cb12-230" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0978, '</span><span class="ex">grad_norm</span><span class="st">': 0.1513671875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019975866275088837, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-231"><a href="#cb12-231" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1056, '</span><span class="ex">grad_norm</span><span class="st">': 0.1396484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019975459128706156, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-232"><a href="#cb12-232" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.116, '</span><span class="ex">grad_norm</span><span class="st">': 0.1474609375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001997504858084227, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-233"><a href="#cb12-233" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1197, '</span><span class="ex">grad_norm</span><span class="st">': 0.17578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019974634631637173, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-234"><a href="#cb12-234" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1108, '</span><span class="ex">grad_norm</span><span class="st">': 0.169921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019974217281232019, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-235"><a href="#cb12-235" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1131, '</span><span class="ex">grad_norm</span><span class="st">': 0.2216796875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019973796529769108, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-236"><a href="#cb12-236" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1186, '</span><span class="ex">grad_norm</span><span class="st">': 0.16015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001997337237739192, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-237"><a href="#cb12-237" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1044, '</span><span class="ex">grad_norm</span><span class="st">': 0.2021484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019972944824245078, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-238"><a href="#cb12-238" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1091, '</span><span class="ex">grad_norm</span><span class="st">': 0.2197265625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019972513870474375, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-239"><a href="#cb12-239" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1098, '</span><span class="ex">grad_norm</span><span class="st">': 0.185546875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019972079516226754, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-240"><a href="#cb12-240" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0996, '</span><span class="ex">grad_norm</span><span class="st">': 0.166015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001997164176165033, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-241"><a href="#cb12-241" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.124, '</span><span class="ex">grad_norm</span><span class="st">': 0.150390625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001997120060689437, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-242"><a href="#cb12-242" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1075, '</span><span class="ex">grad_norm</span><span class="st">': 0.1953125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019970756052109295, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-243"><a href="#cb12-243" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.112, '</span><span class="ex">grad_norm</span><span class="st">': 0.1748046875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019970308097446698, '</span><span class="ex">epoch</span><span class="st">': 0.08}</span></span>
<span id="cb12-244"><a href="#cb12-244" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.106, '</span><span class="ex">grad_norm</span><span class="st">': 0.1435546875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001996985674305932, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-245"><a href="#cb12-245" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1106, '</span><span class="ex">grad_norm</span><span class="st">': 0.177734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001996940198910107, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-246"><a href="#cb12-246" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1015, '</span><span class="ex">grad_norm</span><span class="st">': 0.1787109375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019968943835727013, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-247"><a href="#cb12-247" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1123, '</span><span class="ex">grad_norm</span><span class="st">': 0.1728515625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019968482283093367, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-248"><a href="#cb12-248" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.106, '</span><span class="ex">grad_norm</span><span class="st">': 0.173828125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019968017331357517, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-249"><a href="#cb12-249" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1481, '</span><span class="ex">grad_norm</span><span class="st">': 0.173828125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019967548980678008, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-250"><a href="#cb12-250" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1166, '</span><span class="ex">grad_norm</span><span class="st">': 0.17578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019967077231214535, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-251"><a href="#cb12-251" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0998, '</span><span class="ex">grad_norm</span><span class="st">': 0.1708984375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001996660208312796, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-252"><a href="#cb12-252" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0926, '</span><span class="ex">grad_norm</span><span class="st">': 0.14453125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019966123536580303, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-253"><a href="#cb12-253" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0877, '</span><span class="ex">grad_norm</span><span class="st">': 0.1484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019965641591734737, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-254"><a href="#cb12-254" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0894, '</span><span class="ex">grad_norm</span><span class="st">': 0.173828125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019965156248755606, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-255"><a href="#cb12-255" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.139, '</span><span class="ex">grad_norm</span><span class="st">': 0.189453125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019964667507808395, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-256"><a href="#cb12-256" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1024, '</span><span class="ex">grad_norm</span><span class="st">': 0.20703125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019964175369059764, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-257"><a href="#cb12-257" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1251, '</span><span class="ex">grad_norm</span><span class="st">': 0.15234375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019963679832677518, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-258"><a href="#cb12-258" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.124, '</span><span class="ex">grad_norm</span><span class="st">': 0.1875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019963180898830633, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-259"><a href="#cb12-259" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1109, '</span><span class="ex">grad_norm</span><span class="st">': 0.1650390625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001996267856768924, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-260"><a href="#cb12-260" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0973, '</span><span class="ex">grad_norm</span><span class="st">': 0.16015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001996217283942462, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-261"><a href="#cb12-261" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1159, '</span><span class="ex">grad_norm</span><span class="st">': 0.1845703125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001996166371420922, '</span><span class="ex">epoch</span><span class="st">': 0.09}</span></span>
<span id="cb12-262"><a href="#cb12-262" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1231, '</span><span class="ex">grad_norm</span><span class="st">': 0.1689453125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001996115119221665, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-263"><a href="#cb12-263" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0957, '</span><span class="ex">grad_norm</span><span class="st">': 0.2373046875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019960635273621666, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-264"><a href="#cb12-264" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1082, '</span><span class="ex">grad_norm</span><span class="st">': 0.1611328125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019960115958600193, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-265"><a href="#cb12-265" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1012, '</span><span class="ex">grad_norm</span><span class="st">': 0.1650390625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019959593247329305, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-266"><a href="#cb12-266" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1108, '</span><span class="ex">grad_norm</span><span class="st">': 0.2177734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001995906713998724, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-267"><a href="#cb12-267" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0988, '</span><span class="ex">grad_norm</span><span class="st">': 0.1875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019958537636753393, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-268"><a href="#cb12-268" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0922, '</span><span class="ex">grad_norm</span><span class="st">': 0.2451171875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019958004737808318, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-269"><a href="#cb12-269" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1051, '</span><span class="ex">grad_norm</span><span class="st">': 0.1611328125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019957468443333723, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-270"><a href="#cb12-270" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1174, '</span><span class="ex">grad_norm</span><span class="st">': 0.1826171875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001995692875351248, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-271"><a href="#cb12-271" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1081, '</span><span class="ex">grad_norm</span><span class="st">': 0.1669921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019956385668528612, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-272"><a href="#cb12-272" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0972, '</span><span class="ex">grad_norm</span><span class="st">': 0.18359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019955839188567307, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-273"><a href="#cb12-273" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.121, '</span><span class="ex">grad_norm</span><span class="st">': 0.1630859375, '</span><span class="ex">learning_rate</span><span class="st">': 0.000199552893138149, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-274"><a href="#cb12-274" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0851, '</span><span class="ex">grad_norm</span><span class="st">': 0.140625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019954736044458892, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-275"><a href="#cb12-275" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0979, '</span><span class="ex">grad_norm</span><span class="st">': 0.171875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019954179380687946, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-276"><a href="#cb12-276" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0904, '</span><span class="ex">grad_norm</span><span class="st">': 0.166015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019953619322691865, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-277"><a href="#cb12-277" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0865, '</span><span class="ex">grad_norm</span><span class="st">': 0.16796875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019953055870661627, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-278"><a href="#cb12-278" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0862, '</span><span class="ex">grad_norm</span><span class="st">': 0.16015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019952489024789363, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-279"><a href="#cb12-279" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1032, '</span><span class="ex">grad_norm</span><span class="st">': 0.201171875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019951918785268352, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-280"><a href="#cb12-280" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1136, '</span><span class="ex">grad_norm</span><span class="st">': 0.171875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001995134515229304, '</span><span class="ex">epoch</span><span class="st">': 0.1}</span></span>
<span id="cb12-281"><a href="#cb12-281" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0864, '</span><span class="ex">grad_norm</span><span class="st">': 0.1669921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001995076812605903, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-282"><a href="#cb12-282" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0912, '</span><span class="ex">grad_norm</span><span class="st">': 0.1689453125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019950187706763078, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-283"><a href="#cb12-283" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1108, '</span><span class="ex">grad_norm</span><span class="st">': 0.1748046875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019949603894603096, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-284"><a href="#cb12-284" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0828, '</span><span class="ex">grad_norm</span><span class="st">': 0.193359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019949016689778157, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-285"><a href="#cb12-285" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0886, '</span><span class="ex">grad_norm</span><span class="st">': 0.1767578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019948426092488488, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-286"><a href="#cb12-286" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0986, '</span><span class="ex">grad_norm</span><span class="st">': 0.2119140625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019947832102935474, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-287"><a href="#cb12-287" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0855, '</span><span class="ex">grad_norm</span><span class="st">': 0.20703125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019947234721321658, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-288"><a href="#cb12-288" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.092, '</span><span class="ex">grad_norm</span><span class="st">': 0.193359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019946633947850738, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-289"><a href="#cb12-289" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0766, '</span><span class="ex">grad_norm</span><span class="st">': 0.1611328125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001994602978272756, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-290"><a href="#cb12-290" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1023, '</span><span class="ex">grad_norm</span><span class="st">': 0.166015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001994542222615815, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-291"><a href="#cb12-291" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0769, '</span><span class="ex">grad_norm</span><span class="st">': 0.177734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019944811278349667, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-292"><a href="#cb12-292" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1332, '</span><span class="ex">grad_norm</span><span class="st">': 0.208984375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019944196939510435, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-293"><a href="#cb12-293" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0935, '</span><span class="ex">grad_norm</span><span class="st">': 0.1826171875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001994357920984994, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-294"><a href="#cb12-294" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0977, '</span><span class="ex">grad_norm</span><span class="st">': 0.193359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001994295808957881, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-295"><a href="#cb12-295" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0902, '</span><span class="ex">grad_norm</span><span class="st">': 0.1806640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001994233357890884, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-296"><a href="#cb12-296" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1121, '</span><span class="ex">grad_norm</span><span class="st">': 0.1669921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019941705678052984, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-297"><a href="#cb12-297" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0833, '</span><span class="ex">grad_norm</span><span class="st">': 0.1640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019941074387225344, '</span><span class="ex">epoch</span><span class="st">': 0.11}</span></span>
<span id="cb12-298"><a href="#cb12-298" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0999, '</span><span class="ex">grad_norm</span><span class="st">': 0.1865234375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019940439706641176, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-299"><a href="#cb12-299" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1172, '</span><span class="ex">grad_norm</span><span class="st">': 0.1728515625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019939801636516903, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-300"><a href="#cb12-300" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0768, '</span><span class="ex">grad_norm</span><span class="st">': 0.169921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019939160177070094, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-301"><a href="#cb12-301" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1099, '</span><span class="ex">grad_norm</span><span class="st">': 0.1650390625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001993851532851948, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-302"><a href="#cb12-302" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1244, '</span><span class="ex">grad_norm</span><span class="st">': 0.203125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001993786709108494, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-303"><a href="#cb12-303" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1133, '</span><span class="ex">grad_norm</span><span class="st">': 0.2021484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019937215464987514, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-304"><a href="#cb12-304" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0935, '</span><span class="ex">grad_norm</span><span class="st">': 0.181640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019936560450449403, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-305"><a href="#cb12-305" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.12, '</span><span class="ex">grad_norm</span><span class="st">': 0.2353515625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019935902047693948, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-306"><a href="#cb12-306" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0775, '</span><span class="ex">grad_norm</span><span class="st">': 0.197265625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001993524025694566, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-307"><a href="#cb12-307" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.092, '</span><span class="ex">grad_norm</span><span class="st">': 0.185546875, '</span><span class="ex">learning_rate</span><span class="st">': 0.000199345750784302, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-308"><a href="#cb12-308" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1121, '</span><span class="ex">grad_norm</span><span class="st">': 0.1904296875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001993390651237438, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-309"><a href="#cb12-309" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0827, '</span><span class="ex">grad_norm</span><span class="st">': 0.1826171875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019933234559006176, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-310"><a href="#cb12-310" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0652, '</span><span class="ex">grad_norm</span><span class="st">': 0.2041015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019932559218554708, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-311"><a href="#cb12-311" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0792, '</span><span class="ex">grad_norm</span><span class="st">': 0.2001953125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019931880491250262, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-312"><a href="#cb12-312" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0869, '</span><span class="ex">grad_norm</span><span class="st">': 0.197265625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019931198377324272, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-313"><a href="#cb12-313" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0849, '</span><span class="ex">grad_norm</span><span class="st">': 0.169921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019930512877009327, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-314"><a href="#cb12-314" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1043, '</span><span class="ex">grad_norm</span><span class="st">': 0.17578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019929823990539174, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-315"><a href="#cb12-315" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0628, '</span><span class="ex">grad_norm</span><span class="st">': 0.1533203125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019929131718148714, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-316"><a href="#cb12-316" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1013, '</span><span class="ex">grad_norm</span><span class="st">': 0.1943359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019928436060073998, '</span><span class="ex">epoch</span><span class="st">': 0.12}</span></span>
<span id="cb12-317"><a href="#cb12-317" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0878, '</span><span class="ex">grad_norm</span><span class="st">': 0.1875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001992773701655224, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-318"><a href="#cb12-318" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0823, '</span><span class="ex">grad_norm</span><span class="st">': 0.1748046875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019927034587821795, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-319"><a href="#cb12-319" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.084, '</span><span class="ex">grad_norm</span><span class="st">': 0.16015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001992632877412219, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-320"><a href="#cb12-320" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0913, '</span><span class="ex">grad_norm</span><span class="st">': 0.1650390625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019925619575694094, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-321"><a href="#cb12-321" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0853, '</span><span class="ex">grad_norm</span><span class="st">': 0.2021484375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001992490699277933, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-322"><a href="#cb12-322" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0723, '</span><span class="ex">grad_norm</span><span class="st">': 0.1982421875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019924191025620877, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-323"><a href="#cb12-323" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0963, '</span><span class="ex">grad_norm</span><span class="st">': 0.236328125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019923471674462875, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-324"><a href="#cb12-324" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0707, '</span><span class="ex">grad_norm</span><span class="st">': 0.1953125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001992274893955061, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-325"><a href="#cb12-325" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0704, '</span><span class="ex">grad_norm</span><span class="st">': 0.2158203125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019922022821130517, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-326"><a href="#cb12-326" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0637, '</span><span class="ex">grad_norm</span><span class="st">': 0.1728515625, '</span><span class="ex">learning_rate</span><span class="st">': 0.000199212933194502, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-327"><a href="#cb12-327" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0749, '</span><span class="ex">grad_norm</span><span class="st">': 0.205078125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019920560434758406, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-328"><a href="#cb12-328" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0842, '</span><span class="ex">grad_norm</span><span class="st">': 0.2421875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019919824167305035, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-329"><a href="#cb12-329" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0794, '</span><span class="ex">grad_norm</span><span class="st">': 0.2138671875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019919084517341145, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-330"><a href="#cb12-330" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0816, '</span><span class="ex">grad_norm</span><span class="st">': 0.19921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019918341485118942, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-331"><a href="#cb12-331" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0912, '</span><span class="ex">grad_norm</span><span class="st">': 0.2177734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019917595070891798, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-332"><a href="#cb12-332" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1035, '</span><span class="ex">grad_norm</span><span class="st">': 0.1875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001991684527491422, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-333"><a href="#cb12-333" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0785, '</span><span class="ex">grad_norm</span><span class="st">': 0.1953125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019916092097441878, '</span><span class="ex">epoch</span><span class="st">': 0.13}</span></span>
<span id="cb12-334"><a href="#cb12-334" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0704, '</span><span class="ex">grad_norm</span><span class="st">': 0.1630859375, '</span><span class="ex">learning_rate</span><span class="st">': 0.000199153355387316, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-335"><a href="#cb12-335" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1205, '</span><span class="ex">grad_norm</span><span class="st">': 0.181640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019914575599041352, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-336"><a href="#cb12-336" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0902, '</span><span class="ex">grad_norm</span><span class="st">': 0.1748046875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019913812278630274, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-337"><a href="#cb12-337" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0841, '</span><span class="ex">grad_norm</span><span class="st">': 0.1806640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019913045577758633, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-338"><a href="#cb12-338" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1027, '</span><span class="ex">grad_norm</span><span class="st">': 0.1904296875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019912275496687874, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-339"><a href="#cb12-339" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0851, '</span><span class="ex">grad_norm</span><span class="st">': 0.1806640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001991150203568058, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-340"><a href="#cb12-340" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0725, '</span><span class="ex">grad_norm</span><span class="st">': 0.263671875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019910725195000485, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-341"><a href="#cb12-341" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.081, '</span><span class="ex">grad_norm</span><span class="st">': 0.193359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001990994497491248, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-342"><a href="#cb12-342" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0643, '</span><span class="ex">grad_norm</span><span class="st">': 0.181640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019909161375682616, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-343"><a href="#cb12-343" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0832, '</span><span class="ex">grad_norm</span><span class="st">': 0.2138671875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019908374397578082, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-344"><a href="#cb12-344" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0708, '</span><span class="ex">grad_norm</span><span class="st">': 0.169921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001990758404086723, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-345"><a href="#cb12-345" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.067, '</span><span class="ex">grad_norm</span><span class="st">': 0.1728515625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019906790305819553, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-346"><a href="#cb12-346" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0874, '</span><span class="ex">grad_norm</span><span class="st">': 0.21875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001990599319270571, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-347"><a href="#cb12-347" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0639, '</span><span class="ex">grad_norm</span><span class="st">': 0.1689453125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019905192701797503, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-348"><a href="#cb12-348" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0857, '</span><span class="ex">grad_norm</span><span class="st">': 0.1923828125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019904388833367882, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-349"><a href="#cb12-349" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0855, '</span><span class="ex">grad_norm</span><span class="st">': 0.177734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001990358158769096, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-350"><a href="#cb12-350" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0867, '</span><span class="ex">grad_norm</span><span class="st">': 0.1884765625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019902770965041992, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-351"><a href="#cb12-351" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0866, '</span><span class="ex">grad_norm</span><span class="st">': 0.2060546875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019901956965697387, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-352"><a href="#cb12-352" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0941, '</span><span class="ex">grad_norm</span><span class="st">': 0.1953125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019901139589934713, '</span><span class="ex">epoch</span><span class="st">': 0.14}</span></span>
<span id="cb12-353"><a href="#cb12-353" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0755, '</span><span class="ex">grad_norm</span><span class="st">': 0.1806640625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001990031883803268, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-354"><a href="#cb12-354" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0792, '</span><span class="ex">grad_norm</span><span class="st">': 0.18359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001989949471027115, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-355"><a href="#cb12-355" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0632, '</span><span class="ex">grad_norm</span><span class="st">': 0.177734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001989866720693114, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-356"><a href="#cb12-356" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0646, '</span><span class="ex">grad_norm</span><span class="st">': 0.1845703125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001989783632829481, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-357"><a href="#cb12-357" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.1032, '</span><span class="ex">grad_norm</span><span class="st">': 0.2177734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019897002074645485, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-358"><a href="#cb12-358" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0795, '</span><span class="ex">grad_norm</span><span class="st">': 0.17578125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019896164446267633, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-359"><a href="#cb12-359" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.052, '</span><span class="ex">grad_norm</span><span class="st">': 0.173828125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019895323443446867, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-360"><a href="#cb12-360" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.072, '</span><span class="ex">grad_norm</span><span class="st">': 0.193359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001989447906646996, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-361"><a href="#cb12-361" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0774, '</span><span class="ex">grad_norm</span><span class="st">': 0.2041015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001989363131562483, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-362"><a href="#cb12-362" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0684, '</span><span class="ex">grad_norm</span><span class="st">': 0.2255859375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001989278019120055, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-363"><a href="#cb12-363" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0632, '</span><span class="ex">grad_norm</span><span class="st">': 0.16015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019891925693487337, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-364"><a href="#cb12-364" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0689, '</span><span class="ex">grad_norm</span><span class="st">': 0.1943359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019891067822776565, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-365"><a href="#cb12-365" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0703, '</span><span class="ex">grad_norm</span><span class="st">': 0.2041015625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001989020657936075, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-366"><a href="#cb12-366" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0784, '</span><span class="ex">grad_norm</span><span class="st">': 0.203125, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001988934196353357, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-367"><a href="#cb12-367" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0832, '</span><span class="ex">grad_norm</span><span class="st">': 0.203125, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019888473975589844, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-368"><a href="#cb12-368" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0645, '</span><span class="ex">grad_norm</span><span class="st">': 0.1630859375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001988760261582554, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-369"><a href="#cb12-369" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0749, '</span><span class="ex">grad_norm</span><span class="st">': 0.1943359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019886727884537778, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-370"><a href="#cb12-370" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0877, '</span><span class="ex">grad_norm</span><span class="st">': 0.171875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019885849782024832, '</span><span class="ex">epoch</span><span class="st">': 0.15}</span></span>
<span id="cb12-371"><a href="#cb12-371" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0722, '</span><span class="ex">grad_norm</span><span class="st">': 0.177734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001988496830858612, '</span><span class="ex">epoch</span><span class="st">': 0.16}</span></span>
<span id="cb12-372"><a href="#cb12-372" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0981, '</span><span class="ex">grad_norm</span><span class="st">': 0.1943359375, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019884083464522212, '</span><span class="ex">epoch</span><span class="st">': 0.16}</span></span>
<span id="cb12-373"><a href="#cb12-373" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0957, '</span><span class="ex">grad_norm</span><span class="st">': 0.1962890625, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019883195250134823, '</span><span class="ex">epoch</span><span class="st">': 0.16}</span></span>
<span id="cb12-374"><a href="#cb12-374" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0924, '</span><span class="ex">grad_norm</span><span class="st">': 0.201171875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019882303665726828, '</span><span class="ex">epoch</span><span class="st">': 0.16}</span></span>
<span id="cb12-375"><a href="#cb12-375" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0593, '</span><span class="ex">grad_norm</span><span class="st">': 0.1669921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001988140871160223, '</span><span class="ex">epoch</span><span class="st">': 0.16}</span></span>
<span id="cb12-376"><a href="#cb12-376" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0666, '</span><span class="ex">grad_norm</span><span class="st">': 0.1650390625, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001988051038806621, '</span><span class="ex">epoch</span><span class="st">': 0.16}</span></span>
<span id="cb12-377"><a href="#cb12-377" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0799, '</span><span class="ex">grad_norm</span><span class="st">': 0.177734375, '</span><span class="ex">learning_rate</span><span class="st">': 0.0001987960869542508, '</span><span class="ex">epoch</span><span class="st">': 0.16}                </span></span>
<span id="cb12-378"><a href="#cb12-378" aria-hidden="true" tabindex="-1"></a><span class="st">{'</span><span class="ex">loss</span><span class="st">': 0.0625, '</span><span class="ex">grad_norm</span><span class="st">': 0.19921875, '</span><span class="ex">learning_rate</span><span class="st">': 0.00019878703633986294, '</span><span class="ex">epoch</span><span class="st">': 0.16}                </span></span>
<span id="cb12-379"><a href="#cb12-379" aria-hidden="true" tabindex="-1"></a><span class="st">  5%|███▌                                                                | 287/5400 [2:07:55&lt;31:19:58, 22.06s/it]^CTraceback (most recent call last):</span></span>
<span id="cb12-380"><a href="#cb12-380" aria-hidden="true" tabindex="-1"></a><span class="st">  File "/root/miniconda3/envs/py3.10/bin/accelerate", line 8, in &lt;module&gt;</span></span>
<span id="cb12-381"><a href="#cb12-381" aria-hidden="true" tabindex="-1"></a><span class="st">    sys.exit(main())</span></span>
<span id="cb12-382"><a href="#cb12-382" aria-hidden="true" tabindex="-1"></a><span class="st">  File "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 46, in main</span></span>
<span id="cb12-383"><a href="#cb12-383" aria-hidden="true" tabindex="-1"></a><span class="st">    args.func(args)</span></span>
<span id="cb12-384"><a href="#cb12-384" aria-hidden="true" tabindex="-1"></a><span class="st">  File "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1082, in launch_command</span></span>
<span id="cb12-385"><a href="#cb12-385" aria-hidden="true" tabindex="-1"></a><span class="st">    simple_launcher(args)</span></span>
<span id="cb12-386"><a href="#cb12-386" aria-hidden="true" tabindex="-1"></a><span class="st">  File "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/commands/launch.py", line 685, in simple_launcher</span></span>
<span id="cb12-387"><a href="#cb12-387" aria-hidden="true" tabindex="-1"></a><span class="st">    process.wait()</span></span>
<span id="cb12-388"><a href="#cb12-388" aria-hidden="true" tabindex="-1"></a><span class="st">  File "/root/miniconda3/envs/py3.10/lib/python3.10/subprocess.py", line 1209, in wait</span></span>
<span id="cb12-389"><a href="#cb12-389" aria-hidden="true" tabindex="-1"></a><span class="st">    return self._wait(timeout=timeout)</span></span>
<span id="cb12-390"><a href="#cb12-390" aria-hidden="true" tabindex="-1"></a><span class="st">  File "/root/miniconda3/envs/py3.10/lib/python3.10/subprocess.py", line 1959, in _wait</span></span>
<span id="cb12-391"><a href="#cb12-391" aria-hidden="true" tabindex="-1"></a><span class="st">    (pid, sts) = self._try_wait(0)</span></span>
<span id="cb12-392"><a href="#cb12-392" aria-hidden="true" tabindex="-1"></a><span class="st">  File "/root/miniconda3/envs/py3.10/lib/python3.10/subprocess.py", line 1917, in _try_wait</span></span>
<span id="cb12-393"><a href="#cb12-393" aria-hidden="true" tabindex="-1"></a><span class="st">    (pid, sts) = os.waitpid(self.pid, wait_flags)</span></span>
<span id="cb12-394"><a href="#cb12-394" aria-hidden="true" tabindex="-1"></a><span class="st">KeyboardInterrupt</span></span>
<span id="cb12-395"><a href="#cb12-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-396"><a href="#cb12-396" aria-hidden="true" tabindex="-1"></a><span class="st">root@6df7cfbf0d81:~/axolotl/hc-mistral-alpaca# /root/miniconda3/envs/py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.</span></span>
<span id="cb12-397"><a href="#cb12-397" aria-hidden="true" tabindex="-1"></a><span class="st">  warnings.warn(</span></span>
<span id="cb12-398"><a href="#cb12-398" aria-hidden="true" tabindex="-1"></a><span class="st">  5%|███▌                                                                | 287/5400 [2:08:21&lt;38:06:49, 26.84s/it]</span></span>
<span id="cb12-399"><a href="#cb12-399" aria-hidden="true" tabindex="-1"></a><span class="st">^Cndb: - 0.011 MB of 0.011 MB uploaded</span></span>
<span id="cb12-400"><a href="#cb12-400" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: / 0.054 MB of 0.054 MB uploaded-alpaca# wandb: \ 0.011 MB of 0.054 MB uploaded</span></span>
<span id="cb12-401"><a href="#cb12-401" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: Run history:</span></span>
<span id="cb12-402"><a href="#cb12-402" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:               eval/loss ▁</span></span>
<span id="cb12-403"><a href="#cb12-403" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:            eval/runtime ▁</span></span>
<span id="cb12-404"><a href="#cb12-404" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: eval/samples_per_second ▁</span></span>
<span id="cb12-405"><a href="#cb12-405" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:   eval/steps_per_second ▁</span></span>
<span id="cb12-406"><a href="#cb12-406" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:             train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</span></span>
<span id="cb12-407"><a href="#cb12-407" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:       train/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</span></span>
<span id="cb12-408"><a href="#cb12-408" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:         train/grad_norm █▆▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</span></span>
<span id="cb12-409"><a href="#cb12-409" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:     train/learning_rate ▁▃▆█████████████████████████████████████</span></span>
<span id="cb12-410"><a href="#cb12-410" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:              train/loss █▅▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</span></span>
<span id="cb12-411"><a href="#cb12-411" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: </span></span>
<span id="cb12-412"><a href="#cb12-412" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: Run summary:</span></span>
<span id="cb12-413"><a href="#cb12-413" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:               eval/loss 1.19008</span></span>
<span id="cb12-414"><a href="#cb12-414" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:            eval/runtime 1342.7584</span></span>
<span id="cb12-415"><a href="#cb12-415" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: eval/samples_per_second 9.533</span></span>
<span id="cb12-416"><a href="#cb12-416" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:   eval/steps_per_second 0.596</span></span>
<span id="cb12-417"><a href="#cb12-417" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:             train/epoch 0.15944</span></span>
<span id="cb12-418"><a href="#cb12-418" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:       train/global_step 287</span></span>
<span id="cb12-419"><a href="#cb12-419" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:         train/grad_norm 0.19922</span></span>
<span id="cb12-420"><a href="#cb12-420" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:     train/learning_rate 0.0002</span></span>
<span id="cb12-421"><a href="#cb12-421" aria-hidden="true" tabindex="-1"></a><span class="st">wandb:              train/loss 0.0625</span></span>
<span id="cb12-422"><a href="#cb12-422" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: </span></span>
<span id="cb12-423"><a href="#cb12-423" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: 🚀 View run lyric-wildflower-5 at: https://wandb.ai/law/hc-axolotl-mistral/runs/iewv47f2</span></span>
<span id="cb12-424"><a href="#cb12-424" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: ⭐️ View project at: https://wandb.ai/law/hc-axolotl-mistral</span></span>
<span id="cb12-425"><a href="#cb12-425" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: Synced 6 W&amp;B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)</span></span>
<span id="cb12-426"><a href="#cb12-426" aria-hidden="true" tabindex="-1"></a><span class="st">wandb: Find logs at: ./wandb/run-20240524_000556-iewv47f2/logs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lawwu\.github\.io\/blog\.html");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="lawwu/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">

<div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>