---
title: "KDD - Day"
author: "Lawrence Wu"    
date: "2023-08-08"
categories: ["KDD"]
draft: true
---

Notes from Ed Chi's keynote at KDD 2023.

# 8 year cycles
- 1991 - invention of web browser
- 1999 - invention of Google and search
  - indexed the world's books
- 2007 - invention of mobile phone
- 2016: Functions that Deep Neural Network Can Learn
  - Text: Humans can be augmented by statistical machine translations
  - Image --> Caption: Put in an image and it would generate salient captions. This was an incredbile breakthrough since no specific instructions were given but accurate captions were **generated**.
  - How many people recognized in 2016 that there was going to be a generative AI cycle.

- By 2016, image recognition by machines/AI was already super-human, 3% error rate vs 5% error rate
- What changed about deep learning to make it a useful method?
- Getting comfortable with revolutions: as humans we couldn't fly 100 years ago. But now, we get delayed 30 minutes and get annoyed.
  - Arthur C Clarke quote
  - "Any commonplace technology is not magic!" - Ed Chi's corollary

#  LLM-based chatbots and asssistants 

- Timeline of how we got here
  - 2014: sequence to sequence learning with neural networks
  - 2017: Attention is all you need
  - 2020: Towards a Human-like Open Domain Chatbot
  - 2022: LaMDA: Language Model for Dialogue Applications
  - 2022-01: Chain-of-Thought Prompting Eliciits Reasoning in LLMs
TODO: See picture for other things

- Multi-task language models
- LaMDA
  - Large model, up to 137B parameters
  - Fine-tuned for sensibility, specificity, interestingness, safety, factuality
  - Foreshadowed Bard

- Possible soon: Everyone can have their own personal assistant that is not merely transactional but understands contexts.
- Bard
  - With Bard, we realized LLMs could begin to do planning, "Help me design a plan to read 20 books in a year"
  - No longer just a cute assistant but they could be helpful assistants in a wide variety of topics
  - Improved multilingual understanding, understanding idioms. Explain idioms and why they can be misunderstood.
  - Coding capabilities: more surprising to Ed. 
    - explain JAX, explain the code within the google/github repo
    - can you fix the code with a bug and add line by line comments in Korean
    - Learning about Double Machine Learning
  
- Insight (Data + Data Efficiency is the key to Conversational AI)
  - Pre-training (1B examples, 1T tokens)
  - Fine-tuning (10k examples)
  - Prompting (1 example)
    - small changes, micro-prompting can lead to big changes in the text that is generated. there is a parallel to human language where small changes to what you say can elicit different responses, e.g. adding "please" and "thank you"

# Tool-use: Retrieval-Augmnettation and Multi-modality in LLMs

- Limitations of LLMs
- Retrieval-Augmentation: Leveraging External Knowledge
  - TODO: insert image
  - Humans learned how to use tools like Google, how to craft the right search query to get the right results
  - We're teaching an LLM how to use tools like Google
- RETRO: Retrieval-augmneted generative model. 
  - the generator processses the question and the retrieved docs/passages separately
- Multi-modality output
  - Query generation may call an image service or an image generation service
  - Image input and output
- Coming Soon: Tool-use App Integration
  - 1st party tools from Google
  - 3rd party tools: Adobe Firefly to generate images, Uber, shopping service

# Augmneted Intelligence

- Human intelligence is augmented by search (needed two revolutions: mobile and search)
- LLMs + search/tools --> Super LLMs
- Humans + Super LLMs --> Super super humans?!

# Reasoning

## Human Intelligence vs Machine Learning 

- Denny Zhou: Differences between how humans and machines learn.
- Humans learn from only a few examples while machine learning needs tons of laebled data to train a model
- Attempts to fill the gap
TODO: See slide

## Can we teach LLMs like we teach kids?

- hypothesis on how to improve reasoning

### Chain-of-Thought = "explanation" + "answer"

- 2022 paper
- Brekathrough capabilities: Reasoning tasks
  - Standard prompting
  - Chain of thought prompting - adding an example of an explantation/answer to the prompt allows the LLM to replicate that reasoning capabilities

### Self-conssitency decoding

- Paper: Self-Consistency Improves Chain of Thought Reasoning in Language Models
- if you create diverse reasoning paths with different temperatures and then took a majority vote, this would lead to better performance

### Least-to-most Prompting

- Paper: Least-to-Most...
- Decompose question into subquestions
- Sequentially solve subquestions

### Instruction Finetuning: Enables zero-shot prompting

- ML: Data --> Prediction
- Instruction fine-tuning: Instruction --> Answer
- FLAN Instruction tuning
  - Fine-tunes model on a large set of varied instructions that use a simple and intuitive description of the task
  - instruction tuning improves performance on unseen tasks only for models for certain sizes (68B)

### FLAN2: Finetune with 1800+ tasks, bigger models

### Reasoning Summary

TODO: See slide

# Future Challenges for LLMs

Interesting he made a comment, thanking OpenAI for releasing ChatGPT because that was the impetus for Google's LaMDA team being able to release Bard into the marketplace. As an outisder, it's fascinating thinking about what those internal discussions must've been like.

## Responsibility and Safety

- Constitutional AI - instruction tuning is one way to help keep LLMs safe

## Factuality, Grounding, and Attribution
- Grounding and Attribution of Answers
- RARR: Retrofit Attribution using Research and Revision

## Human <-> AI Content Loop and Ecosystem
- Many reasons for identifying machine-generated content - e.g. avoid loops of using LLM-generated examples for future LLMs
- Humans are not good at detection LLM-generated content (Ironic from a Turing test POV)

## Personalization and User Memory

- We want the LLM experience personalized to you, understand your needs, and respect your privacy
- Serving efficiency
  - Low rank models
  - If he was a PhD student, he would invest in solving this problem. Can be very profitable amking models more efficient.
- Pigeonhole problem: aoiding over memorization of your preferences

# Conclusion

- History tells us we were due for a revolution and LLMs first applications are chatbots because LLMs understanding converstional context better
- Just like humans are empowered by tools and reasoning capabilities, **LLMs are now augmented with tools and capable of reasoning via language**

# Q&A

- How large do foundation models need to be?
  - Base models need to be sufficiently large or "double-digit billions". At this size, it is on the order of 1 cent per query
  - Costs are continuing to drop
- What are your thoughts about security/privacy with LLMs?
  - This is important because you don't want user preferences bleeding into other users
  - Differential privacy techniques can be applied
  - Localize some parameter changes in a separate tower that can be added to the base model
- When will LLMs develop causal reasoning capabilities?
  - One derivation from the Chain of Thought paper is that LLMs can do causal reasoning. The LLMs can explain themselves. 
- As data scientists, we are used to building specialized models and we have control over the parameters. With LLMs, the paradigm has shifted where we have to trust these pre-trained models. What are your thoughts?
  - Our ability to trust an LLM is going to take a human flavor. You build trust over time with a human. But a human usually cannot fully explain how they came to that decision.
  - You can interrogate these models for how it came up with these answers.
- How do we continue to be able to train models on human-generated data? 
  - Watermarking
    - for images is easier
    - for text - there is more sparsity
    - more pessismistic about watermarking being the solution to this problem
  - Have not heard a single proposal that is promising
  - Turing Award for work here...
- What abilities will humans retain that are superior than LLMs?
  - For the next few years at least, humans have a physical body
  - Reasoning capabilities: still some gaps between humans and LLMs
  - From the other side, machines never get tired
  - Remember 
  - Humans have a spiritual component. A machine or LLM will never have a spiritual component or a soul.
- Thoughts on evaluation of these LLMs? How can we differentiate between models?
  - In developing LaMDA and Bard, how do we build an evaluation framework to take the technology.
  - Defining three categories: quality, factuality/groundedness, safety/persona. In these 3 areas, developed sub-metrics in all of these. Spent a lot of money and time on each of these areas. 
    - Creativity: writing a poem, summarizing documents


# Large Language Model Day

<https://bigmodel.ai/llmday-kdd23/>

# Schedule

| Time         | Event                                              | Speaker/Details                                                                                                 |
|--------------|----------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
| **Date:**    | **Aug. 8, 2023**                                   | **Room:** Grand A                                                                                               |
| 10:00-12:00  | Distinguished Keynotes                             |                                                                                                                  |
| 10:00-10:10  | Opening Remarks                                    | Jie Tang (Tsinghua University)                                                                                   |
| 10:10-11:00  | Jaime Teevan (Microsoft)                           | From Documents to Dialogues: How LLMs are Shaping the Future of Work                                             |
| 11:00-11:50  | Denny Zhou (Google DeepMind)                       | Teach language models to reason                                                                                  |
| 12:00-13:30  | Lunch                                              |                                                                                                                  |
| 13:30-15:30  | Keynotes                                           |                                                                                                                  |
| 13:30-14:10  | Vedanuj Goswami (Meta FAIR)                        | Llama 2: Open Foundation and Fine-Tuned Chat Models                                                              |
| 14:10-14:50  | Peng Zhang (Zhipu AI)                              | From GLM-130B to ChatGLM                                                                                         |
| 14:50-15:30  | Jason Wei (OpenAI)                                 | The large language model renaissance: paradigms and challenges                                                   |
| 15:30-16:00  | Coffee Break                                       |                                                                                                                  |
| 16:00-17:30  | Panel: Paradigm Shifts in the Era of LLMs          | Opportunities and Challenges in Academia, Industry, and Society. Moderator: Qiaozhu Mei (University of Michigan)  |
|              |                                                    | Invited Panelists: Ed Chi (Google DeepMind), Vedanuj Goswami (Meta FAIR), Jaime Teevan (Microsoft), Vy Vo (Intel Labs), Denny Zhou (Google DeepMind) |


# Opening Remarks: Jie Tang (Tsinghua University)

- Opportunity to KDD: Unified model for all data (text, DB, image, multimedia)
- Opportnity to Academia: 
  - Before we categorize our research vertically
  - Next, we may need to reorganize our research horizontally
- Opportunity to startup:
  - LLM offers a chance to implement AGI
  - Vision of a new startup is not necessary to answer "how", it is more about "why"?

# From Documents to Dialogues: How LLMs are Shaping the Future of Work - Jaime Teevan (Microsoft)

How we share and disseminate knowledge

- Also going to mirror's Ed's talk where Jaime is going to share about a history.
- How the future of work is going to change and how the history of academic research
- Pursuit of GDP Growth [slide]
- "The Current Moment"
- The search engine was the first AI-scale application. Enabled by the cloud.
  - Worked at Infoseek in the early days
  - Had 4 kids during this time (oldest was born in 2004, oldest in 2008)
  - In those 4 years: Both cloud, edge devices (mobile) and social networks (create information at scale) came into existence. Mechnical Turk was also created. The combination of those things created Imagenet, which led to much development.
  - Have to be able to make use of fragmented time and attention. Her focus of research. Micro-productivity and how people work with AI.
- Worked at Microsoft Research after doctorate
  - Information retrieval
  - Worked with Satya Nadella as his technical advisor
- Now here role is to think about how to drive disruption, bring research into products
  - in the midst of this was the pandemic/COVID.
- AI is more of an internal disruption than an external one
  - Spotify: "Today has been a great year in AI"
- Fall 2022: Meeting with Sam Altman getting to try out GPT-4 for the first time. Had to drive to campus to try it out because it was so secretive.
  - Your job is to get this into all of Microsoft's products
  - Driving home, had to pull over and scream because it was so amazing

## Enterprise Grade AI
taking the world's best language models and making them work for the enterprise involves these 4 things:
  - Global Scale
  - Grounded in your Data
  - Trustworthy
  - Embedded in Existing Workflows

- Global Scale: Multilingual
  - shipped M365 copilot in all tier 1 languages because of language model's ability to translate
  - translating through EN improves performance for some languages but not for others
    - high resource languages tend to do better without neeeding to go through English
    - non-latin scripts tend to have additional challenges
- Global Scale: Efficiency
- **Just like how everyone uses the internet, everyone will use language models**
- Global Scale: Sustainable
  - Underwater data centers are a thing!
- Grounded in your data: RAG [slide]
  - search over knowledge and new types of data: documents, chat history, application context
  - grounded in the context:
    - word document
    - chat history
    - transcript
- Grounded in your data: Private by design [slide]
- Trustworthy: Differential privacy
  - Better privacy/utility trade-off: similar accuracy for private vs. non-private fine-tuning
  - Privacy episilon?
- Trustworthy: Measuring privacy
  - Combining DP and PII scrubbing effictively eliminates the risk of privacy
- Trustworthy: Responsible
  - requires layers of mitgation
  - LLMs foreground new challenges
    - Hallucination and errors
    - Jailbreaks and prompt injection
    - Harmful content and code
    - Manipulation, human-like behavior
  - Enterprise vs. consumer context
- Embedding in Existing Workflows: ODSL
  - Office Domain Specific Language
  - Excel is Turing Complete!
    - As soon as you can translate natural language into a DSL, you unlock a lot of value
    - Excel + LLMs

## Knowledge in Conversations

- Grounding vs. Grounded
  - After grounding, we create grounded content
  - In the LLM context: the first few prompts are the process of grounding
  - Microsoft is a document company - LLMs can extract knowledge from those documents. 
    - Not just facts
    - Style
    - Structure
- Collective Intelligence - how to capture all the knowledge across a community like KDD and synthesize it
  - Analogy: maps. Structured data. Allows us to navigate the world and create geopolitical boundaries. Google Maps has centralized a lot of this knowledge. 
  - Chat Log Analysis
    - Come up with Prompting Do's and Don'ts
- Prompt Support: Creating the LLM "Ribbon"
  - support learning: identify and surface tips in situ

## Lead like a Scientist

## Q&A

- What do you foresee about the future of remote work?
  - Research shows in person work is valuable
  - Language models may help people access fast real-time institutional knowledge 
- Once we have products built on top of LLMs, how do you see the future of development?
  - A lot of this will be emergent behavior
  - With Twitter/X, they didn't invent hashtags
- What do you see the challenges of ethics by design?
  - ?
- How do you audit a model whether you are meeting those metrics?
  - For Microsoft products: there are metrics tied to each guideline in their Responsible AI guidelines
- Can you comment on the future of LLMs in the context of open source? As scientists, how can we validate models that are closed?
  - Human parity has been our measure for so long, what's next?
  - This is something we have to figure out.
  - Source selection for RAG
  - Model selection for efficiency is important
- How do you think about security at Microsoft?
   - Red-teaming for security and ethics
   - Interesting: a lot of models are used in red-teaming efforts.

  # Teach language models to reason (Denny Zhou (Google DeepMind))

  - Leads the Reasoning team at Google DeepMind
  - What do you expect from AI?
  - My little expectation on AI
  - Does ML meet this expectation?
    - Semi-supervised learning
    - etc.
  - What is missing in ML?
    - Reasoning 
    - Humans can learn from a few examples because humans can reason
  - Teach LLMs to reason like we teach kids
  - Toy problem: concaenate the last letter of each word
    - Elon Musk --> nk
    - Bill Gates --> ls
    - Solve it by machine learning
    - Solve it by LLMs
  - Training an LLM
    - You can think of training an LLM like training a parrot to mimic human language
    - Few-shot prompting for last-letter concatenation
      - give it the two exammples and one input to get the answer
- Why we created the last-letter-concatenation task?
  - Make ML fail
  - Make few-shot prompting
  - But trivial for humans
- Chain of thoguht prompting
  - Adding "thought" before "answer"
  - End of 2021 - discovered this
- One demonstration is engouh, just like humans
- Standard few-shot prmopting vs. Chain-of-thought prompting
- Can LLMs solve math word problems?
- Apply CoT to any task
  - all tasks can be solved by CoT without machine learning
  - 100x-1000xdata efficient than supervised SoTA in literature - only need 1-2 examples!
- Multilingual CoT
- Apply CoT to solve BIG-Bench Hard
- "Thought" does NOT have to be "step by step"
- Self-consistency decoding - greatly improves chain-of-thought decoding
  - prompt a language model using example chains of thought
  - sample from the LLM decoder to generate a diverse set of reasoning paths
  - choose the most consistent answer using the majority vote
  - crushed GSM8K SoTA with only 8 examples
- How many more examples are needed for finetuning to be comparable to CoT + SC?
  - From original paper: two additional orders of magnitude of training data to reach an 80% solve rate
  - But with only 8 examples, the model can be taught reasoning
- Solve high school math problems
  - Fine-tuning PaLM with math data
  - SC + CoT solves 50%
  - non-math grad students solve 40%
- Motivation to SC decoding
  - Answer in the greedy ouput from CoT DOES NOT EQUAL the most likely answer
  - SC leads to the most likely answer
    - The full probability of each answer is computed by summing over all reasoning paths (margialize the reasoning paths)
    - Implementation via sampling: sample multiple times, then choose the most common answer
  - Implications from the probabilistic explanation

## Least-to-most prompting

Enables easy-to-hard generalization

- CoT fails to generalize to harder problems
- Least-to-most prompting = Planning + Reasoning 
  1. Decompose a complex prolem into a list of easier subproblems
  2. Sequentially solve these subproblems
- Solve math word problems by least-to-most prompting 
- How does a LLM plan and rank tasks easy to hard?
- Can use this for common sense reasoning
- Can use this for solving math word problems.
  - Did Aristotle use a laptop?
  - Are chincillas cold-blooded?
- Last-letter task generalization (still not perfect)
- SCAN (compositional generalization): text to actions
  - 100% accuracy using least-to-most
- CFQ (compositional generalization): text to code
  - Using 1% of the data, crush SoTA results
- Post on compositional generalization: <https://ai.googleblog.com/2020/03/measuring-compositional-generalization.html>

## LLMs for Code

How to generate high-quality code?

- Teaching LLMs to Self-Debug
  - self-debug to generate higher quality code
- Large Language Models as Tool Makers
  - Paper: <https://arxiv.org/abs/2305.17126>
  - One way:
    - Reduce serving costs using distillation or quantization
    - For most models we use a small model
    - For some models we use a large model
  - Use a few instances to make a tool --> reuse the tool to solve similar instances
    - Analogy of deep learning libraries - talented programmers develop pytorch, practioners `import torch` to solve their problems

## Common big prompt for any task?

Yes!

- Key idea: making a big prompt by combining prompts from different tasks, and then using it for any task
- Magic: any task: including tasks which are not even seen
- Implementation: Too 

## Instruction Tuning

- Enable zero-shot prompting in any task
- Pioneered by FLAN and T0
- Store the large prompts in model weights
- Example of parsing all names from this message, then sort and add Quoc Le to the list.
- Why does this work?
  - What I cannot create, I do not understand - Feynmand
  - We know how to create LLMs but do not know how they work
- Emergent properties
  - All these are emergent properties
  - Emergent properties are discovered, not designed by LLM builders
- How to make parrots intelligent? Scaling up!
- Toward understanding in-context learning
  - Paper: [What learning algorithm is in-context learning? Investigations with linear models
](https://arxiv.org/abs/2211.15661)
  - learned models are encoded in activations!

## Smmmary
[slide]

- These ideas are trivial if LLMs are humans

# Llama 2: Open Foundation and Fine-Tuned Chat Models - Vedanuj Goswami (Meta FAIR): 

- Pretraining
- Finetuning (SFT and RHLF)
- Safety

## Pretraining

- 2T tokens, 40% more than LLaMA1 
- 1.5x-7x more compute than LLaMA1
- grouped query attention
- scaling training beyond 2K GPUs
- GQA was only used for the 34B and 70B models
- More compute and longer training - don't see model saturation yet. Can train for model to see better performance.
  - Training optimal: Given a given amount of compute
  - Inference optimal: more in the regime of inference optimal
- Long context pretraining (2k to 4k)
  - context length to use in pretraining is determined by the pretraining data distribution
  - when you have longer context length, the kv-cache increases a lot and it becomes infeasible to have larger batches
  - GQA - 8 heads vs. 1 head in MHA (multi-head attention)
- Parallelism

## Pretrained Model Evaluation
  - on par or better with open source models
  - still gap with closed source models

## Fine-tuning

- SFT or Instruction tuning - similar to the point Denny made in his talk, this allows you to encode instructions/prompts within the model itself
  - 3rd party datasets lack diversity and quality
  - focus on fewer but clean instruction-tuning data for higher quality models
  - collected about 27k samples
  - SFT model output often matched or outperformed human annotated data. So they shifted to getting human preference data with the remaining budget.
- Human preference data
  - 1.4M samples collected
  - Compared to other open source datasets like Anthropic, OpenAI, StackExchange etc., their samples were longer
- Reward Modeling - want the model to helpful and harmless
  - usually these rewards are in conflict so they decided to train two separate models
- Iterative Finetuning with RLHF
  - 5 total iterative loops
  - Two approaches: Proximal Policy Optimization, Rejection sampling. The 5th iterative loop was done with PPO AND rejection sampling.
- Favorable comparison win-rate vs ChatGPT on helpfulness and safety
- HumanEval results compared to other open source models on single turn and multi-turn prompts

## Safety

- Impact of Safety RHLF - improvement in safety without sacrificing helpfulness
- Context Distillation
  - Generate safety pre-prompts using various adjectives associated with safe behavior like responsible, respectful, wise
  - prefix a safety pre-prompt to adversarial prompts
- Safety Evaluation
  - Multi-turn evaluation generates more unsafe responses

## Some interesting observations

- dynamic rescaling of temperature contingent upon the context for creative vs. factual prompts. For factual prompts, model provides same response inspite of rising temperature
- models show emergent behavior to organize knowledge by time
- tool usage also emerges

## Q&A

- How did you ensure the training was stable? The training curves are smooth?
  - Depends on calibrating the learning rate and the quality of the data. Bad batches of data lead to spikes.
- How to make these models learn continually?
  - Inspired by Denny's talk, if we can improve it's reasoning capabilities, then it may just need a few prompts to generalize to new unseen tasks.
- What's your opinion on these two steps of SFT vs. RHLF?
  - Once models get to a certain size, the model gets better than humans at generating SFT-like responses
  - It's easier for humans to compare two responses than to generate a response (gives the model an idea of human preferences)
- How long did it take to train your models?
  - 45 days to train 70B on 2,000 GPUs

# From GLM-130B to ChatGLM - Peng Zhang (Zhipu AI)

- teaching machines to think like humans
- all-in on LLMs, 400 people working on this
- Zhipu's GLM models vs OpenAI GPT models
  - GLM (auto-regressive blank-filling) vs. GPT (generative pre-training)
  - Tokens: a mix of English and Chinese models
- General Language Model (GLM)
- Training stability
  - Tradeoff between stability and efficiency

# The large language model renaissance: paradigms and challenges - Jason Wei (OpenAI)

- Timeline
  - 2018 - BERT
  - 2023 - ChatGPT - ask in natural language
  - Contrast between the two is night and day
- What is legacy thinking? How can we adapt to the new way of thinking?
- Outline
  - Scaling Laws
  - Emergent abilities
  - Reasoning via prompting
  - How they work technically and how it affects AI work.
- Scaling Laws
  - Palm2 more than 1 mole (6e23) of flops
  - Scaling is hard and was not obvious at the time
   - Technical challenges
   - Psychological challenges
  - Scaling predictabily improves performance ("scaling laws")
    - Test Loss vs. Compute
    - Increase compute --> loss is supposed to go down smoothly
    - You should expect to get a better language model as you scale compute
    - Spans 7 orders of magnitude
  - Scaling laws: certain metrics can be very predictable
  - Changes in the nature of AI work: scaling laws
    - 5 years ago: Many individual or small-scale projects, bottom-up research culture, run the code once; then submit to NeurIPS
    - Technical paradigm shift (b/c/ training the best models require scaling compute and data)
    - Now: Teams usually have dozens of people, everyone works together toward one focused goal (top down set?), tooling and infra matter a lot (increased value in being a good software engineering)
  - 202 downstream tasks in BIG-Bench
    - Smoothly increasing (29%) - small tasks
    - Flat (22%)
    - Inverse scaling (2.5%)
    - Not correlated with scale (13%)
    - Emergent abilities (33%) - flat for awhile
- Emergence in science
  - emergence is a qualitative change that arises from quantitative changes (aka phase shifts)
  - popularized by this 1972 piece by Nobel-Prize winning physicist
    - with a bit of uranium
    - with a bit of calcium...
  - emergence in large language models
    - an ability is emergent if it is not present in smaller models, but is present in larger models
  - Emergence in few-shot prompting: examples
    - performance is flat for small models
    - performance spikes to well above-random for large models
  - Emergence in prompting: example
    - Simple translation task
  - Implications of Emergence
    - there is an area of emergent abilities that can be "unlocked" with larger models
  - 3 implications of Emergence
    - Unpredictable
    - Unintentional - not specified by trainer of the model
    - One model, many-tasks
    - Suggested further reading: Emergent deception and emergent optimization
  - Changes in the nature of AI work: emergent abilities
    - 5 years ago
      - a few benchmarks for many years (CIFAR, ImageNet)
      - easy to rank models
      - task-specific architectures, data and protocols
    - Technical paradigm shift: a single model performs many tasks without the tasks being explicitly specified at pre-training
    - Now
      - Need to create new benchmarks all the time
      - Hard to decide if one model is universally better
      - Create general technology; relatively easy to pivot


## Reasoning 

- What is the difference between human intelligence and machine learning?
- Chain-of-thought prompting
- CoT itself requires scaling
  - For small models, CoT hurts performance
  - For large models, CoT helps performance
  - Why?
- Least-to-most prompting
  - the potential of problem decompsition
    - write a research proposal about the best approaches for aligning a super-intelligent artificial intelligence
- Changes in the nature of AI work: reasoning via prompting
  - 5 years ago:
    - Type 1 tasks: easy to evaluate, debug models
    - Task specification via training data and protocols
    - Black magic of AI = hyperparameter tuning
  - Technical paradigm shift: LLMs can perform multi-step reasoning via prmpting
  - Now
    - Type 2 tasks: harder to evaluate / debug models
    - Task specificiation via natural language prompt
    - Black magic of AI = prompt engineering
  - Some potential research directions
    - Evaluation
    - Factuality 
    - Multimodality
    - Tool use
    - Super-alignment

## Q&A

- Can LLMs be used for game theory, forecasting?
  - Humans are bad at forecasting, but it may be possible for LLMs to have this behavior?
- What are your thoughts on serving models cheaper?
  - Chinchilla scaling laws - use more data to train for less data.
- What are your thoughts on models scaling and getting improved performance?
  - Based on current scaling laws we probably won't see model saturation for awhile
  - Open source community
- What about emergent abilities that are harmful that we can't see? Is it a game of cat and mouse?
  - Great blogpost on emergent deception
  - These are things that he's looking into on the "super-alignment" team
- How many papers do you read per day? How do you keep up?
  - I don't read any papers per day.
  - Fortunately works with a lot of people who work in AI
  - Uses Twitter to find what's interesting
- What about tabular data? Are LLMs good for using tabular data?
  - ChatGPT already is pretty good working with tabular data (Code Interpreter)
- Can you say anything about GPT5?
  - No.
- How did you come up with chain-of-thought?
  - Really interested in meditation. Stream of consciousness was a thing in meditation. LLMs can produce stream of consciousness. Asked if LLMs could solve word math problems and he applied stream of consciousness. Originally thought it could be called stream-of-thought but sounded informal so he called it chain-of-thought.

## Thoughts

- Jason talked about how LLMs will affect AI Research (his forte), but how does LLMs affect any profession?

# Paradigm Shifts in the Era of LLMs: Opportunities and Challenges in Academia, Industry, and Society.

- Panelists: Ed Chi (Google DeepMind), Vedanuj Goswami (Meta FAIR), Jaime Teevan (Microsoft), Vy Vo (Intel Labs), Denny Zhou (Google DeepMind)
- What's one perspective on LLMs you have?
  - Denny Zhou: Happy that many people know the importance of reasoning. Reasoning is all you need.
  - Vy Vo: Comparing human brains vs. LLMs. Comparative intelligence approaches will be more important goign forward.
  - Ed: Has a background in HCI and cognitive science
  - Vedanuj: On the panel because he helped training LLaMA2 models. 
  - Jaime: On the panel because she brings organizational diversity. Knowledge is increasingly embedded in conversations. How do we setup people to have successful conversations?
- Some researchers in certain fields fear their fields will be eliminated because of LLMs. What fields will disappear?
  - Lawrence: There's an analog to work/jobs
  - Jaime: Should be excited to reimagine how research is conducted. How knowledge is disseminated. We have an amazing new tool on our hands now. 
  - Ed: People working on methods are more likely to be out of a job, people working on data are more likely to be safe. 40-50% of questions touched on knowledge graphs and injecting them into LLMs - this community spent years on turning unstructured data into a knowledge graph. But a LLM is probably more capable than a knowledge graph. Turn knowledge graphs back into text that can be used to train LLMs. 
  - Vy: Information is represented in multiple forms. Don't think that language is the only useful way to represent data/information. 
    - We still don't understand how LLMs are doing what they are doing
    - Emerging field to understand machine intelligence. Will be able to steer it. 
    - Grapple with the comparative approach.
  - Denny: LLMs are much weaker than what we thought. 
    - Inconsistency: GPT-4 will make mistakes on elementary school math problems. 
    - Where does reasoning come from? If it doesn't come from language, can we use synthetic language to understand reasoning.
- What are some research directions to understand how LLMs are working?
  - Ed
    - Geoff Hinton said if an ML is making a good prediction, why do we care how he or she is making it?
    - Humans are a black box. Cogitive science - humans make decision and justify their decisions post-hoc.
    - You can interrogate the LLM directly for why it made a prediction/decision. 
  - Vy: Deep learning framework for neuroscience. All animal brains are black boxes. Paper said what is being optimized for in the biological brain and AI systems. Look at the loss function that the model is being trained on.
- Is it responsible for human beings to use that much energy? What is the future of AI in this regard?
  - 5 watts to think
  - 1-2 watts to generate a word
  - 540B or 175B parameter model - 400W per GPU
  - Jaime: Not trying to abdicate human intelligence. But like any tech, it's expensive in the beginning. Will expect it to improve in efficiency going forward.
- Are there smarter ways to do research in LLMs without 4,000 GPUs?
  - Denny: Few companies are going to build LLMs. But research community can try to understand HOW LLMs work. 
  - Ed: 30% of job is to how to organize research and set research agendas. Decompose the problem into 4 dimensions: data, compute, people and structure. Think about whether you have an advantage in these 4 areas. When you look at industry, startups and academics, it's hard to compete on data and compute but they can compete with people and structure. It's not just all about scale. The reasoning breakthroughs in LLMs came up by understanding a little bit about cognitive science and education.
- How do I define my core strengths as a researcher?
  - Jaime: We don't know what we don't know. We are trained to figure that. What are the skills that a human needs? These are big hard questions. Whether you are a big tech company, startup or an 18-year old you have to figure stuff out.
  - Ed: Two alternative futures. Living in the Bay Area, encounter investors who are investing into startups for AI for enterprise use cases. 
    1. Treat a LLM as a generic human. Augment it with a retrieval data, it will work as an assistant. As a VC I will be very happy. If you believe in Denny's work, this is Scenario 1.
    2. Train the model on a custom corpus, RAG only will not get to you human-level performance. As a VC I will be unhappy.