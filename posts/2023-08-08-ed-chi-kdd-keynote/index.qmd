---
title: "KDD - Day 2"
author: "Lawrence Wu"    
date: "2023-08-08"
categories: ["KDD"]
draft: true
---

Notes from Ed Chi's keynote at KDD 2023.

# 8 year cycles
- 1991 - invention of web browser
- 1999 - invention of Google and search
  - indexed the world's books
- 2007 - invention of mobile phone
- 2016: Functions that Deep Neural Network Can Learn
  - Text: Humans can be augmented by statistical machine translations
  - Image --> Caption: Put in an image and it would generate salient captions. This was an incredbile breakthrough since no specific instructions were given but accurate captions were **generated**.
  - How many people recognized in 2016 that there was going to be a generative AI cycle.

- By 2016, image recognition by machines/AI was already super-human, 3% error rate vs 5% error rate
- What changed about deep learning to make it a useful method?
- Getting comfortable with revolutions: as humans we couldn't fly 100 years ago. But now, we get delayed 30 minutes and get annoyed.
  - Arthur C Clarke quote
  - "Any commonplace technology is not magic!" - Ed Chi's corollary

#  LLM-based chatbots and asssistants 

- Timeline of how we got here
  - 2014: sequence to sequence learning with neural networks
  - 2017: Attention is all you need
  - 2020: Towards a Human-like Open Domain Chatbot
  - 2022: LaMDA: Language Model for Dialogue Applications
  - 2022-01: Chain-of-Thought Prompting Eliciits Reasoning in LLMs
TODO: See picture for other things

- Multi-task language models
- LaMDA
  - Large model, up to 137B parameters
  - Fine-tuned for sensibility, specificity, interestingness, safety, factuality
  - Foreshadowed Bard

- Possible soon: Everyone can have their own personal assistant that is not merely transactional but understands contexts.
- Bard
  - With Bard, we realized LLMs could begin to do planning, "Help me design a plan to read 20 books in a year"
  - No longer just a cute assistant but they could be helpful assistants in a wide variety of topics
  - Improved multilingual understanding, understanding idioms. Explain idioms and why they can be misunderstood.
  - Coding capabilities: more surprising to Ed. 
    - explain JAX, explain the code within the google/github repo
    - can you fix the code with a bug and add line by line comments in Korean
    - Learning about Double Machine Learning
  
- Insight (Data + Data Efficiency is the key to Conversational AI)
  - Pre-training (1B examples, 1T tokens)
  - Fine-tuning (10k examples)
  - Prompting (1 example)
    - small changes, micro-prompting can lead to big changes in the text that is generated. there is a parallel to human language where small changes to what you say can elicit different responses, e.g. adding "please" and "thank you"

# Tool-use: Retrieval-Augmnettation and Multi-modality in LLMs

- Limitations of LLMs
- Retrieval-Augmentation: Leveraging External Knowledge
  - TODO: insert image
  - Humans learned how to use tools like Google, how to craft the right search query to get the right results
  - We're teaching an LLM how to use tools like Google
- RETRO: Retrieval-augmneted generative model. 
  - the generator processses the question and the retrieved docs/passages separately
- Multi-modality output
  - Query generation may call an image service or an image generation service
  - Image input and output
- Coming Soon: Tool-use App Integration
  - 1st party tools from Google
  - 3rd party tools: Adobe Firefly to generate images, Uber, shopping service

# Augmneted Intelligence

- Human intelligence is augmented by search (needed two revolutions: mobile and search)
- LLMs + search/tools --> Super LLMs
- Humans + Super LLMs --> Super super humans?!

# Reasoning

## Human Intelligence vs Machine Learning 

- Denny Zhou: Differences between how humans and machines learn.
- Humans learn from only a few examples while machine learning needs tons of laebled data to train a model
- Attempts to fill the gap
TODO: See slide

## Can we teach LLMs like we teach kids?

- hypothesis on how to improve reasoning

### Chain-of-Thought = "explanation" + "answer"

- 2022 paper
- Brekathrough capabilities: Reasoning tasks
  - Standard prompting
  - Chain of thought prompting - adding an example of an explantation/answer to the prompt allows the LLM to replicate that reasoning capabilities

### Self-conssitency decoding

- Paper: Self-Consistency Improves Chain of Thought Reasoning in Language Models
- if you create diverse reasoning paths with different temperatures and then took a majority vote, this would lead to better performance

### Least-to-most Prompting

- Paper: Least-to-Most...
- Decompose question into subquestions
- Sequentially solve subquestions

### Instruction Finetuning: Enables zero-shot prompting

- ML: Data --> Prediction
- Instruction fine-tuning: Instruction --> Answer
- FLAN Instruction tuning
  - Fine-tunes model on a large set of varied instructions that use a simple and intuitive description of the task
  - instruction tuning improves performance on unseen tasks only for models for certain sizes (68B)

### FLAN2: Finetune with 1800+ tasks, bigger models

### Reasoning Summary

TODO: See slide

# Future Challenges for LLMs

Interesting he made a comment, thanking OpenAI for releasing ChatGPT because that was the impetus for Google's LaMDA team being able to release Bard into the marketplace. As an outisder, it's fascinating thinking about what those internal discussions must've been like.

## Responsibility and Safety

- Constitutional AI - instruction tuning is one way to help keep LLMs safe

## Factuality, Grounding, and Attribution
- Grounding and Attribution of Answers
- RARR: Retrofit Attribution using Research and Revision

## Human <-> AI Content Loop and Ecosystem
- Many reasons for identifying machine-generated content - e.g. avoid loops of using LLM-generated examples for future LLMs
- Humans are not good at detection LLM-generated content (Ironic from a Turing test POV)

## Personalization and User Memory

- We want the LLM experience personalized to you, understand your needs, and respect your privacy
- Serving efficiency
  - Low rank models
  - If he was a PhD student, he would invest in solving this problem. Can be very profitable amking models more efficient.
- Pigeonhole problem: aoiding over memorization of your preferences

# Conclusion

- History tells us we were due for a revolution and LLMs first applications are chatbots because LLMs understanding converstional context better
- Just like humans are empowered by tools and reasoning capabilities, **LLMs are now augmented with tools and capable of reasoning via language**

# Q&A

- How large do foundation models need to be?
  - Base models need to be sufficiently large or "double-digit billions". At this size, it is on the order of 1 cent per query
  - Costs are continuing to drop
- What are your thoughts about security/privacy with LLMs?
  - This is important because you don't want user preferences bleeding into other users
  - Differential privacy techniques can be applied
  - Localize some parameter changes in a separate tower that can be added to the base model
- When will LLMs develop causal reasoning capabilities?
  - One derivation from the Chain of Thought paper is that LLMs can do causal reasoning. The LLMs can explain themselves. 
- As data scientists, we are used to building specialized models and we have control over the parameters. With LLMs, the paradigm has shifted where we have to trust these pre-trained models. What are your thoughts?
  - Our ability to trust an LLM is going to take a human flavor. You build trust over time with a human. But a human usually cannot fully explain how they came to that decision.
  - You can interrogate these models for how it came up with these answers.
- How do we continue to be able to train models on human-generated data? 
  - Watermarking
    - for images is easier
    - for text - there is more sparsity
    - more pessismistic about watermarking being the solution to this problem
  - Have not heard a single proposal that is promising
  - Turing Award for work here...
- What abilities will humans retain that are superior than LLMs?
  - For the next few years at least, humans have a physical body
  - Reasoning capabilities: still some gaps between humans and LLMs
  - From the other side, machines never get tired
  - Remember 
  - Humans have a spiritual component. A machine or LLM will never have a spiritual component or a soul.
- Thoughts on evaluation of these LLMs? How can we differentiate between models?
  - In developing LaMDA and Bard, how do we build an evaluation framework to take the technology.
  - Defining three categories: quality, factuality/groundedness, safety/persona. In these 3 areas, developed sub-metrics in all of these. Spent a lot of money and time on each of these areas. 
    - Creativity: writing a poem, summarizing documents


# Large Language Model Day

<https://bigmodel.ai/llmday-kdd23/>

# Schedule

| Time         | Event                                              | Speaker/Details                                                                                                 |
|--------------|----------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
| **Date:**    | **Aug. 8, 2023**                                   | **Room:** Grand A                                                                                               |
| 10:00-12:00  | Distinguished Keynotes                             |                                                                                                                  |
| 10:00-10:10  | Opening Remarks                                    | Jie Tang (Tsinghua University)                                                                                   |
| 10:10-11:00  | Jaime Teevan (Microsoft)                           | From Documents to Dialogues: How LLMs are Shaping the Future of Work                                             |
| 11:00-11:50  | Denny Zhou (Google DeepMind)                       | Teach language models to reason                                                                                  |
| 12:00-13:30  | Lunch                                              |                                                                                                                  |
| 13:30-15:30  | Keynotes                                           |                                                                                                                  |
| 13:30-14:10  | Vedanuj Goswami (Meta FAIR)                        | Llama 2: Open Foundation and Fine-Tuned Chat Models                                                              |
| 14:10-14:50  | Peng Zhang (Zhipu AI)                              | From GLM-130B to ChatGLM                                                                                         |
| 14:50-15:30  | Jason Wei (OpenAI)                                 | The large language model renaissance: paradigms and challenges                                                   |
| 15:30-16:00  | Coffee Break                                       |                                                                                                                  |
| 16:00-17:30  | Panel: Paradigm Shifts in the Era of LLMs          | Opportunities and Challenges in Academia, Industry, and Society. Moderator: Qiaozhu Mei (University of Michigan)  |
|              |                                                    | Invited Panelists: Ed Chi (Google DeepMind), Vedanuj Goswami (Meta FAIR), Jaime Teevan (Microsoft), Vy Vo (Intel Labs), Denny Zhou (Google DeepMind) |


# Opening Remarks: Jie Tang (Tsinghua University)

- Opportunity to KDD: Unified model for all data (text, DB, image, multimedia)
- Opportnity to Academia: 
  - Before we categorize our research vertically
  - Next, we may need to reorganize our research horizontally
- Opportunity to startup:
  - LLM offers a chance to implement AGI
  - Vision of a new startup is not necessary to answer "how", it is more about "why"?

# From Documents to Dialogues: How LLMs are Shaping the Future of Work - Jaime Teevan (Microsoft)

How we share and disseminate knowledge

- Also going to mirror's Ed's talk where Jaime is going to share about a history.
- How the future of work is going to change and how the history of academic research
- Pursuit of GDP Growth [slide]
- "The Current Moment"
- The search engine was the first AI-scale application. Enabled by the cloud.
  - Worked at Infoseek in the early days
  - Had 4 kids during this time (oldest was born in 2004, oldest in 2008)
  - In those 4 years: Both cloud, edge devices (mobile) and social networks (create information at scale) came into existence. Mechnical Turk was also created. The combination of those things created Imagenet, which led to much development.
  - Have to be able to make use of fragmented time and attention. Her focus of research. Micro-productivity and how people work with AI.
- Worked at Microsoft Research after doctorate
  - Information retrieval
  - Worked with Satya Nadella as his technical advisor
- Now here role is to think about how to drive disruption, bring research into products
  - in the midst of this was the pandemic/COVID.
- AI is more of an internal disruption than an external one
  - Spotify: "Today has been a great year in AI"
- Fall 2022: Meeting with Sam Altman getting to try out GPT-4 for the first time. Had to drive to campus to try it out because it was so secretive.
  - Your job is to get this into all of Microsoft's products
  - Driving home, had to pull over and scream because it was so amazing

## Enterprise Grade AI
taking the world's best language models and making them work for the enterprise involves these 4 things:
  - Global Scale
  - Grounded in your Data
  - Trustworthy
  - Embedded in Existing Workflows

- Global Scale: Multilingual
  - shipped M365 copilot in all tier 1 languages because of language model's ability to translate
  - translating through EN improves performance for some languages but not for others
    - high resource languages tend to do better without neeeding to go through English
    - non-latin scripts tend to have additional challenges
- Global Scale: Efficiency
- **Just like how everyone uses the internet, everyone will use language models**
- Global Scale: Sustainable
  - Underwater data centers are a thing!
- Grounded in your data: RAG [slide]
  - search over knowledge and new types of data: documents, chat history, application context
  - grounded in the context:
    - word document
    - chat history
    - transcript
- Grounded in your data: Private by design [slide]
- Trustworthy: Differential privacy
  - Better privacy/utility trade-off: similar accuracy for private vs. non-private fine-tuning
  - Privacy episilon?
- Trustworthy: Measuring privacy
  - Combining DP and PII scrubbing effictively eliminates the risk of privacy
- Trustworthy: Responsible
  - requires layers of mitgation
  - LLMs foreground new challenges
    - Hallucination and errors
    - Jailbreaks and prompt injection
    - Harmful content and code
    - Manipulation, human-like behavior
  - Enterprise vs. consumer context
- Embedding in Existing Workflows: ODSL
  - Office Domain Specific Language
  - Excel is Turing Complete!
    - As soon as you can translate natural language into a DSL, you unlock a lot of value
    - Excel + LLMs

## Knowledge in Conversations

- Grounding vs. Grounded
  - After grounding, we create grounded content
  - In the LLM context: the first few prompts are the process of grounding
  - Microsoft is a document company - LLMs can extract knowledge from those documents. 
    - Not just facts
    - Style
    - Structure
- Collective Intelligence - how to capture all the knowledge across a community like KDD and synthesize it
  - Analogy: maps. Structured data. Allows us to navigate the world and create geopolitical boundaries. Google Maps has centralized a lot of this knowledge. 
  - Chat Log Analysis
    - Come up with Prompting Do's and Don'ts
- Prompt Support: Creating the LLM "Ribbon"
  - support learning: identify and surface tips in situ

## Lead like a Scientist

## Q&A

- What do you foresee about the future of remote work?
  - Research shows in person work is valuable
  - Language models may help people access fast real-time institutional knowledge 
- Once we have products built on top of LLMs, how do you see the future of development?
  - A lot of this will be emergent behavior
  - With Twitter/X, they didn't invent hashtags
- What do you see the challenges of ethics by design?
  - ?
- How do you audit a model whether you are meeting those metrics?
  - For Microsoft products: there are metrics tied to each guideline in their Responsible AI guidelines
- Can you comment on the future of LLMs in the context of open source? As scientists, how can we validate models that are closed?
  - Human parity has been our measure for so long, what's next?
  - This is something we have to figure out.
  - Source selection for RAG
  - Model selection for efficiency is important
- How do you think about security at Microsoft?
   - Red-teaming for security and ethics
   - Interesting: a lot of models are used in red-teaming efforts.

  # Teach language models to reason (Denny Zhou (Google DeepMind))

  - Leads the Reasoning team at Google DeepMind
  - What do you expect from AI?
  - My little expectation on AI
  - Does ML meet this expectation?
    - Semi-supervised learning
    - etc.
  - What is missing in ML?
    - Reasoning 
    - Humans can learn from a few examples because humans can reason
  - Teach LLMs to reason like we teach kids
  - Toy problem: concaenate the last letter of each word
    - Elon Musk --> nk
    - Bill Gates --> ls
    - Solve it by machine learning
    - Solve it by LLMs
  - Training an LLM
    - You can think of training an LLM like training a parrot to mimic human language
    - Few-shot prompting for last-letter concatenation
      - give it the two exammples and one input to get the answer
- Why we created the last-letter-concatenation task?
  - Make ML fail
  - Make few-shot prompting
  - But trivial for humans
- Chain of thoguht prompting
  - Adding "thought" before "answer"
  - End of 2021 - discovered this
- One demonstration is engouh, just like humans
- Standard few-shot prmopting vs. Chain-of-thought prompting
- Can LLMs solve math word problems?
- Apply CoT to any task
  - all tasks can be solved by CoT without machine learning
  - 100x-1000xdata efficient than supervised SoTA in literature - only need 1-2 examples!
- Multilingual CoT
- Apply CoT to solve BIG-Bench Hard
- "Thought" does NOT have to be "step by step"
- Self-consistency decoding - greatly improves chain-of-thought decoding
  - prompt a language model using example chains of thought
  - sample from the LLM decoder to generate a diverse set of reasoning paths
  - choose the most consistent answer using the majority vote
  - crushed GSM8K SoTA with only 8 examples
- How many more examples are needed for finetuning to be comparable to CoT + SC?
  - From original paper: two additional orders of magnitude of training data to reach an 80% solve rate
  - But with only 8 examples, the model can be taught reasoning
- Solve high school math problems
  - Fine-tuning PaLM with math data
  - SC + CoT solves 50%
  - non-math grad students solve 40%
- Motivation to SC decoding
  - Answer in the greedy ouput from CoT DOES NOT EQUAL the most likely answer
  - SC leads to the most likely answer
    - The full probability of each answer is computed by summing over all reasoning paths (margialize the reasoning paths)
    - Implementation via sampling: sample multiple times, then choose the most common answer
  - Implications from the probabilistic explanation

## Least-to-most prompting

Enables easy-to-hard generalization

- CoT fails to generalize to harder problems
- Least-to-most prompting = Planning + Reasoning 
  1. Decompose a complex prolem into a list of easier subproblems
  2. Sequentially solve these subproblems
- Solve math word problems by least-to-most prompting 
- How does a LLM plan and rank tasks easy to hard?
- Can use this for common sense reasoning
- Can use this for solving math word problems.
  - Did Aristotle use a laptop?
  - Are chincillas cold-blooded?
- Last-letter task generalization (still not perfect)
- SCAN (compositional generalization): text to actions
  - 100% accuracy using least-to-most
- CFQ (compositional generalization): text to code
  - Using 1% of the data, crush SoTA results

## LLMs for Code

How to generate high-quality code?

- Teaching LLMs to Self-Debug
  - self-debug to generate higher quality code
- LLMs as Tools Makers
  - One way:
    - Reduce serving costs using distillation or quantization
    - For most models we use a small model
    - For some models we use a large model
  - Use a few instances to make a tool --> reuse the tool to solve similar instances
    - Analogy of deep learning libraries - talented programmers develop pytorch, practioners `import torch` to solve their problems

## Common big prompt for any task?

Yes!

- Key idea: making a big prompt by combining prompts from different tasks, and then using it for any task
- Magic: any task: including tasks which are not even seen
- Implementation: Too 

## Instruction Tuning

- Enable zero-shot prompting in any task
- Pioneered by FLAN and T0
- Store the large prompts in model weights
- Example of parsing all names from this message, then sort and add Quoc Le to the list.
- Why does this work?
  - What I cannot create, I do not understand - Feynmand
  - We know how to create LLMs but do not know how they work
- Emergent properties
  - All these are emergent properties
  - Emergent properties are discovered, not designed by LLM builders
- How to make parrots intelligent? Scaling up!
- Toward understanding in-context learning
  - in-context learning ....
  - learned models are encoded in activations

## Smmmary
[slide]

- These ideas are trivial if LLMs are humans