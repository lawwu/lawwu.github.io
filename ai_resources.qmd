---
title: "AI/ML/NLP Resources"
comments: false
---

# Timeline
Inspired by @osanseviero's [ml_timeline](https://github.com/osanseviero/ml_timeline)

## April 2023

* 12: DeepSpeed Chat - DeepSpeed Chat offers an end-to-end RLHF pipeline to train ChatGPT-like models. This is the missing piece from other efforts like Alpaca and Vicuna. The RLHF pipeline is replicated from the InstructGPT paper. ([tweet](https://twitter.com/omarsar0/status/1645936415941836804))
* 12: Dolly 2.0 - Introducing the first *commercially viable*, open source, instruction-following LLM. Dolly 2.0 is available for commercial applications without having to pay for API access or sharing data with 3rd parties. ([post](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm), [tweet](https://twitter.com/alighodsi/status/1646151697415168006))
* 9: baby GPT from Andrej Karpathy "I think it's interesting to train/study tiny GPTs because it becomes tractable to visualize and get an intuitive sense of the entire dynamical system." ([tweet](https://twitter.com/karpathy/status/1645115622517542913))
* 7: SegGPT: Segmenting Everything in context ([tweet](https://twitter.com/_akhaliq/status/1644147931178496001))
* 6: StackLlama ([tweet](https://twitter.com/lvwerra/status/1643998302738759683))
* 6: VideoCrafter: text to video model ([tweet](https://twitter.com/TomLikesRobots/status/1643878218498207744))
* 5: LMQL - A query language for programming (large) language models. ([tweet](https://twitter.com/lmqllang/status/1643582539745964033))
* 5: [StackLLaMA: A hands-on guide to train LLaMA with RLHF
](https://huggingface.co/blog/stackllama) ([post](https://huggingface.co/blog/stackllama))
* 6: Generative Novel View synthesis ([tweet](https://twitter.com/_akhaliq/status/1643790003779059715))
* 5: SAM - Segment anything ([tweet](https://twitter.com/MetaAI/status/1643599800414380038))
* 5: ChatArena, multi-agent game environments for LLMs ([tweet](https://twitter.com/mindjimmy/status/1643633046208249856))
* 5: Kandinsky 2.1 for image generation ([tweet](https://twitter.com/nearcyan/status/1643421466795417600))
* 5: LLaMA-Adapter ([tweet](https://twitter.com/lupantech/status/1643385891338227712))
* 5: LatentVideo Diffusion Models for long video generation ([tweet](https://twitter.com/_akhaliq/status/1643627527594815488))
* 4: MolFeat, a hub of molecular featurizers ([tweet](https://twitter.com/datamol_io/status/1643263399915311104))
* 4: LangChain announced their $10M seed round ([tweet](https://twitter.com/hwchase17/status/1643301144717066240))
* 4: Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models ([paper](https://arxiv.org/abs/2304.01852)) 
* 4: Kandinsky 2.1 ([tweet](https://twitter.com/_akhaliq/status/1643191350672646144))
* 4: IGEL, an instruction-uned German LLM ([tweet](https://twitter.com/_philschmid/status/1643278444992626689))
* 4: Koala-13B: A Dialogue Model for Academic Research ([tweet](https://twitter.com/AlphaSignalAI/status/1643306708716904461), [blog](https://bair.berkeley.edu/blog/2023/04/03/koala/))
* 4: Baize: An Open-Source chat model with PEFT ([tweet](https://twitter.com/arankomatsuzaki/status/1643054506148614146))
* 3: Vicuna-13B weights are released ([tweet](https://twitter.com/lmsysorg/status/1642968294998306816))
* 3: A Survey of Large Language Models ([tweet](https://twitter.com/arankomatsuzaki/status/1642686213147738112))
* 3: babyagi - Open-sourcing “Baby AGI”, a paired down version of the “Task-Driven Autonomous Agent” at 105 lines of code. ([tweet](https://twitter.com/yoheinakajima/status/1642881722495954945))


## March 2023
* 30: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace ([paper](https://arxiv.org/abs/2303.17580), [code](https://github.com/microsoft/JARVIS)) - Using a LLM as brain, HuggingGPT identifies what HuggingFace models to use to solve tasks. Notably Microsoft is calling this `JARVIS`
* 30: Ethics and Society Newsletter #3: Ethical Openness at Hugging Face - ([post](https://huggingface.co/blog/ethics-soc-3))
* 30: BloombergGPT: A Large Language Model for Finance ([tweet](https://twitter.com/TechAtBloomberg/status/1641772329658114053), [paper](https://arxiv.org/abs/2303.17564))
* 30: Nucleotide Transformer, SOTA Genomics ([tweet](https://twitter.com/instadeepai/status/1641075963051012097), [code](https://github.com/instadeepai/nucleotide-transformer))
* 30: ColossalChat ([blog](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b), [code](https://github.com/hpcaitech/ColossalAI))
* 29: GeoV-9b ([tweet](https://twitter.com/labmlai/status/1641357802009395201), [code](https://github.com/geov-ai/geov), [weights](https://huggingface.co/GeoV/GeoV-9b), [colab](https://colab.research.google.com/github/geov-ai/geov/blob/master/notebooks/generate.ipynb))
* 29: Spanish BERTIN GPT-J-6B Alpaca and Alpaca LoRA ([tweet](https://twitter.com/versae/status/1641124547414900736))
* 29: LLaMA Adapter ([tweet](https://twitter.com/lupantech/status/1640899600281395200), [code](https://github.com/ZrrSkywalker/LLaMA-Adapter), [paper](https://huggingface.co/papers/2303.16199))
* 28: PRESTO dataset ([github](https://github.com/google-research-datasets/presto))
* Auto-GPT: An experimental open-source attempt to make GPT-4 fully autonomous. ([tweet](https://twitter.com/SigGravitas/status/1640918447382237184))
* 28: OpenFlamingo ([tweet](https://twitter.com/anas_awadalla/status/1640766789977251840), [blog](https://laion.ai/blog/open-flamingo/))
* 28: Raven RWKV (RWKV finetuned on alpaca and codealpaca) ([tweet](https://twitter.com/BlinkDL_AI/status/1640742627216875524), [demo](https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B))
* 28: Cerebras-GPT ([tweet](https://twitter.com/CerebrasSystems/status/1640725880711569408), [models](https://huggingface.co/cerebras))
* 28: GPT4All ([tweet](https://twitter.com/andriy_mulyar/status/1640836003194630144), [code](https://github.com/nomic-ai/gpt4all))
* 28: Replit Partners with Google Cloud ([tweet](https://twitter.com/Replit/status/1640745029080866817))
* 27: LLaMA voice chat + Siri TTS ([tweet](https://twitter.com/ggerganov/status/1640416314773700608))
* 27: GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models ([paper](https://arxiv.org/pdf/2303.10130.pdf)) – Paper that identifies the occupations that have the highest exposure to automation by GPT. In related news, ResumeBuilder found [1 in 4 companies have already replaced workers with ChatGPT](https://www.resumebuilder.com/1-in-4-companies-have-already-replaced-workers-with-chatgpt/#:~:text=With%20the%20emergence%20of%20ChatGPT,%2C%20write%20code%2C%20and%20more)
* 26: Japanese Alpaca LoRA ([tweet](https://twitter.com/kun1em0n/status/1639965140429963264), [demo](https://huggingface.co/spaces/kunishou/Japanese-Alpaca-LoRA-7b-DEMO), [report](https://note.com/kun1emon/n/n1533345d5d26))
* 26: LLaMA voice chat ([tweet](https://twitter.com/ggerganov/status/1640022482307502085))
* 24: Text2Video-Zero ([tweet](https://twitter.com/_akhaliq/status/1639062868850266112), [code](https://github.com/Picsart-AI-Research/Text2Video-Zero))
* 24: Dolly ([tweet](https://twitter.com/databricks/status/1639239800145465344), [code](https://github.com/databrickslabs/dolly), [demo](https://huggingface.co/databricks/dolly-v1-6b))
* 22: Sparks of Artificial General Intelligence ([paper](https://arxiv.org/abs/2303.12712))
* 22: Alpaca LoRA as a chatbot ([tweet](https://twitter.com/algo_diver/status/1638525828773576704), [code](https://github.com/deep-diver/Alpaca-LoRA-Serve)).
* 20: Reflexion: an autonomous agent with dynamic memory and self-reflection ([paper](https://arxiv.org/abs/2303.11366)). A related [post](https://nanothoughts.substack.com/p/reflecting-on-reflexion).
* 20: [GPT 4 and the Uncharted Territories of Language](https://www.fast.ai/posts/2023-03-20-wittgenstein.html) - Jeremy Howard uses GPT-4 to write a blog post highlighting some of the dangers of language models. He used 4 prompts to write the post.
* 20: Runway Gen-2 ([tweet](https://twitter.com/runwayml/status/1637800500459458562))
* 24: SwissBERT ([tweet](https://twitter.com/j_vamvas/status/1639192870828556290), [blog](https://vamvas.ch/introducing-swissbert))
* 17: Alpacoom: BLOOM fine-tuned on Alpaca's dataset using LoRA ([tweet](https://twitter.com/mrm8488/status/1636742703055527937?s=20), [model](https://huggingface.co/mrm8488/Alpacoom))
* 16: Alpaca LoRA: instruct tune LLAMA on consumer hardware ([tweet](https://twitter.com/_akhaliq/status/1636416647518097408), [code](https://github.com/tloen/alpaca-lora))
* 15: GPT-4 Technical Paper ([paper](https://arxiv.org/abs/2303.08774)) - highlights some of the amazing improvements GPT-4 has made over GPT-3
* 13: [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) – Stanford’s CRFM group released a 1.5B parameter GPT-3 like model. They were the first to demonstrate you can get GPT-like performance using only 52k instruction-following data points. On the self-instruct evaluation set, Alpaca shows many behaviors similar to OpenAI’s text-davinci-003, but is also surprisingly small and easy/cheap to reproduce. I think one reason OpenAI dropped their pricing by 90% with GPT-4 is because they wanted to achieve wide distribution of their model.
 

## August 2022
* 21: Emergent Abilities of Large Language Models ([paper](https://openreview.net/forum?id=yzkSU5zdwD), [blog](https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html))


# Resources

- [Dive into Deep Learning](https://d2l.ai/chapter_introduction/index.html) - Comprehensive guide to deep learning
- [Fast.ai](https://www.fast.ai/)
- [Google's Deep learning tuning playbook](https://github.com/google-research/tuning_playbook)
- [Course Notes from Andrew Ng's Deep Learning Specialization](https://github.com/lijqhs/deeplearning-notes)
- [Jay Alammar's personal site](https://jalammar.github.io/) - Visualizing machine learning one concept at a time
- [Practical Deep Learning for Coders](https://course.fast.ai/)
  - [Part 2](https://course.fast.ai/Lessons/part2.html) - released 2023-04 [tweet](https://twitter.com/jeremyphoward/status/1643410603363692544)
  - [Part 1](https://course.fast.ai/Lessons/lesson1.html) - "v5" - released 2022-07-21
- [What is Prompt Engineering](https://prmpts.ai/blog/what-is-prompt-engineering) - like how Googling became a skill (aka "Google-fu"), I think Prompt Engineering is an important skill to develop
- [awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts) - A curated list of awesome ChatGPT prompts. I like "Act as a Linux Terminal" prompt.
- [Prompt Engineering Guide](https://www.promptingguide.ai/) - "Motivated by the high interest in developing with LLMs, we have created this new prompt engineering guide that contains all the latest papers, learning guides, lectures, references, and tools related to prompt engineering." Code: [repo](https://github.com/dair-ai/Prompt-Engineering-Guide)

## Podcasts

- [DeepPapers](https://www.deeppapers.dev/) - Deep Papers is a podcast series featuring deep dives on today’s seminal AI papers and research

## YouTube

- [@lexfridman](https://www.youtube.com/@lexfridman) - and associated [transcripts](https://karpathy.ai/lexicap/)
- [@AndrejKarpathy](https://www.youtube.com/c/AndrejKarpathy)
- [@jamesbriggs](https://www.youtube.com/@jamesbriggs)
- [@ai-explained-](https://www.youtube.com/@ai-explained-)

## Twitter

- [@jeremyphoward](https://twitter.com/jeremyphoward)
- [@radekosmulski](https://twitter.com/radekosmulski)
- [@omarsar0](https://twitter.com/omarsar0)

## Newsletters

- [Davis Summarizes Papers](https://dblalock.substack.com/)
- [Data Science Programming News](https://dspn.substack.com/) - Run by [Eric J Ma](https://ericmjl.github.io/)

## Libraries / Tools

- [Github Copilot](https://github.com/features/copilot) - I use Copilot in my IDE, VS Code and it's dramatically improved my producitivity (10-20%?). More than that it makes coding less tedious and lowers the activiation energy for coding tasks. For example generating docstrings is trivial (and happens much more frequently!). And because the recommendations are inline, the developer's 'flow' is not broken. I also moved from Jupyter Notebooks in a browser to using Jupyter in VS Code. Radek Omulski has a [blog post](https://radekosmulski.com/an-ide-for-the-era-of-ai/) for how to set this up.
- [LangChain](https://github.com/hwchase17/langchain) - Building applications with LLMs through composability
- [llama_index](https://github.com/jerryjliu/llama_index) - LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.
- [streamlit](https://github.com/streamlit/streamlit) - Python framework for buliding UIs. I've used this a lot for data science demos. Resources to inspire you: [awesome-streamlit](https://github.com/MarcSkovMadsen/awesome-streamlit) and [Streamlit's gallery](https://streamlit.io/gallery)
- [gradio](https://github.com/gradio-app/gradio) - similar to Streamlit but more for ML/NLP models.
- [marvin](https://github.com/PrefectHQ/marvin) - Meet Marvin: a batteries-included library for building AI-powered software. Marvin's job is to integrate AI directly into your codebase by making it look and feel like any other function.

# Ethical AI

- [Ethics & Society at HuggingFace](https://huggingface.co/spaces/society-ethics/about)

# Transformers

- [transformers](https://github.com/huggingface/transformers)
- 2023-01-27 - [The Transfomer Family Version 2.0](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)
- 2023-01-17 - [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY) - fantastic, in-depth tutorial by Andrej Karpathy building up transformers from scratch
- 2022-01-03 - [Illustrated Retrieval Transformer](https://jalammar.github.io/illustrated-retrieval-transformer/)
- 2020-12-17 - [Explaining Transformers](https://jalammar.github.io/explaining-transformers/)
- 2020-07-27 - [How GPT3 Works Visually](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)
- 2020-04-07 - [The Transformer Family](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/)
- 2019-08-12 - [Illustrated GPT2](https://jalammar.github.io/illustrated-gpt2/)
- 2018-06-27 - [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- 2018-04-03 - [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- 2017-06-12 - [Attention is All You Need](https://arxiv.org/abs/1706.03762)

# GPT

| Model   | Parameters         | Number of Transformer Layers | Tokens Trained On (Estimated) | GPU Hours / Cost to Train      | Release Date | Changes                                                                 | Link to Paper                                                           |
| ------- | ------------------ | ---------------------------- | ----------------------------- | ------------------------------ | ------------ | ----------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| GPT-1   | 117M               | 12                           | 8.3 billion                   | Not publicly disclosed          | June 2018    | Initial release of the GPT architecture                                 | [paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) |
| GPT-2   | 1.5B        | 48 (largest model)           | 40 billion                    | ~256 GPU hours (estimated)      | February 2019| Increased model size, dataset, and training compute                     | [paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) |
| GPT-3   | 175B        | 96 (largest model)           | 45 terabytes (raw text)       | ~355,000 GPU hours (estimated)  | June 2020    | [Alternates dense and sprase self-attention layers](https://jalammar.github.io/how-gpt3-works-visualizations-animations/). Further increase in model size, dataset, and training compute           | [papers](https://arxiv.org/abs/2005.14165) |
| GPT-3.5 / ChatGPT | 355B | ? | ? | ? | 2022-11-30 | RLHF Alignment | [post](https://openai.com/blog/chatgpt) |
| GPT-4   | 1 trillion (?)      | 196 (largest model)          | 100 terabytes (raw text)      | Not publicly disclosed          | 2023-03-14 | Context windows of 8,192 and 32,768 tokens, introduction of the System Message, multi-modal (images and text)                           | [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)
                                    |


The above table was generated by GPT-4 using the prompt "For each of the models GPT-1, GPT-2, GPT-3, and GPT-4, can you provide a table with the following fields? 
| Model | Parameters | Number of Transformer Layers | Tokens Trained On (Estimated) | GPU Hours / Cost to Train | Release Date | Changes | Link to Paper |  Can you provide the table in markdown code but wrap it in ``` so it doesn't get rendered by the browser" and then edited



# Predictions

- 2023-04-01 - [@AllenDowney](https://mobile.twitter.com/AllenDowney) predicts "The great majority of coding will be LLM-assisted, starting now." ([tweet](https://mobile.twitter.com/AllenDowney/status/1642292405201190915), [blog](https://www.allendowney.com/blog/2023/04/02/llm-assisted-programming/))
- 2020-05-08
  - [Ilya Sutskever](https://www.youtube.com/watch?v=13CZPWmke6A&t=2548&ab_channel=LexFridman) - Doesn't think backpropogation will be replaced. 
  - [Ilya Sutskever](https://www.youtube.com/watch?v=13CZPWmke6A&t=2055s&ab_channel=LexFridman) - It is no longer possible for one person with one GPU to make significant breakthroughs in deep learning research. The deep learning "stack" is too deep. Ilya describes the stack as: ideas, systems to build datasets, distributed programming, building the actual cluster, GPU programming and putting it all together. "It can be quite hard for a single person to become world class in every single layer of the stack." OpenAI's technical paper: enumerate the number of people in each list.
  - [Ilya Sutskever](https://www.youtube.com/watch?v=13CZPWmke6A&t=2565&ab_channel=LexFridman) - I think that the neural networks that will produce the reasoning breakthroughs of the future will be very similar to the architectures that exist today... Humans can reason. So why can't neural networks?


