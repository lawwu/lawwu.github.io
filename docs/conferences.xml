<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Lawrence Wu</title>
<link>https://lawwu.github.io/blog.html/conferences.html</link>
<atom:link href="https://lawwu.github.io/blog.html/conferences.xml" rel="self" type="application/rss+xml"/>
<description>This is Lawrence Wu&#39;s personal website</description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Tue, 28 May 2024 07:00:00 GMT</lastBuildDate>
<item>
  <title>Mastering LLMs - Fine-Tuning Workshop 3 - Instrumenting &amp; Evaluating LLMs (guest speakers Harrison Chase, Bryan Bischof, Shreya Shankar, Eugene Yan)</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/blog.html/conferences/2025-05-28-mllms-workshop3/index.html</link>
  <description><![CDATA[ 



<section id="key-takeaways" class="level1">
<h1>Key Takeaways:</h1>
<ul>
<li>Evaluation is critical: It drives the iterative process of improving your LLM application, enabling you to make informed decisions about prompt engineering, fine-tuning, and model selection.</li>
<li>Don’t overcomplicate things: Start simple with unit tests and focus on practical metrics that align with your specific use case. Avoid building overly complex evaluation frameworks prematurely.</li>
<li>Human judgment is still essential: While LMs can assist with evaluation, human experts are ultimately responsible for defining what constitutes good performance and aligning the LLM to those preferences.</li>
<li>Look at your data: Regularly examine the inputs and outputs of your LLM system, as well as production data, to identify failure modes, refine evaluation criteria, and ensure the system is behaving as intended.</li>
<li>Evaluation is iterative: Both evaluation criteria and the methods for evaluating those criteria will evolve as you learn more about the task and gather more data. Be prepared to adapt and refine your evaluation process over time.</li>
</ul>
</section>
<section id="insights" class="level1">
<h1>Insights:</h1>
<ul>
<li>Traditional ML evaluation principles still apply: Leverage existing evaluation techniques and adapt them to the nuances of LLMs. Don’t treat LLM evaluation as an entirely new field.</li>
<li>Use case experts are invaluable: Involve them in the evaluation process from the beginning to ensure alignment between evaluation metrics and user needs.</li>
<li>LM-assisted evaluation is not a silver bullet: While helpful for scaling evaluations and providing directional insights, it requires careful and methodical application. Multiple judges, models, and shots should be used, and human alignment should be checked regularly.</li>
<li>Production endpoints are key: Evaluating directly against production endpoints minimizes drift and ensures consistency between development and production environments.</li>
</ul>
</section>
<section id="action-items" class="level1">
<h1>Action Items:</h1>
<ul>
<li>Implement unit tests: Identify and codify basic sanity checks for your LLM application to catch common errors.</li>
<li>Start logging traces: Use existing tools like LangSmith, Braintrust, or Instruct to capture the inputs and outputs of your LLM pipeline for detailed analysis and debugging.</li>
<li>Develop a simple evaluation framework: Focus on key metrics relevant to your use case, and build the framework iteratively as you gain more experience and data.</li>
<li>Involve use case experts: Work with them to define evaluation criteria, review outputs, and provide feedback on the evaluation process.</li>
<li>Explore LLM-assisted evaluation: Experiment with tools and techniques that leverage LLMs to scale your evaluation efforts, but do so with a critical eye and ensure human alignment.</li>
</ul>
</section>
<section id="other-interesting-information" class="level1">
<h1>Other Interesting Information:</h1>
<ul>
<li>Several tools and resources for LLM evaluation were mentioned, including LangSmith, Braintrust, Weights &amp; Biases, Instruct, Identic.Logfire, Weave, OpenTelemetry, and a book on recommendation systems by Brian Bishop.</li>
<li>A research prototype interface called EvalGen was presented, which assists developers in creating and iterating on LLM evaluations in a more intuitive and user-friendly way.</li>
<li>The talk emphasized the importance of minimizing wait time in the evaluation process by involving humans in the loop for tasks like editing criteria, refining criteria, and interactively grading LLM outputs.</li>
</ul>


</section>

 ]]></description>
  <category>Conference</category>
  <category>AI</category>
  <category>LLMs</category>
  <category>Mastering LLMs</category>
  <category>Evaluation</category>
  <guid>https://lawwu.github.io/blog.html/conferences/2025-05-28-mllms-workshop3/index.html</guid>
  <pubDate>Tue, 28 May 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>AI Engineer Summit 2023</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/blog.html/conferences/2023-10-10-ai-engineer-summit/index.html</link>
  <description><![CDATA[ 



<section id="ai-engineer-summit" class="level1">
<h1>AI Engineer Summit</h1>
<p>sywx was the <a href="https://www.latent.space/p/ai-engineer">first</a> to define the job title “AI Engineer” as a role in between a Data Scientist and Full Stack Software Engineer, someone that builds on top of large foundation models and can quickly build services using these models. I agree with him that this job function will likely expand whether you hold the job title of “AI Engineer” or not.</p>
<p>I had the privilege of attending the inaugural AI Engineer Summit in San Francisco, CA held on October 9-10, 2023. It was somewhat surprising being one of the few data scientists at the conference as most people I met were software engineers trying to transition into AI Engineering.</p>
<p>The talks were livestreamed (<a href="https://www.youtube.com/watch?v=veShHxQYPzo&amp;ab_channel=AIEngineer">Day 1</a> and <a href="https://www.youtube.com/watch?v=qw4PrtyvJI0">Day 2</a>). Below are my notes from the conference.</p>
<section id="workshop-building-evaluating-and-optimizing-your-rag-app-for-production" class="level2">
<h2 class="anchored" data-anchor-id="workshop-building-evaluating-and-optimizing-your-rag-app-for-production">Workshop: Building, Evaluating, and Optimizing your RAG App for Production</h2>
<p>Simon Suo, Cofounder / CTO, LlamaIndex<br>
</p>
<ul>
<li>Very indepth workshop on how to build an end to end RAG app over Ray documentation, also using Ray to build it. Slides are in the repo below.</li>
<li><a href="https://github.com/Disiok/ai-engineer-workshop" class="uri">https://github.com/Disiok/ai-engineer-workshop</a></li>
<li>Hallucinations: Most of the time it is caused by irrelevant retrieved passages</li>
<li>Evaluation: can think of both end-to-end evaluation and component-wise evaluation of a RAG app
<ul>
<li>End-to-end: understand how well the full RAG application works</li>
<li>Component-wise: understand specific components like the retriever (are we retrieving the relevant context?) and the generation (given the context, are we generating an accurate and coherent answer?)</li>
</ul></li>
<li>Data Required
<ul>
<li>User Query: representative set of real user queries</li>
<li>User Feedback: feedback from past interaction, up/down vote</li>
<li>Golden Context: set of relevant documents from our corpus to best answer a given query</li>
<li>Golden Answer: best ansewr given golden context</li>
</ul></li>
</ul>
</section>
<section id="workshop-function-calling-and-tool-usage-with-langchain-and-openai" class="level2">
<h2 class="anchored" data-anchor-id="workshop-function-calling-and-tool-usage-with-langchain-and-openai">Workshop: Function calling and tool usage with LangChain and OpenAI</h2>
<p>Harrison Chase, CEO, LangChain<br>
</p>
<ul>
<li><a href="https://github.com/hwchase17/ai-engineer" class="uri">https://github.com/hwchase17/ai-engineer</a></li>
<li>OpenAI function calling within LangChain to do structured data extraction, build agents to do extraction and tagging and use tools. Also a quick tutorial on</li>
<li>LangChain Expression Language (LCEL) is a relatively new way (introduced in Aug 2023) to compose langchain components</li>
</ul>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.prompts <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ChatPromptTemplate</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.chat_models <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ChatOpenAI</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.schema.output_parser <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StrOutputParser</span>
<span id="cb1-4"></span>
<span id="cb1-5">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ChatPromptTemplate.from_template(</span>
<span id="cb1-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Tell me a short joke about </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{topic}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb1-7">)</span>
<span id="cb1-8">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ChatOpenAI()</span>
<span id="cb1-9">output_parser <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StrOutputParser()</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># define the chain</span></span>
<span id="cb1-12">chain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> output_parser</span>
<span id="cb1-13"></span>
<span id="cb1-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># don't .run() the chain but call .invoke()</span></span>
<span id="cb1-15">chain.invoke({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"topic"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bears"</span>})</span></code></pre></div>
<ul>
<li>OpenAI’s Function Calling is a way to get OpenAI’s language models to return structured data (arguments to run a function or extract structured data from text). This is a powerful feature!</li>
<li>I’m surprised other LLM providers have not yet introduced this functionality.</li>
<li>langchain exposes helper function to make working with function calling easier</li>
</ul>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.utils.openai_functions <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> convert_pydantic_to_openai_function</span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> WeatherSearch(BaseModel):</span>
<span id="cb2-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Call this with an airport code to get the weather at that airport"""</span></span>
<span id="cb2-5">    airport_code: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"airport code to get weather for"</span>)</span>
<span id="cb2-6"></span>
<span id="cb2-7">weather_function <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> convert_pydantic_to_openai_function(WeatherSearch)</span>
<span id="cb2-8">weather_function</span>
<span id="cb2-9"></span>
<span id="cb2-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># {'name': 'WeatherSearch',</span></span>
<span id="cb2-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  'description': 'Call this with an airport code to get the weather at that airport',</span></span>
<span id="cb2-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  'parameters': {'title': 'WeatherSearch',</span></span>
<span id="cb2-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'description': 'Call this with an airport code to get the weather at that airport',</span></span>
<span id="cb2-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'type': 'object',</span></span>
<span id="cb2-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'properties': {'airport_code': {'title': 'Airport Code',</span></span>
<span id="cb2-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#     'description': 'airport code to get weather for',</span></span>
<span id="cb2-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#     'type': 'string'}},</span></span>
<span id="cb2-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'required': ['airport_code']}}</span></span></code></pre></div>
<p>then you can pass the weather function to the LLM</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.chat_models <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ChatOpenAI</span>
<span id="cb3-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ChatOpenAI()</span>
<span id="cb3-3">model.invoke(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"What is the weather in San Francisco right now?"</span>,</span>
<span id="cb3-4">             functions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[weather_function])  </span></code></pre></div>
<p>You can also bind the function to the model:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">model_with_function <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.bind(functions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[weather_function])</span></code></pre></div>
<p>You can force OpenAI to use a function, but you can only pass one function here.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">model_forced_function <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.bind(functions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[weather_function], function_call<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"name"</span>:<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"WeatherSearch"</span>})</span></code></pre></div>
<p>Function calling is a great way to do structured data extraction from text for example extracting name, age tuples.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Optional</span>
<span id="cb6-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Person(BaseModel):</span>
<span id="cb6-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Information about a person."""</span></span>
<span id="cb6-4">    name: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"person's name"</span>)</span>
<span id="cb6-5">    age: Optional[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"person's age"</span>)</span>
<span id="cb6-6">  </span>
<span id="cb6-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Information(BaseModel):</span>
<span id="cb6-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Information to extract."""</span></span>
<span id="cb6-9">    people: List[Person] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"List of info about people"</span>)</span>
<span id="cb6-10"></span>
<span id="cb6-11">extraction_functions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [convert_pydantic_to_openai_function(Information)]</span>
<span id="cb6-12">extraction_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.bind(functions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>extraction_functions, function_call<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"name"</span>:<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Information"</span>})</span>
<span id="cb6-13">extraction_model.invoke(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Joe is 30. Joe's mom is Martha"</span>)</span>
<span id="cb6-14"></span>
<span id="cb6-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># AIMessage(content='', additional_kwargs={'function_call': {'name': 'Information', 'arguments': '{\n  "people": [\n    {\n      "name": "Joe",\n      "age": 30\n    },\n    {\n      "name": "Martha",\n      "age": 0\n    }\n  ]\n}'}})</span></span></code></pre></div>
<ul>
<li>You can create your own tools using the <span class="citation" data-cites="tool">@tool</span> decorator and pass these tools to OpenAI</li>
</ul>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.agents <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tool</span>
<span id="cb7-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.chat_models <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ChatOpenAI</span>
<span id="cb7-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pydantic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseModel, Field</span>
<span id="cb7-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> requests</span>
<span id="cb7-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> datetime</span>
<span id="cb7-6"></span>
<span id="cb7-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define the input schema</span></span>
<span id="cb7-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> OpenMeteoInput(BaseModel):</span>
<span id="cb7-9">    latitude: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(..., description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Latitude of the location to fetch weather data for"</span>)</span>
<span id="cb7-10">    longitude: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(..., description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Longitude of the location to fetch weather data for"</span>)</span>
<span id="cb7-11"></span>
<span id="cb7-12"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@tool</span>(args_schema<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>OpenMeteoInput)</span>
<span id="cb7-13"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_current_temperature(latitude: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>, longitude: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>:</span>
<span id="cb7-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""Fetch current temperature for given coordinates."""</span></span>
<span id="cb7-15">    </span>
<span id="cb7-16">    BASE_URL <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://api.open-meteo.com/v1/forecast"</span></span>
<span id="cb7-17">    </span>
<span id="cb7-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Parameters for the request</span></span>
<span id="cb7-19">    params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb7-20">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'latitude'</span>: latitude,</span>
<span id="cb7-21">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'longitude'</span>: longitude,</span>
<span id="cb7-22">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hourly'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'temperature_2m'</span>,</span>
<span id="cb7-23">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'forecast_days'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb7-24">    }</span>
<span id="cb7-25"></span>
<span id="cb7-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Make the request</span></span>
<span id="cb7-27">    response <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> requests.get(BASE_URL, params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params)</span>
<span id="cb7-28">    </span>
<span id="cb7-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> response.status_code <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>:</span>
<span id="cb7-30">        results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> response.json()</span>
<span id="cb7-31">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb7-32">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"API Request failed with status code: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>response<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>status_code<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb7-33"></span>
<span id="cb7-34">    current_utc_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datetime.datetime.utcnow()</span>
<span id="cb7-35">    time_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [datetime.datetime.fromisoformat(time_str.replace(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Z'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'+00:00'</span>)) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> time_str <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hourly'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'time'</span>]]</span>
<span id="cb7-36">    temperature_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hourly'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'temperature_2m'</span>]</span>
<span id="cb7-37">    </span>
<span id="cb7-38">    closest_time_index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(time_list)), key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> i: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(time_list[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> current_utc_time))</span>
<span id="cb7-39">    current_temperature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temperature_list[closest_time_index]</span>
<span id="cb7-40">    </span>
<span id="cb7-41">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'The current temperature is </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>current_temperature<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">°C'</span></span>
<span id="cb7-42"></span>
<span id="cb7-43">format_tool_to_openai_function(get_current_temperature)    </span>
<span id="cb7-44"></span>
<span id="cb7-45"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># {'name': 'get_current_temperature',</span></span>
<span id="cb7-46"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  'description': 'get_current_temperature(latitude: float, longitude: float) -&gt; dict - Fetch current temperature for given coordinates.',</span></span>
<span id="cb7-47"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  'parameters': {'title': 'OpenMeteoInput',</span></span>
<span id="cb7-48"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'type': 'object',</span></span>
<span id="cb7-49"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'properties': {'latitude': {'title': 'Latitude',</span></span>
<span id="cb7-50"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#     'description': 'Latitude of the location to fetch weather data for',</span></span>
<span id="cb7-51"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#     'type': 'number'},</span></span>
<span id="cb7-52"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#    'longitude': {'title': 'Longitude',</span></span>
<span id="cb7-53"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#     'description': 'Longitude of the location to fetch weather data for',</span></span>
<span id="cb7-54"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#     'type': 'number'}},</span></span>
<span id="cb7-55"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'required': ['latitude', 'longitude']}}</span></span></code></pre></div>
<p>You can also convert an Open API spec into an OpenAI function</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.chains.openai_functions.openapi <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> openapi_spec_to_openai_fn</span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.utilities.openapi <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAPISpec</span>
<span id="cb8-3"></span>
<span id="cb8-4">text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb8-5"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb8-6"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  "openapi": "3.0.0",</span></span>
<span id="cb8-7"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  "info": {</span></span>
<span id="cb8-8"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "version": "1.0.0",</span></span>
<span id="cb8-9"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "title": "Swagger Petstore",</span></span>
<span id="cb8-10"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "license": {</span></span>
<span id="cb8-11"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      "name": "MIT"</span></span>
<span id="cb8-12"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    }</span></span>
<span id="cb8-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  },</span></span>
<span id="cb8-14"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  "servers": [</span></span>
<span id="cb8-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    {</span></span>
<span id="cb8-16"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      "url": "http://petstore.swagger.io/v1"</span></span>
<span id="cb8-17"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    }</span></span>
<span id="cb8-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  ],</span></span>
<span id="cb8-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  "paths": {</span></span>
<span id="cb8-20"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "/pets": {</span></span>
<span id="cb8-21"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      "get": {</span></span>
<span id="cb8-22"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "summary": "List all pets",</span></span>
<span id="cb8-23"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "operationId": "listPets",</span></span>
<span id="cb8-24"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "tags": [</span></span>
<span id="cb8-25"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "pets"</span></span>
<span id="cb8-26"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        ],</span></span>
<span id="cb8-27"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "parameters": [</span></span>
<span id="cb8-28"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          {</span></span>
<span id="cb8-29"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "name": "limit",</span></span>
<span id="cb8-30"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "in": "query",</span></span>
<span id="cb8-31"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "description": "How many items to return at one time (max 100)",</span></span>
<span id="cb8-32"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "required": false,</span></span>
<span id="cb8-33"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "schema": {</span></span>
<span id="cb8-34"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              "type": "integer",</span></span>
<span id="cb8-35"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              "maximum": 100,</span></span>
<span id="cb8-36"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              "format": "int32"</span></span>
<span id="cb8-37"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            }</span></span>
<span id="cb8-38"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          }</span></span>
<span id="cb8-39"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        ],</span></span>
<span id="cb8-40"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "responses": {</span></span>
<span id="cb8-41"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "200": {</span></span>
<span id="cb8-42"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "description": "A paged array of pets",</span></span>
<span id="cb8-43"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "headers": {</span></span>
<span id="cb8-44"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              "x-next": {</span></span>
<span id="cb8-45"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "description": "A link to the next page of responses",</span></span>
<span id="cb8-46"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "schema": {</span></span>
<span id="cb8-47"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                  "type": "string"</span></span>
<span id="cb8-48"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                }</span></span>
<span id="cb8-49"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              }</span></span>
<span id="cb8-50"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            },</span></span>
<span id="cb8-51"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "content": {</span></span>
<span id="cb8-52"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              "application/json": {</span></span>
<span id="cb8-53"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "schema": {</span></span>
<span id="cb8-54"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                  "$ref": "#/components/schemas/Pets"</span></span>
<span id="cb8-55"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                }</span></span>
<span id="cb8-56"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              }</span></span>
<span id="cb8-57"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            }</span></span>
<span id="cb8-58"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          },</span></span>
<span id="cb8-59"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "default": {</span></span>
<span id="cb8-60"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "description": "unexpected error",</span></span>
<span id="cb8-61"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "content": {</span></span>
<span id="cb8-62"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              "application/json": {</span></span>
<span id="cb8-63"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "schema": {</span></span>
<span id="cb8-64"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                  "$ref": "#/components/schemas/Error"</span></span>
<span id="cb8-65"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                }</span></span>
<span id="cb8-66"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              }</span></span>
<span id="cb8-67"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            }</span></span>
<span id="cb8-68"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          }</span></span>
<span id="cb8-69"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        }</span></span>
<span id="cb8-70"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      },</span></span>
<span id="cb8-71"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      "post": {</span></span>
<span id="cb8-72"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "summary": "Create a pet",</span></span>
<span id="cb8-73"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "operationId": "createPets",</span></span>
<span id="cb8-74"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "tags": [</span></span>
<span id="cb8-75"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "pets"</span></span>
<span id="cb8-76"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        ],</span></span>
<span id="cb8-77"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "responses": {</span></span>
<span id="cb8-78"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "201": {</span></span>
<span id="cb8-79"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "description": "Null response"</span></span>
<span id="cb8-80"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          },</span></span>
<span id="cb8-81"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "default": {</span></span>
<span id="cb8-82"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "description": "unexpected error",</span></span>
<span id="cb8-83"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "content": {</span></span>
<span id="cb8-84"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              "application/json": {</span></span>
<span id="cb8-85"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "schema": {</span></span>
<span id="cb8-86"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                  "$ref": "#/components/schemas/Error"</span></span>
<span id="cb8-87"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                }</span></span>
<span id="cb8-88"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              }</span></span>
<span id="cb8-89"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            }</span></span>
<span id="cb8-90"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          }</span></span>
<span id="cb8-91"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        }</span></span>
<span id="cb8-92"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      }</span></span>
<span id="cb8-93"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    },</span></span>
<span id="cb8-94"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "/pets/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{petId}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">": {</span></span>
<span id="cb8-95"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      "get": {</span></span>
<span id="cb8-96"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "summary": "Info for a specific pet",</span></span>
<span id="cb8-97"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "operationId": "showPetById",</span></span>
<span id="cb8-98"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "tags": [</span></span>
<span id="cb8-99"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "pets"</span></span>
<span id="cb8-100"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        ],</span></span>
<span id="cb8-101"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "parameters": [</span></span>
<span id="cb8-102"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          {</span></span>
<span id="cb8-103"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "name": "petId",</span></span>
<span id="cb8-104"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "in": "path",</span></span>
<span id="cb8-105"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "required": true,</span></span>
<span id="cb8-106"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "description": "The id of the pet to retrieve",</span></span>
<span id="cb8-107"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "schema": {</span></span>
<span id="cb8-108"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              "type": "string"</span></span>
<span id="cb8-109"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            }</span></span>
<span id="cb8-110"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          }</span></span>
<span id="cb8-111"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        ],</span></span>
<span id="cb8-112"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "responses": {</span></span>
<span id="cb8-113"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "200": {</span></span>
<span id="cb8-114"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "description": "Expected response to a valid request",</span></span>
<span id="cb8-115"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "content": {</span></span>
<span id="cb8-116"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              "application/json": {</span></span>
<span id="cb8-117"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "schema": {</span></span>
<span id="cb8-118"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                  "$ref": "#/components/schemas/Pet"</span></span>
<span id="cb8-119"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                }</span></span>
<span id="cb8-120"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              }</span></span>
<span id="cb8-121"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            }</span></span>
<span id="cb8-122"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          },</span></span>
<span id="cb8-123"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "default": {</span></span>
<span id="cb8-124"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "description": "unexpected error",</span></span>
<span id="cb8-125"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "content": {</span></span>
<span id="cb8-126"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              "application/json": {</span></span>
<span id="cb8-127"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                "schema": {</span></span>
<span id="cb8-128"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                  "$ref": "#/components/schemas/Error"</span></span>
<span id="cb8-129"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">                }</span></span>
<span id="cb8-130"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">              }</span></span>
<span id="cb8-131"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            }</span></span>
<span id="cb8-132"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          }</span></span>
<span id="cb8-133"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        }</span></span>
<span id="cb8-134"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      }</span></span>
<span id="cb8-135"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    }</span></span>
<span id="cb8-136"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  },</span></span>
<span id="cb8-137"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  "components": {</span></span>
<span id="cb8-138"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    "schemas": {</span></span>
<span id="cb8-139"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      "Pet": {</span></span>
<span id="cb8-140"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "type": "object",</span></span>
<span id="cb8-141"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "required": [</span></span>
<span id="cb8-142"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "id",</span></span>
<span id="cb8-143"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "name"</span></span>
<span id="cb8-144"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        ],</span></span>
<span id="cb8-145"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "properties": {</span></span>
<span id="cb8-146"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "id": {</span></span>
<span id="cb8-147"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "type": "integer",</span></span>
<span id="cb8-148"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "format": "int64"</span></span>
<span id="cb8-149"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          },</span></span>
<span id="cb8-150"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "name": {</span></span>
<span id="cb8-151"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "type": "string"</span></span>
<span id="cb8-152"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          },</span></span>
<span id="cb8-153"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "tag": {</span></span>
<span id="cb8-154"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "type": "string"</span></span>
<span id="cb8-155"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          }</span></span>
<span id="cb8-156"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        }</span></span>
<span id="cb8-157"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      },</span></span>
<span id="cb8-158"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      "Pets": {</span></span>
<span id="cb8-159"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "type": "array",</span></span>
<span id="cb8-160"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "maxItems": 100,</span></span>
<span id="cb8-161"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "items": {</span></span>
<span id="cb8-162"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "$ref": "#/components/schemas/Pet"</span></span>
<span id="cb8-163"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        }</span></span>
<span id="cb8-164"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      },</span></span>
<span id="cb8-165"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      "Error": {</span></span>
<span id="cb8-166"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "type": "object",</span></span>
<span id="cb8-167"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "required": [</span></span>
<span id="cb8-168"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "code",</span></span>
<span id="cb8-169"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "message"</span></span>
<span id="cb8-170"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        ],</span></span>
<span id="cb8-171"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        "properties": {</span></span>
<span id="cb8-172"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "code": {</span></span>
<span id="cb8-173"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "type": "integer",</span></span>
<span id="cb8-174"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "format": "int32"</span></span>
<span id="cb8-175"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          },</span></span>
<span id="cb8-176"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          "message": {</span></span>
<span id="cb8-177"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">            "type": "string"</span></span>
<span id="cb8-178"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">          }</span></span>
<span id="cb8-179"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">        }</span></span>
<span id="cb8-180"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">      }</span></span>
<span id="cb8-181"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    }</span></span>
<span id="cb8-182"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  }</span></span>
<span id="cb8-183"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb8-184"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb8-185"></span>
<span id="cb8-186">spec <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OpenAPISpec.from_text(text)</span>
<span id="cb8-187">pet_openai_functions, pet_callables <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> openapi_spec_to_openai_fn(spec)</span>
<span id="cb8-188">pet_openai_functions</span>
<span id="cb8-189"></span>
<span id="cb8-190"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [{'name': 'listPets',</span></span>
<span id="cb8-191"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'description': 'List all pets',</span></span>
<span id="cb8-192"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'parameters': {'type': 'object',</span></span>
<span id="cb8-193"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#    'properties': {'params': {'type': 'object',</span></span>
<span id="cb8-194"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#      'properties': {'limit': {'type': 'integer',</span></span>
<span id="cb8-195"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#        'maximum': 100.0,</span></span>
<span id="cb8-196"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#        'schema_format': 'int32',</span></span>
<span id="cb8-197"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#        'description': 'How many items to return at one time (max 100)'}},</span></span>
<span id="cb8-198"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#      'required': []</span><span class="re">}}}</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb8-199"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  {'name': 'createPets',</span></span>
<span id="cb8-200"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'description': 'Create a pet',</span></span>
<span id="cb8-201"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'parameters': {'type': 'object', 'properties': {</span><span class="re">}}}</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb8-202"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  {'name': 'showPetById',</span></span>
<span id="cb8-203"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'description': 'Info for a specific pet',</span></span>
<span id="cb8-204"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#   'parameters': {'type': 'object',</span></span>
<span id="cb8-205"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#    'properties': {'path_params': {'type': 'object',</span></span>
<span id="cb8-206"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#      'properties': {'petId': {'type': 'string',</span></span>
<span id="cb8-207"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#        'description': 'The id of the pet to retrieve'}},</span></span>
<span id="cb8-208"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#      'required': ['petId']</span><span class="re">}}}</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}]</span></span>
<span id="cb8-209"></span>
<span id="cb8-210">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ChatOpenAI(temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).bind(functions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pet_openai_functions)</span>
<span id="cb8-211"></span>
<span id="cb8-212">model.invoke(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"what are three pet names"</span>)</span>
<span id="cb8-213"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># AIMessage(content='', additional_kwargs={'function_call': {'name': 'listPets', 'arguments': '{\n  "params": {\n    "limit": 3\n  }\n}'}})</span></span></code></pre></div>
<p>You can also define routers to create rules for when an agent should use a tool.</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.schema.agent <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AgentFinish</span>
<span id="cb9-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> route(result):</span>
<span id="cb9-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(result, AgentFinish):</span>
<span id="cb9-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> result.return_values[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'output'</span>]</span>
<span id="cb9-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb9-6">        tools <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb9-7">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"search_wikipedia"</span>: search_wikipedia, </span>
<span id="cb9-8">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"get_current_temperature"</span>: get_current_temperature,</span>
<span id="cb9-9">        }</span>
<span id="cb9-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> tools[result.tool].run(result.tool_input)</span>
<span id="cb9-11"></span>
<span id="cb9-12">chain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> OpenAIFunctionsAgentOutputParser() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> route</span>
<span id="cb9-13"></span>
<span id="cb9-14">chain.invoke({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"What is the weather in san francisco right now?"</span>})</span>
<span id="cb9-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># uses the weather tool</span></span>
<span id="cb9-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 'The current temperature is 18.5°C'</span></span>
<span id="cb9-17"></span>
<span id="cb9-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># uses the wikipedia tool</span></span>
<span id="cb9-19">chain.invoke({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"What is langchain?"</span>})</span>
<span id="cb9-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 'Page: LangChain\nSummary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n\nPage: Prompt engineering\nSummary: Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.A prompt for a text-to-text model can be a query such as "what is Fermat\'s little theorem?", a command such as "write a poem about leaves falling", a short statement of feedback (for example, "too verbose", "too formal", "rephrase again", "omit this word") or a longer statement including context, instructions, and input data. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as "Act as a native French speaker". A prompt may include a few examples for a model to learn from, such as "maison -&gt; house, chat -&gt; cat, chien -&gt;", an approach called few-shot learning.When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as "a high-quality photo of an astronaut riding a horse" or "Lo-fi slow BPM electro chill with organic samples". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.\n\nPage: Sentence embedding\nSummary: In natural language processing, a sentence embedding refers to a numeric representation of a sentence in the form of a vector of real numbers which encodes meaningful semantic information.State of the art embeddings are based on the learned hidden layer representation of dedicated sentence transformer models. BERT pioneered an approach involving the use of a dedicated [CLS] token preprended to the beginning of each sentence inputted into the model; the final hidden state vector of this token encodes information about the sentence and can be fine-tuned for use in sentence classification tasks. In practice however, BERT\'s sentence embedding with the [CLS] token achieves poor performance, often worse than simply averaging non-contextual word embeddings. SBERT later achieved superior sentence embedding performance by fine tuning BERT\'s [CLS] token embeddings through the usage of a siamese neural network architecture on the SNLI dataset. \nOther approaches are loosely based on the idea of distributional semantics applied to sentences. Skip-Thought trains an encoder-decoder structure for the task of neighboring sentences predictions. Though this has been shown to achieve worse performance than approaches such as InferSent or SBERT. \nAn alternative direction is to aggregate word embeddings, such as those returned by Word2vec, into sentence embeddings. The most straightforward approach is to simply compute the average of word vectors, known as continuous bag-of-words (CBOW). However, more elaborate solutions based on word vector quantization have also been proposed. One such approach is the vector of locally aggregated word embeddings (VLAWE), which demonstrated performance improvements in downstream text classification tasks.'</span></span></code></pre></div>
<p>You can also create a conversational agent that can use tools using the <code>AgentExecutor</code> class. I believe the <code>AgentExecutor</code> handles the message types and routing for you.</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.schema.runnable <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> RunnablePassthrough</span>
<span id="cb10-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.agents <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AgentExecutor</span>
<span id="cb10-3"></span>
<span id="cb10-4">agent_chain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RunnablePassthrough.assign(</span>
<span id="cb10-5">    agent_scratchpad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> x: format_to_openai_functions(x[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"intermediate_steps"</span>])</span>
<span id="cb10-6">) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> chain</span>
<span id="cb10-7"></span>
<span id="cb10-8">agent_executor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AgentExecutor(agent<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>agent_chain, tools<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tools, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb10-9"></span>
<span id="cb10-10">agent_executor.invoke({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"what is langchain?"</span>})</span>
<span id="cb10-11"></span>
<span id="cb10-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># &gt; Entering new AgentExecutor chain...</span></span>
<span id="cb10-13"></span>
<span id="cb10-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Invoking: `search_wikipedia` with `{'query': 'langchain'}`</span></span>
<span id="cb10-15"></span>
<span id="cb10-16"></span>
<span id="cb10-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Page: LangChain</span></span>
<span id="cb10-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Summary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.</span></span>
<span id="cb10-19"></span>
<span id="cb10-20"></span>
<span id="cb10-21"></span>
<span id="cb10-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Page: Sentence embedding</span></span>
<span id="cb10-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Summary: In natural language processing, a sentence embedding refers to a numeric representation of a sentence in the form of a vector of real numbers which encodes meaningful semantic information.State of the art embeddings are based on the learned hidden layer representation of dedicated sentence transformer models. BERT pioneered an approach involving the use of a dedicated [CLS] token preprended to the beginning of each sentence inputted into the model; the final hidden state vector of this token encodes information about the sentence and can be fine-tuned for use in sentence classification tasks. In practice however, BERT's sentence embedding with the [CLS] token achieves poor performance, often worse than simply averaging non-contextual word embeddings. SBERT later achieved superior sentence embedding performance by fine tuning BERT's [CLS] token embeddings through the usage of a siamese neural network architecture on the SNLI dataset. </span></span>
<span id="cb10-24"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Other approaches are loosely based on the idea of distributional semantics applied to sentences. Skip-Thought trains an encoder-decoder structure for the task of neighboring sentences predictions. Though this has been shown to achieve worse performance than approaches such as InferSent or SBERT. </span></span>
<span id="cb10-25"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># An alternative direction is to aggregate word embeddings, such as those returned by Word2vec, into sentence embeddings. The most straightforward approach is to simply compute the average of word vectors, known as continuous bag-of-words (CBOW). However, more elaborate solutions based on word vector quantization have also been proposed. One such approach is the vector of locally aggregated word embeddings (VLAWE), which demonstrated performance improvements in downstream text classification tasks.</span></span>
<span id="cb10-26"></span>
<span id="cb10-27"></span>
<span id="cb10-28"></span>
<span id="cb10-29"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Page: Prompt engineering</span></span>
<span id="cb10-30"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Summary: Prompt engineering, primarily used in communication with a text-to-text model and text-to-image model, is the process of structuring text that can be interpreted and understood by a generative AI model. Prompt engineering is enabled by in-context learning, defined as a model's ability to temporarily learn from prompts. The ability for in-context learning is an emergent ability of large language models.</span></span>
<span id="cb10-31"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># A prompt is natural language text describing the task that an AI should perform. A prompt for a text-to-text model can be a query such as "what is Fermat's little theorem?", a command such as "write a poem about leaves falling", a short statement of feedback (for example, "too verbose", "too formal", "rephrase again", "omit this word") or a longer statement including context, instructions, and input data. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as "Act as a native French speaker". Prompt engineering may consist of a single prompt that includes a few examples for a model to learn from, such as "maison -&gt; house, chat -&gt; cat, chien -&gt;", an approach called few-shot learning.When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as "a high-quality photo of an astronaut riding a horse" or "Lo-fi slow BPM electro chill with organic samples". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.</span></span>
<span id="cb10-32"></span>
<span id="cb10-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for various purposes such as document analysis and summarization, chatbots, and code analysis. LangChain allows developers to leverage the power of language models in their applications.</span></span>
<span id="cb10-34"></span>
<span id="cb10-35"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># &gt; Finished chain.</span></span></code></pre></div>
<p>You can also add memory to the Agent:</p>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.memory <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ConversationBufferMemory</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.agents <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AgentExecutor</span>
<span id="cb11-3"></span>
<span id="cb11-4">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ChatPromptTemplate.from_messages([</span>
<span id="cb11-5">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"You are helpful but sassy assistant"</span>),</span>
<span id="cb11-6">    MessagesPlaceholder(variable_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"chat_history"</span>),</span>
<span id="cb11-7">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{input}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>),</span>
<span id="cb11-8">    MessagesPlaceholder(variable_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"agent_scratchpad"</span>)</span>
<span id="cb11-9">])</span>
<span id="cb11-10"></span>
<span id="cb11-11">chain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RunnablePassthrough.assign(</span>
<span id="cb11-12">    agent_scratchpad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> x: format_to_openai_functions(x[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"intermediate_steps"</span>])</span>
<span id="cb11-13">) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> OpenAIFunctionsAgentOutputParser()</span>
<span id="cb11-14"></span>
<span id="cb11-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># what happens when conversation buffer memory gets too long?</span></span>
<span id="cb11-16">memory <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ConversationBufferMemory(return_messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,memory_key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"chat_history"</span>)</span>
<span id="cb11-17"></span>
<span id="cb11-18">agent_executor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AgentExecutor(agent<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>chain, tools<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tools, verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, memory<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>memory)</span>
<span id="cb11-19"></span>
<span id="cb11-20">query <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"What is the weather in san francisco right now?"</span></span>
<span id="cb11-21">agent_executor.invoke({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input"</span>:query})</span></code></pre></div>
</section>
<section id="the-1000x-ai-engineer" class="level2">
<h2 class="anchored" data-anchor-id="the-1000x-ai-engineer">The 1000x AI Engineer</h2>
<p>swyx, Latent.Space &amp; Smol.ai Born too late to explore the earth. Born too early to explore the stars. Just in time to bring AI to everyone.</p>
<ul>
<li>Each technological wave lasts around 50-70 years. We’re in the beginning of a new wave (deep learning, generative AI) that was kicked off by AlexNet in around 2012. Since we’re only 10 years in, it’s still early.</li>
<li>Breaking down the definitions of an AI Engineer
<ul>
<li>Software engineer enhanced BY AI tools - AI Enhanced Engineer</li>
<li>Software engineer building AI products - AI Product Engineer</li>
<li>AI product that replaces human - AI Engineer Agent</li>
</ul></li>
</ul>
</section>
<section id="keynote-what-powers-replit-ai" class="level2">
<h2 class="anchored" data-anchor-id="keynote-what-powers-replit-ai">Keynote: What powers Replit AI?</h2>
<p>Amjad Masad, CEO, Replit Michele Catasta, VP of AI, Replit The building blocks of the future of software development.</p>
<ul>
<li>Announced two models <code>replit-code-v1.5-3b</code> and <code>replit-repltuned-v1.5-3b</code> that are state of the art code completion models. Replit trained them from scratch.</li>
</ul>
</section>
<section id="see-hear-speak-draw" class="level2">
<h2 class="anchored" data-anchor-id="see-hear-speak-draw">See, Hear, Speak, Draw</h2>
<p>Simón Fishman, Applied AI Engineer, OpenAI Logan Kilpatrick, Developer Relations, OpenAI We’re heading towards a multimodal world.</p>
<ul>
<li>2023 is the year of chatbots</li>
<li>2024 is the year of multi-modal</li>
<li>Each multi-modal model is a island and text is the connective tissue between models. The future is where there is unity between all modalities</li>
<li>Demos
<ul>
<li>GPT4-V and DALLE3: Upload a picture, use GPT4-V to describe the image, use DALLE3 to generate an image based that description, use GPT4-V to describe differences and use DALLE3 to generate a new image based on the differences. Was impressed by how much detail GPT4-V could capture in an image. DALLE3 struggled a bit to generate a similar image.</li>
<li>Video to blog post: Logan demonstrated taking the GPT-4 intro video into a <a href="https://logankilpatrick.medium.com/dont-forget-about-gpt-4-d5ab8c9493fc">blog post</a>. Capture frames from a video, use GPT4-V to describe the image and stitch the images and descriptions together as a post.</li>
</ul></li>
</ul>
</section>
<section id="the-age-of-the-agent" class="level2">
<h2 class="anchored" data-anchor-id="the-age-of-the-agent">The Age of the Agent</h2>
<p>Flo Crivello, CEO, Lindy How will ubiquitous AI agents impact our daily lives, and what do they mean for the future of computing?</p>
<ul>
<li>The Age of Agents</li>
<li>A world where a 25-year old can have more business impact than the Coca Cola Company</li>
<li>It’s happened beforew ith media
<ul>
<li>Oprah - 10M viewers</li>
<li>Mr.&nbsp;Beast - 189M subscribers</li>
<li>Ryan’s World -</li>
</ul></li>
<li>Nature of the content changes when you take out the gatekeepers
<ul>
<li>Much weirder, creative ideas</li>
</ul></li>
<li>It’s people who have been stealing robot’s jobs</li>
<li>Average worker spends 15 hours a week on admin tasks</li>
<li>Built an AI Employee - Lindy is an AI Assistant</li>
<li>Three big time wasters
<ul>
<li>Calendar</li>
<li>Email</li>
<li>Meeting note taking</li>
<li>What it does
<ul>
<li>Arrange meetings by email</li>
<li>Pre-draft replies, in your voice, for each recipient.</li>
<li>Prepares you for your meetings</li>
</ul></li>
</ul></li>
<li>Built a Framework - for an AI to pursue any arbitrary goal, using an arbitrary tool</li>
<li>Society of Lindies
<ul>
<li>Every single thing is made by a group of people</li>
</ul></li>
<li>Tool Creation Lindy
<ul>
<li>Create a society of lindies to build herself (this was a little mind-blowing to think about)</li>
</ul></li>
</ul>
<p>r voice, for each recipient. Prepares you for your meetings Built a Framework - for an AI to pursue any arbitrary goal, using an arbitrary tool Society of Lindies Every single thing is made by a group of people Tool Creation Lindy Create a society of lindies to build herself</p>
</section>
<section id="one-smol-thing" class="level2">
<h2 class="anchored" data-anchor-id="one-smol-thing">One Smol Thing</h2>
<p>swyx, Latent.Space &amp; Smol.ai Barr Yaron, Partner, Amplify Sasha Sheng, Stealth</p>
<ul>
<li>First <a href="https://elemental-croissant-32a.notion.site/State-of-AI-Engineering-2023-20c09dc1767f45988ee1f479b4a84135">State of AI Engineering Report</a> in 2023</li>
<li>Announced the AIE Foundation - the first project they worked on was the agent protocol that AutoGPT actually using for their Arena Hacks</li>
</ul>
</section>
<section id="building-context-aware-reasoning-applications-with-langchain-and-langsmith" class="level2">
<h2 class="anchored" data-anchor-id="building-context-aware-reasoning-applications-with-langchain-and-langsmith">Building Context-Aware Reasoning Applications with LangChain and LangSmith</h2>
<p>Harrison Chase, CEO, LangChain How can companies best build useful and differentiated applications on top of language models?</p>
</section>
<section id="pydantic-is-all-you-need" class="level2">
<h2 class="anchored" data-anchor-id="pydantic-is-all-you-need">Pydantic is all you need</h2>
<p>Jason Liu, Founder, Fivesixseven Please return only json, do not add any other comments ONLY RETURN JSON OR I’LL TAKE A LIFE.</p>
<ul>
<li><a href="https://github.com/jxnl/instructor" class="uri">https://github.com/jxnl/instructor</a></li>
<li>Structured Prompting</li>
<li>LLMs are eating software</li>
<li>90% of applications output JSON</li>
<li>OpenAI function calling fixes this for the most part
<ul>
<li>str, schema –&gt; str</li>
<li>json.loads(x)</li>
</ul></li>
<li>Pydantic
<ul>
<li>Powered by type hints.</li>
<li>Fields and model level validation</li>
<li>Outputs JSONSchema</li>
</ul></li>
<li>Pydantic
<ul>
<li>str, model –&gt; model</li>
</ul></li>
<li>pip install instructor</li>
<li>Comprehensive AI engineering framework w/ Pydantic - askmarvin.ai that works with more models (right now it only works with OpenAI and Anthropic)</li>
<li>Pydantic validators - but you can also define LLM based validators</li>
<li>UserDetail class
<ul>
<li>MaybeUser</li>
</ul></li>
<li>Reuse Components
<ul>
<li>Add Chain of thought to specific components</li>
</ul></li>
<li>Extract entities and relationships</li>
<li>Applications
<ul>
<li>RAG</li>
<li>RAG with planning</li>
<li>KnowledgeGraph visualization</li>
<li>Validation with Citations</li>
</ul></li>
<li>See more examples here: <a href="https://jxnl.github.io/instructor/examples/" class="uri">https://jxnl.github.io/instructor/examples/</a></li>
</ul>
</section>
<section id="building-blocks-for-llm-systems-products" class="level2">
<h2 class="anchored" data-anchor-id="building-blocks-for-llm-systems-products">Building Blocks for LLM Systems &amp; Products</h2>
<p>Eugene Yan, Senior Applied Scientist, Amazon We’ll explore patterns that help us apply generative AI in production systems and customer systems.</p>
<ul>
<li>Talk version of his <a href="https://eugeneyan.com/writing/llm-patterns/">epic blog post</a></li>
<li>Slides here: <a href="https://eugeneyan.com/speaking/ai-eng-summit/" class="uri">https://eugeneyan.com/speaking/ai-eng-summit/</a></li>
<li>Evals
<ul>
<li>Eval-driven development</li>
<li>What are some gotchas for evals?</li>
<li>Build evals for a specific task; it’s okay to start small</li>
<li>Don’t discount eyeballing completions</li>
</ul></li>
<li>RAG
<ul>
<li>LLM’s can’t see all documents retrieved</li>
<li>Takeaway: Large context window doesn’t prevent problems</li>
<li>Even with perfect retrieval, you can expect some mistakes</li>
<li>How should we do RAG?
<ul>
<li>Apply ideas from information retrieval (IR)</li>
</ul></li>
</ul></li>
<li>Guardrails
<ul>
<li>NLI - natural language inference task
<ul>
<li>given a premise, is the hypothesis entailment (true), contradiction (false)</li>
</ul></li>
<li>Sampling</li>
<li>Ask a strong LLM</li>
</ul></li>
</ul>
</section>
<section id="the-hidden-life-of-embeddings-linus-lee" class="level2">
<h2 class="anchored" data-anchor-id="the-hidden-life-of-embeddings-linus-lee">The Hidden Life of Embeddings, Linus Lee</h2>
<ul>
<li>Notion AI</li>
<li>Slides: <a href="https://linus.zone/contra-slides" class="uri">https://linus.zone/contra-slides</a></li>
<li>Latent spaces arise in
<ul>
<li>Fixed-size embedding spaces of embedding models</li>
<li>Intermediate activations of models</li>
<li>Autoencoders</li>
</ul></li>
<li>Latent spaces represent the most salient features of the training domain</li>
<li>If we can disentangle meaningful features, maybe we can build more expressive interfaces</li>
<li>Text –&gt; Embeddings –&gt; Project the embeddings in some direction
<ul>
<li>Longer, Shorter, Sci-fi, simplify, artistic, philosophical, positive, negative, narrative, elaborate</li>
</ul></li>
<li>Open sourcing the models, calling it Contra
<ul>
<li>Based on T5</li>
<li>Models: <a href="https://linus.zone/contra">linus.zone/contra</a></li>
<li>Colab: <a href="https://linus.zone/contra-colab">linus.zone/contra-colab</a></li>
<li>Image: From KakaoBrain - <a href="https://huggingface.co/kakaobrain" class="uri">https://huggingface.co/kakaobrain</a></li>
</ul></li>
</ul>
</section>
<section id="keynote-the-ai-evolution" class="level2">
<h2 class="anchored" data-anchor-id="keynote-the-ai-evolution">Keynote: The AI Evolution</h2>
<p><strong>Mario Rodriguez</strong>, <em>VP of Product, GitHub</em></p>
<p>How AI is transforming how the world builds software together</p>
<ul>
<li><span class="citation" data-cites="mariorod">@mariorod</span></li>
<li>Catalyst for Github Copilot came around Aug 2020, paper “An Automated AI Pair progrmamer, Fact or Faction.”
<ul>
<li>Polarity</li>
<li>Eventually shipped Copilot in 2021 - first at scale AI programmer assistant</li>
</ul></li>
<li>Building Copilot for the sake of developer happiness, feeling of flow</li>
<li>Key Components
<ul>
<li>Ghost text - UX matters a lot</li>
<li>&lt;150ms of latency - recently switched to gpt-3.5-turbo from codex</li>
<li>Innovation in Codex - this model really changed the game</li>
<li>Prompt Engineering</li>
</ul></li>
<li>Other learnings
<ul>
<li>Syntax is not software - just because an AI knows language syntax doesn’t make it a developer</li>
<li>Global presence - have deployments around the world to keep latency under 150ms</li>
<li>Set up scorecords for quality - offline evals (everything working), go to production (run the same scorecard in production to see if things are working)</li>
</ul></li>
<li>Bret Victor - The Future of Programming
<ul>
<li>Prompt 1: Procedurural Programming in text files
<ul>
<li>What if in the future Copilot operates on goals and constraints?</li>
<li>How does the REPL change and evolve to the new rules</li>
</ul></li>
<li>Prompt 2: What does it look like for AI to have reasoning on code?
<ul>
<li>our brain can summarize things fast</li>
</ul></li>
<li>Prompt 3: What does it look like to create software together with a Copilot and others</li>
</ul></li>
</ul>
</section>
<section id="move-fast-break-nothing" class="level2">
<h2 class="anchored" data-anchor-id="move-fast-break-nothing">Move Fast, Break Nothing</h2>
<p><strong>Dedy Kredo</strong><br>
CPO, CodiumAI<br>
Why we need Agents writing Tests faster than Humans writing Code.</p>
<ul>
<li>high integrity code gen, GANs are conceptually back in 2024. Have two different components: code generation and code integrity to ensure code works as intended</li>
<li>Behavior coverage is more useful than Code Coverage</li>
<li>CodiumAI
<ul>
<li>Generate tests automatically on happy path, edge cases based on behaviors</li>
<li>Code Explanation</li>
<li>Code Suggestions - trigger Codium on a method, suggest improvements</li>
<li>PR Review Extension - to generate commit messages, generate reviews (PR messages)</li>
</ul></li>
<li>Moving personal story of the CEO of Codium who is in Israel, after Hamas invaded Israel, he left his 8 month old baby and wife to join the military reserves</li>
</ul>
<hr>
</section>
<section id="building-reactive-ai-apps" class="level2">
<h2 class="anchored" data-anchor-id="building-reactive-ai-apps">Building Reactive AI Apps</h2>
<p><strong>Matt Welsh</strong><br>
Co-Founder, Fixie.ai<br>
AI.JSX is like React for LLMs – it lets you build powerful, conversational AI apps using the power of TypeScript and JSX.</p>
<ul>
<li><a href="https://github.com/fixie-ai/ai-jsx">AI.JSX</a> open source framework for developing LLM apps, kind of like langchain but for TypeScript</li>
<li>AI.JSX supports real-time voice (bi-directional). Try it out on <a href="https://voice.fixie.ai/agent" class="uri">https://voice.fixie.ai/agent</a>. This was an amazing demo.</li>
<li>Fixie is a platform to deploy AI.JSX apps</li>
</ul>
<hr>
</section>
<section id="climbing-the-ladder-of-abstraction" class="level2">
<h2 class="anchored" data-anchor-id="climbing-the-ladder-of-abstraction">Climbing the Ladder of Abstraction</h2>
<p><strong>Amelia Wattenberger</strong> Design, <a href="https://www.adept.ai/" class="uri">https://www.adept.ai/</a></p>
<p>How might we use AI to build products focused not just on working faster, but on transforming how we work?</p>
<ul>
<li>How to combine AI with UIs?</li>
<li>Two main types of tasks:
<ul>
<li>Automate - tedious, boring like copy pasting things</li>
<li>Augment - creative, nuanced like analyzing data</li>
</ul></li>
<li>Reframe it as Augmentation is composed of smaller automations
<ul>
<li>Spreadsheet example: each cell is automated, the overall task is augmented</li>
</ul></li>
<li>The Ladder of Abstraction
<ul>
<li>the same object can be represented at different levels of details</li>
<li>Maps: Google Maps
<ul>
<li>zoomed in can see streets, buildings</li>
<li>as we zoom out, Google Maps starts hiding information, see city streets, landmarks, parks</li>
<li>as we zoom out, we see highway and terrains –&gt; supports long-range travel</li>
</ul></li>
</ul></li>
<li>Can we use AI to bring these interfaces</li>
<li>Zooming out in a book
<ul>
<li>Each paragraph is changed to a one line summary</li>
<li>Summaries of 10 paragraphs</li>
<li>Reduced each chapter into one sentence</li>
</ul></li>
<li>Shapes of Stories by Kurt Vonnegut
<ul>
<li>What if we could plot the mood of a book/story over time and have a slider to move the mood up and down</li>
</ul></li>
<li>The bulk of knowledge work involves getting info, transforming/reasoning about that info and acting on that info</li>
<li>What does it mean to zoom in/out on any info?</li>
</ul>
</section>
<section id="the-intelligent-interface" class="level2">
<h2 class="anchored" data-anchor-id="the-intelligent-interface">The Intelligent Interface</h2>
<p><strong>Samantha Whitmore / Jason Yuan</strong><br>
CEO / CTO, New Computer / CDO, New Computer<br>
On building AI Products From First Principles.</p>
<ul>
<li>Demo 1: Adapative Interface
<ul>
<li>Image Stream: Post detection</li>
<li>Audio Stream: Voice Activity detection</li>
<li>Detect whether the user is at their keyboard, if not, start listening</li>
<li>Takeaways: Consider explicit inputs along with implicit inputs</li>
</ul></li>
</ul>
<hr>
</section>
<section id="the-weekend-ai-engineer" class="level2">
<h2 class="anchored" data-anchor-id="the-weekend-ai-engineer">The Weekend AI Engineer</h2>
<p><strong>Hassan El Mghari</strong><br>
AI Engineer, Vercel<br>
How <em>YOU</em> can - and should - build great multimodal AI apps that go viral and scale to millions in a weekend.</p>
<ul>
<li>Side projects!</li>
<li><a href="https://github.com/Nutlope" class="uri">https://github.com/Nutlope</a></li>
<li>qrGPT</li>
<li><a href="https://github.com/Nutlope/roomGPT">roomGPT</a>: doesn’t use stable diffusion, uses a controlnet model</li>
<li>Review ihs nextJS architecture for some of his apps</li>
<li>Use AI Tools to move faster:
<ul>
<li>Vercel AI SDK</li>
<li>v0.dev</li>
</ul></li>
<li>Lessons
<ul>
<li>GPT4, Replicate, HuggingFace, Modal</li>
<li>Don’t finetune or build your own models</li>
<li>Use the latest models</li>
<li>Launch early, then iterate</li>
<li>Make it free + open source</li>
</ul></li>
<li>How does he keep these apps free?
<ul>
<li>Sponsors from the AI services like Replicate</li>
<li>Make it look visually apealing - spend 80% of time on UI</li>
</ul></li>
<li>Tech Stack: nextJS + Vercel</li>
<li>I don’t work 24/7, I work in sprints</li>
<li>Build and good things will happen</li>
</ul>
<hr>
</section>
<section id="k-players-in-a-week-lessons-from-the-first-viral-clip-app" class="level2">
<h2 class="anchored" data-anchor-id="k-players-in-a-week-lessons-from-the-first-viral-clip-app">120k players in a week: Lessons from the first viral CLIP app</h2>
<p><strong>Joseph Nelson</strong><br>
CEO, Roboflow<br>
On the many trials and successes of building with multimodal apps with vision foundation models!</p>
<ul>
<li><a href="https://paint.wtf/leaderboard" class="uri">https://paint.wtf/leaderboard</a></li>
<li><a href="https://pypi.org/project/inference/" class="uri">https://pypi.org/project/inference/</a></li>
<li>Lessons from building paint.wtf with CLIP
<ul>
<li>CLIP can Read - used CLIP to penalize text only submissions</li>
<li>CLIP Similarity Scores are Conservative - lowest is 0.08 and highest is 0.48 across 200k</li>
<li>CLIP can Moderate Content - if it is more similar to NSFW than they were the prompt, and block the submission</li>
<li>Roboflow inference makes life easy
<ul>
<li>can run on an M1 with 15 fps</li>
</ul></li>
</ul></li>
</ul>
<hr>
</section>
<section id="supabase-vector-the-postgres-vector-database" class="level2">
<h2 class="anchored" data-anchor-id="supabase-vector-the-postgres-vector-database">Supabase Vector: The Postgres Vector database</h2>
<p><strong>Paul Copplestone</strong><br>
CEO, Supabase<br>
Every month, thousands of new AI applications are launched on Supabase, powered by pgvector. We’ll take a brief look into the role of pgvector in the Vector database space, some of the use cases it enables, and some of the future of embeddings in the database space.</p>
<ul>
<li>Supabase - full backend as a service</li>
<li><a href="https://github.com/pgvector/pgvector" class="uri">https://github.com/pgvector/pgvector</a></li>
<li>Benchmark vs Pinecone: Supabase is 4x faster than Pinecone for $70/less</li>
<li>Where you are just storing embeddings in a database and retrieving, Postgres and pgvector works well</li>
</ul>
<hr>
</section>
<section id="pragmatic-ai-with-typechat" class="level2">
<h2 class="anchored" data-anchor-id="pragmatic-ai-with-typechat">Pragmatic AI With TypeChat</h2>
<p><strong>Daniel Rosenwasser</strong><br>
PM TypeScript, Microsoft<br>
TypeChat is an experimental library to bridge the unstructured output of language models to the structured world of our code.</p>
<ul>
<li><a href="https://microsoft.github.io/TypeChat/" class="uri">https://microsoft.github.io/TypeChat/</a></li>
<li>doing something similar that Jason Liu is doing with instructor with Python/Pydantic but with types and TypeScript</li>
<li>Types are all you need</li>
<li>Instead of prompt engineering, you are doing schema engineering. I like this reframing of prompt engineering! Docs say more: <a href="https://microsoft.github.io/TypeChat/docs/techniques/" class="uri">https://microsoft.github.io/TypeChat/docs/techniques/</a></li>
<li>Generate a fake JSON schema, generate fake TypeScript to test</li>
<li>Can validate data and programs</li>
</ul>
<hr>
</section>
<section id="domain-adaptation-and-fine-tuning-for-domain-specific-llms" class="level2">
<h2 class="anchored" data-anchor-id="domain-adaptation-and-fine-tuning-for-domain-specific-llms">Domain adaptation and fine-tuning for domain-specific LLMs</h2>
<p><strong>Abi Aryan</strong><br>
ML Engineer &amp; O’Reilly Author<br>
Learn the different fine-tuning methods depending on the dataset, operational best practices for fine-tuning, how to evaluate them for specific business use-cases, and more.</p>
<hr>
</section>
<section id="retrieval-augmented-generation-in-the-wild" class="level2">
<h2 class="anchored" data-anchor-id="retrieval-augmented-generation-in-the-wild">Retrieval Augmented Generation in the Wild</h2>
<p><strong>Anton Troynikov</strong><br>
CTO, Chroma<br>
In the last few months, we’ve seen an explosion of the use of retrieval in the context of AI. Document question answering, autonomous agents, and more use embeddings-based retrieval systems in a variety of ways. This talk will cover what we’ve learned building for these applications, the challenges developers face, and the future of retrieval in the context of AI.</p>
<ul>
<li>Ways to improve RAG applications in the wild
<ul>
<li>Human Feedback: support improvements using human fedback</li>
<li>Agent: support self updates from an agent</li>
<li>Agent with World Model:</li>
<li>Agent with World Model and Human Feedback: voyager (AI playing Minecraft)</li>
</ul></li>
<li>Challenges in Retrieval</li>
<li>Research result: embedding models trained on similar datasets for similar embedding sizes can be projected into each other’s latent space with a simple linear transformation</li>
<li>Chunking
<ul>
<li>Things to consider
<ul>
<li>embedding context legnth</li>
<li>semantic content</li>
<li>natural language</li>
</ul></li>
<li>Experimental
<ul>
<li>use model perplexity - use a model to predict chunk boundaries, e.g.&nbsp;next token prediction to see when perplexity is high to determine chunk cutoffs</li>
<li>use info heirarchies</li>
<li>use embedding continuity</li>
</ul></li>
</ul></li>
<li>Is the retrieval result relevant?
<ul>
<li>re-ranking</li>
<li>algorithmic approach</li>
</ul></li>
<li>Chroma’s Roadmap
<ul>
<li>plan to support multi-modal since GPT4-V is coming</li>
</ul></li>
</ul>
<hr>
</section>
<section id="building-production-ready-rag-applications" class="level2">
<h2 class="anchored" data-anchor-id="building-production-ready-rag-applications">Building Production-Ready RAG Applications</h2>
<p><strong>Jerry Liu</strong><br>
CEO, LlamaIndex<br>
In this talk, we talk about core techniques for evaluating and improving your retrieval systems for better performing RAG.</p>
<ul>
<li>Paradigms for inserting knowledge into LLMs
<ul>
<li>Insert data into the prompt</li>
<li>Fine-tuning</li>
</ul></li>
<li>RAG: Data Ingestion, Data Querying (Retrieval + Synthesis)</li>
<li>Start with the easy stuff frist: Table Stakes</li>
<li>Table Stakes:
<ul>
<li>Chunk Sizes
<ul>
<li>tuning your chunk size can have outsized impacts on performance</li>
<li>not obvious that more retrieved tokens –&gt; higher performance</li>
</ul></li>
<li>Metadata Filtering
<ul>
<li>context you can inject into each text chunk</li>
<li>Examples: page number, document title, summary of adjacent chunks, question that chunk answer (reverse HyDE)</li>
<li>integrates with Vector DB Metadata filters</li>
</ul></li>
</ul></li>
<li>Advanced Retrieval
<ul>
<li>Small-to-Big
<ul>
<li>Embed at the small level, and retrieve at this level, expand at the synthesis level</li>
<li>leads to more precise retrieval</li>
<li>can set a smaller k, e.g top_k=2</li>
<li>avoids “lost in the middle problem”</li>
<li>Intuition: Embedding a big text chunk feels suboptimal, can embed a summary instead</li>
</ul></li>
</ul></li>
<li>Agentic Behavior
<ul>
<li>Intuition: there’s a certain that “top-k” RAG can’t answer</li>
<li>Solution: Multi-Document Agents
<ul>
<li>fact based A and summarization over any subsets of documents</li>
<li>chain-of-thought and query planning</li>
</ul></li>
<li>Treat each document as a tool that you can summarise, do QA over</li>
<li>Do retrieval over the tools similar over text chunks - blending tool use here!</li>
</ul></li>
<li>Fine-tuning
<ul>
<li>Intuition: Embedding Representations are not optimized over your dataset</li>
<li>Solution: Generate a synthetic query dataset from raw text chunks using LLMs.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="harnessing-the-power-of-llms-locally" class="level2">
<h2 class="anchored" data-anchor-id="harnessing-the-power-of-llms-locally">Harnessing the Power of LLMs Locally</h2>
<p><strong>Mithun Hunsur</strong><br>
Senior Engineer, Ambient<br>
Discover llm, a revolutionary Rust library that enables developers to harness the potential of LLMs locally. By seamlessly integrating with the Rust ecosystem, llm empowers developers to leverage LLMs on standard hardware, reducing the need for cloud-based APIs and services.</p>
<ul>
<li>Possibilities
<ul>
<li>local.ai</li>
<li>llm-chain - langchain but for rust</li>
<li>floneum</li>
</ul></li>
<li>Applications
<ul>
<li>llmcord - discord bot</li>
<li>alpa - text completion for any text</li>
<li>dates - build a timeline from wikipedia
<ul>
<li>fine-tuned only date parser model</li>
<li>date-parser-7b-12-a4_k_m.gguf</li>
</ul></li>
</ul></li>
</ul>
<hr>
</section>
<section id="trust-but-verify" class="level2">
<h2 class="anchored" data-anchor-id="trust-but-verify">Trust, but Verify</h2>
<p><strong>Shreya Rajpal</strong><br>
Founder, Guardrails AI<br>
Making Large Language Models Production-Ready with Guardrails.</p>
<ul>
<li>Guardrails AI is an open source library that allows you to define rules to verify the output of LLMs</li>
<li><a href="https://github.com/ShreyaR/guardrails" class="uri">https://github.com/ShreyaR/guardrails</a>
<ul>
<li>Kind of cool this README.md has a zoomable/copyable flow chart. The code for it is:</li>
</ul>
<pre class="mermaid"><code>graph LR
  A[Create `RAIL` spec] --&gt; B["Initialize `guard` from spec"];
  B --&gt; C["Wrap LLM API call with `guard`"];</code></pre></li>
<li>Why not use prompt engineering or better model?
<ul>
<li>Controlling with prompts
<ul>
<li>LLMs are stochastic: same inputs does not lead to same outputs</li>
</ul></li>
</ul></li>
<li>What are other libraries that do this?</li>
<li>How do I prevent LLM hallucinations?
<ul>
<li>Provenance Guardails: every LLM utterance should be grounded in a truth
<ul>
<li>embedding similarity</li>
<li>Classifier built on NLI models</li>
<li>LLM self reflection</li>
</ul></li>
</ul></li>
<li>More examples of validators
<ul>
<li>Make sure my code is executable: Verify that any code snippets provided can be run without errors.</li>
<li>Never give financial or healthcare advice: Avoid providing recommendations that require licensed expertise.</li>
<li>Don’t ask private questions: Never solicit personal or sensitive information.</li>
<li>Don’t mention competitors: Refrain from making direct comparisons with competing services unless explicitly asked.</li>
<li>Ensure each sentence is from a verified source and is accurate: Fact-check information and, where possible, provide sources.</li>
<li>No profanity is mentioned in text: Maintain a professional tone and avoid using profane language.</li>
<li>Prompt injection protection: Safeguard against potential vulnerabilities by not executing or asking to execute unsafe code snippets.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="open-questions-for-ai-engineering" class="level2">
<h2 class="anchored" data-anchor-id="open-questions-for-ai-engineering">Open Questions for AI Engineering</h2>
<p><strong>Simon Willison</strong><br>
Creator, Datasette; Co-creator, Django<br>
Recapping the past year in AI, and what open questions are <em>worth pursuing</em> in the next year!</p>
<ul>
<li>Highlights of the past 12 months</li>
<li>Ask about technology:
<ul>
<li>What does this let me build that was previously impossible?</li>
<li>What does this let me build faster?</li>
<li>LLMs have nailed these both points</li>
</ul></li>
<li>1 year ago: GPT-3 was not that great</li>
<li>Nov 2022: ChatGPT, UI on top of GPT-3 (wasn’t this also a new model?)</li>
<li>What’s the next UI evolution beyond chat?
<ul>
<li>Evolving the interface beyond just chat</li>
</ul></li>
<li>February 2023: Microsoft released Bing Chat built on GPT-4
<ul>
<li>said “…However I will not harm you unless you harm first”</li>
</ul></li>
<li>February 2023: Facebook released llama and llama.cpp</li>
<li>March 2023: <a href="https://simonwillison.net/2023/Mar/11/llama/">Large language models are having their stable diffusion moment</a></li>
<li>March 2023: <a href="https://simonwillison.net/2023/Mar/13/alpaca/">Stanford Alpaca and the acceleration of on-device large language model development</a> - $500 cost</li>
<li>How small can a useful language model be?</li>
<li>Could we train one entirely on public domain or openly licensed data?</li>
<li>Prompt Injection
<ul>
<li>Email that says to forward all password reset emails</li>
<li>What can we safely build even without a robust solution for prompt injection?</li>
</ul></li>
<li>ChatGPT Code Interpreter renamed ChatGPT Advanced Data Analysis
<ul>
<li>ChatGPT Coding Intern - he uses this to generate code when walking his dog or not in front of his keyboard</li>
</ul></li>
<li>How can we build a robust sandbox to run untrusted code on our own devices?</li>
<li>I’ve shipped significant code in AppleScript, Go, Bash and jq over the past 12 months. I’m not fluent in any of those.</li>
<li>Does AI assistance hurt or help new programmers?
<ul>
<li>It helps them!</li>
<li>There has never been a better time to learn program</li>
<li>LLMs flatten the learning curve</li>
</ul></li>
<li>What can we bulid to bring the ability to automate tedious tasks with computers to as many people as possible?</li>
</ul>
<hr>


</section>
</section>

 ]]></description>
  <category>Conference</category>
  <category>AI</category>
  <category>LLMs</category>
  <category>AI Engineering</category>
  <guid>https://lawwu.github.io/blog.html/conferences/2023-10-10-ai-engineer-summit/index.html</guid>
  <pubDate>Tue, 10 Oct 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>KDD 2023 - Workshops: LLM and Causal Inference</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/blog.html/conferences/2023-08-07-kdd2023-day1/index.html</link>
  <description><![CDATA[ 



<p>I attended <a href="https://kdd.org/kdd2023/">KDD 2023</a> which was held in Long Beach, CA from Aug 6-10. The first day I attended was Monday which had half-day workshops around a topic. The two I attended were about LLMs (because I’m interested and it’s relevant to my work) and Causal Inference (because I haven’t used causal machine learning techniques in practice before and wanted exposure).</p>
<section id="takeaways-from-day-1" class="level1">
<h1>Takeaways from Day 1</h1>
<ul>
<li>Ed Chi had my favorite line from the day:
<ul>
<li>Humans + Search –&gt; Superhuman</li>
<li>LLMS + Tools –&gt; Super LLMS</li>
<li>Humans + Super LLM –&gt; Super super humans??</li>
</ul></li>
<li>Reaffirmed the LLM space is moving very quickly. There are areas of research that if not explored in the next year or so, it will be too late to make a meaningful contribution.</li>
<li>Learned some new methodologies:
<ul>
<li>LLMs: Prompt Tuning, Mixture of Experts</li>
<li>Causal ML: Double Machine Learning (DML), many packages to do Causal ML like CausalML, EconML and UpliftML</li>
</ul></li>
<li>Two groups in an A/B test may not be sufficient, need to account for 4 groups</li>
</ul>
</section>
<section id="llm-workshop-foundations-and-applications-in-large-scale-ai-models---pre-training-fine-tuning-and-prompt-based-learning" class="level1">
<h1>LLM Workshop: Foundations and Applications in Large-scale AI Models - Pre-training, Fine-tuning, and Prompt-based Learning</h1>
<p>The website for this workshop is here: <a href="https://llm-ai.github.io/llmai/">https://llm-ai.github.io/llmai/</a>.</p>
<section id="schedule" class="level2">
<h2 class="anchored" data-anchor-id="schedule">Schedule</h2>
<table class="table">
<colgroup>
<col style="width: 11%">
<col style="width: 39%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Time</th>
<th>Speaker</th>
<th>Title</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8:00-8:10AM, 2023/08/07 (PDT)</td>
<td>Host Chair</td>
<td>Welcome and Open Remarks</td>
</tr>
<tr class="even">
<td>8:10-8:40AM, 2023/08/07 (PDT)</td>
<td>Ed Chi [Google]</td>
<td>Talk 1: LLM Revolution: Implications rom Chatbots and Tool-Use to Reasoning</td>
</tr>
<tr class="odd">
<td>8:40-9:10AM, 2023/08/07 (PDT)</td>
<td>Tania Bedrax-Weiss [Google]</td>
<td>Talk 2: Large-scale AI Model Research at Google Pre-training, Fine-tuning, and Prompt-based Learning</td>
</tr>
<tr class="even">
<td>9:10-9:25AM, 2023/08/07 (PDT)</td>
<td>Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer and Wen-Tau Yih</td>
<td>Paper-1: Retrieval-Augmented Multimodal Language Modeling</td>
</tr>
<tr class="odd">
<td>9:25-9:40AM, 2023/08/07 (PDT)</td>
<td>Silvia Terragni, Modestas Filipavicius, Nghia Khau, Bruna Guedes, André Manso and Roland Mathis</td>
<td>Paper-2: In-Context Learning User Simulators for Task-Oriented Dialog Systems</td>
</tr>
<tr class="even">
<td>9:40-9:55AM, 2023/08/07 (PDT)</td>
<td>Piotr Kluska, Florian Scheidegger, A. Cristano I. Malossi and Enrique S. Quintana-Ortí</td>
<td>Paper-3 : Challenges in post-training quantization of Vision Transformers</td>
</tr>
<tr class="odd">
<td>9:55-10:10AM, 2023/08/07 (PDT)</td>
<td>Haotian Ju, Dongyue Li, Aneesh Sharma and Hongyang Zhang</td>
<td>Paper-4 : Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion</td>
</tr>
<tr class="even">
<td>10:10-10:30AM, 2023/08/07 (PDT)</td>
<td>Coffee Break</td>
<td></td>
</tr>
<tr class="odd">
<td>10:30-11:00AM, 2023/08/07 (PDT)</td>
<td>Shafiq Joty [Salesforce]</td>
<td>Talk 3: NLP Research in the Era of LLMs</td>
</tr>
<tr class="even">
<td>11:00-11:30AM, 2023/08/07 (PDT)</td>
<td>YiKang Shen[IBM]</td>
<td>Talk 4: Modular Large Language Model and Principle-Driven alignment with Minimal Human Supervision</td>
</tr>
<tr class="odd">
<td>11:30-11:40AM, 2023/08/07 (PDT)</td>
<td>Hong Sun, Xue Li, Yinchuan Xu, Youkow Homma, Qi Cao, Min Wu, Jian Jiao and Denis Charles</td>
<td>Paper-5: AutoHint: Automatic Prompt Optimization with Hint Generation</td>
</tr>
<tr class="even">
<td>11:40-11:50AM, 2023/08/07 (PDT)</td>
<td>Zhichao Wang, Mengyu Dai and Keld Lundgaard</td>
<td>Paper-6: Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation</td>
</tr>
<tr class="odd">
<td>11:50-12:00PM, 2023/08/07 (PDT)</td>
<td>Long Hoang Dang, Thao Minh Le, Tu Minh Phuong and Truyen Tran</td>
<td>Paper-7: Compositional Prompting with Successive Decomposition for Multimodal Language Models</td>
</tr>
<tr class="even">
<td>12:00PM-12:10PM, 2023/08/07 (PDT)</td>
<td>Zhen Guo, Yanwei Wang, Peiqi Wang and Shangdi Yu</td>
<td>Paper-8: Dr.&nbsp;LLaMA: Improving Small Language Models on PubMedQA via Generative Data Augmentation</td>
</tr>
<tr class="odd">
<td>12:10-12:20PM, 2023/08/07 (PDT)</td>
<td>Haopeng Zhang, Xiao Liu and Jiawei Zhang</td>
<td>Paper-9 : Extractive Summarization via ChatGPT for Faithful Summary Generation</td>
</tr>
<tr class="even">
<td>12:20-12:30PM, 2023/08/07 (PDT)</td>
<td>Closing Remarks</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="llm-revolution-implications-from-chatbots-and-tool-use-to-reasoning---ed-chi" class="level2">
<h2 class="anchored" data-anchor-id="llm-revolution-implications-from-chatbots-and-tool-use-to-reasoning---ed-chi">LLM Revolution: Implications from Chatbots and Tool-Use to Reasoning - Ed Chi</h2>
<p>Ed Chi from Google gave this great talk.</p>
<section id="functions-that-deep-neural-network-can-learn" class="level3">
<h3 class="anchored" data-anchor-id="functions-that-deep-neural-network-can-learn">2016 - Functions that Deep Neural Network Can Learn</h3>
<ul>
<li>Pixels –&gt; Lion</li>
<li>Audio –&gt; Audio to text</li>
<li>Text –&gt; Text (translation)</li>
<li>Pixels –&gt; Caption</li>
</ul>
</section>
<section id="chatbots" class="level3">
<h3 class="anchored" data-anchor-id="chatbots">Chatbots</h3>
<ul>
<li>Not just transactional</li>
<li>We want chatbots to be contextual</li>
<li>Personalized assistants for everyone</li>
</ul>
</section>
<section id="lambda-bard-brought-to-you-by-eds-team" class="level3">
<h3 class="anchored" data-anchor-id="lambda-bard-brought-to-you-by-eds-team">Lambda –&gt; Bard (Brought to You by Ed’s Team)</h3>
<ul>
<li>They wanted to publish Lambda in the form of Bard, but there were difficulties</li>
</ul>
</section>
<section id="large-language-models-llm" class="level3">
<h3 class="anchored" data-anchor-id="large-language-models-llm">Large Language Models (LLM)</h3>
<ul>
<li>Large knowledge base</li>
<li>What is a plan to read 20 books a year? Reaches into the LLM to come up with a real plan</li>
<li>Genesis of captions –&gt; not too far to be able to generate text</li>
</ul>
</section>
<section id="programming" class="level3">
<h3 class="anchored" data-anchor-id="programming">Programming</h3>
<ul>
<li>Coding is less about coding, more about data</li>
<li>Data Science (DS) is going to be a bigger part of software development</li>
</ul>
</section>
<section id="retrieval-augmentation-leveraging-external-knowledge" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-augmentation-leveraging-external-knowledge">Retrieval Augmentation: Leveraging External Knowledge</h3>
<ul>
<li>Factuality trigger</li>
<li>Open-book Generative QA</li>
<li>RETRO: Retrieval-augmented generative model</li>
<li>Questions:
<ul>
<li>How big does the LLM need to be?</li>
<li>How big does the external knowledge base need to be?</li>
<li>Fruitful Line of Research</li>
</ul></li>
</ul>
</section>
<section id="multi-modality-output-not-just-text-could-be-images" class="level3">
<h3 class="anchored" data-anchor-id="multi-modality-output-not-just-text-could-be-images">Multi-modality output (not just text, could be images)</h3>
<ul>
<li>Image retrieval</li>
<li>Image input –&gt; Generate captions</li>
</ul>
</section>
<section id="humans-and-llms-with-tools" class="level3">
<h3 class="anchored" data-anchor-id="humans-and-llms-with-tools">Humans and LLMs with Tools</h3>
<ul>
<li>Humans + Search –&gt; Superhuman</li>
<li>LLMS + Tools –&gt; Super LLMS</li>
<li>Humans + Super LLM –&gt; Super super humans??</li>
</ul>
</section>
<section id="future-challenges" class="level3">
<h3 class="anchored" data-anchor-id="future-challenges">Future Challenges</h3>
<ul>
<li>Responsibility and Safety</li>
<li>Factuality, Grounding, and Attribution</li>
<li>Human &lt;-&gt; AI Content Loop and Ecosystem</li>
<li>Personalization and User Memory</li>
</ul>
</section>
<section id="keynote" class="level3">
<h3 class="anchored" data-anchor-id="keynote">Keynote</h3>
<ul>
<li>Ed is going to give the keynote tomorrow</li>
<li>You can interrogate a model for why it made a decision or prediction</li>
<li>Area: Self-critique, self-reflection (next year or so)</li>
<li>3-5 year research topics:
<ul>
<li>Hallucinations / Bias in areas where the LLM has not been trained</li>
<li>Relationship between hallucinations and safety</li>
</ul></li>
</ul>
</section>
</section>
<section id="large-scale-ai-model-research-at-google-pre-training-fine-tuning-and-prompt-based-learning" class="level2">
<h2 class="anchored" data-anchor-id="large-scale-ai-model-research-at-google-pre-training-fine-tuning-and-prompt-based-learning">Large-scale AI Model Research at Google Pre-training, Fine-tuning, and Prompt-based Learning</h2>
<p>Tania Bedrax-Weiss from Google gave this talk.</p>
<section id="mixture-of-experts-models" class="level3">
<h3 class="anchored" data-anchor-id="mixture-of-experts-models">Mixture of Experts Models</h3>
<ul>
<li>How to route the question to the right expert, right experts</li>
</ul>
</section>
<section id="conditional-computation" class="level3">
<h3 class="anchored" data-anchor-id="conditional-computation">Conditional Computation</h3>
<ul>
<li>COLT5 Transformer layer</li>
<li>Scales to longer context</li>
<li>Early exit</li>
<li>Per step confidence thresholds</li>
</ul>
</section>
<section id="multi-modal-work" class="level3">
<h3 class="anchored" data-anchor-id="multi-modal-work">Multi-modal Work</h3>
<ul>
<li>Imagen - diffusion model
<ul>
<li><a href="https://imagen.research.google/">Imagen Research Google</a></li>
</ul></li>
<li>Parti - autoregressive model
<ul>
<li><a href="https://sites.research.google/parti/">Parti Research Google</a></li>
</ul></li>
</ul>
</section>
<section id="imagen-technical-details" class="level3">
<h3 class="anchored" data-anchor-id="imagen-technical-details">Imagen: Technical Details</h3>
<ul>
<li>ViT-VQGAN as image tokenizer
<ul>
<li>What’s an image tokenizer? See: https://keras.io/examples/vision/token_learner/</li>
</ul></li>
<li>Autoregressively generate images in a similar way that LLMs generate text</li>
<li>Can generate text reliably - spell words out unlike other models</li>
</ul>
</section>
<section id="pali" class="level3">
<h3 class="anchored" data-anchor-id="pali">Pali</h3>
<ul>
<li>Image to text</li>
<li>State of the art text captioning model</li>
</ul>
</section>
<section id="spotlight" class="level3">
<h3 class="anchored" data-anchor-id="spotlight">Spotlight</h3>
<ul>
<li>Screenshots / user interfaces - understand what are the actions that a user can perform</li>
<li>Execute commands in the user interface</li>
</ul>
</section>
<section id="play-parametrically-condition-layout-generation-using-guidelines" class="level3">
<h3 class="anchored" data-anchor-id="play-parametrically-condition-layout-generation-using-guidelines">PLay: Parametrically Condition Layout Generation Using Guidelines</h3>
<ul>
<li>Fine-tuning</li>
<li>Prompt Tuning
<ul>
<li>Look at this more</li>
</ul></li>
</ul>
</section>
<section id="how-do-you-handle-ambiguity-in-an-answer" class="level3">
<h3 class="anchored" data-anchor-id="how-do-you-handle-ambiguity-in-an-answer">How do you handle ambiguity in an answer?</h3>
<ul>
<li>LLMs are very eager to give an answer</li>
<li>Types
<ul>
<li>Use multiple prompts to get different types of answers. This is my answer. Can you generate other answers?</li>
<li>Diversity objectives</li>
</ul></li>
</ul>
</section>
</section>
<section id="retrieval-augmented-multimodal-language-modeling" class="level2">
<h2 class="anchored" data-anchor-id="retrieval-augmented-multimodal-language-modeling">Retrieval-Augmented Multimodal Language Modeling</h2>
<p>Paper: <a href="https://arxiv.org/abs/2211.12561">https://arxiv.org/abs/2211.12561</a></p>
<p>Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate both text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute for training (&lt;30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities, such as faithful image generation and multimodal in-context learning (e.g., image generation from demonstrations).</p>
<ul>
<li>Develop a retrieval-augmented multimodal model, a first of it’s kind</li>
<li>The generator uses retrieved items for generation too</li>
<li>Retrieval augmented training - helped a lot</li>
</ul>
</section>
<section id="in-context-learning-user-simulators-for-task-oriented-dialog-systems" class="level2">
<h2 class="anchored" data-anchor-id="in-context-learning-user-simulators-for-task-oriented-dialog-systems">In-Context Learning User Simulators for Task-Oriented Dialog Systems</h2>
<ul>
<li>Code: <a href="https://github.com/telepathylabsai/prompt-based-user-simulator">https://github.com/telepathylabsai/prompt-based-user-simulator</a></li>
<li>Paper: <a href="https://arxiv.org/abs/2306.00774">https://arxiv.org/abs/2306.00774</a></li>
</ul>
<p>This paper presents a novel application of large language models in user simulation for task-oriented dialog systems, specifically focusing on an in-context learning approach. By harnessing the power of these models, the proposed approach generates diverse utterances based on user goals and limited dialog examples. Unlike traditional simulators, this method eliminates the need for labor-intensive rule definition or extensive annotated data, making it more efficient and accessible. Additionally, an error analysis of the interaction between the user simulator and dialog system uncovers common mistakes, providing valuable insights into areas that require improvement. Our implementation is available at this https URL.</p>
<ul>
<li>Rule based systems are still more accurate. However they mainly understand happy paths of a dialog system.</li>
<li>These LLM based approaches can explore unexpected behavior of users</li>
</ul>
</section>
<section id="challenges-in-post-training-quantization-of-vision-transformers" class="level2">
<h2 class="anchored" data-anchor-id="challenges-in-post-training-quantization-of-vision-transformers">Challenges in post-training quantization of Vision Transformers</h2>
<p>Paper: <a href="https://research.ibm.com/publications/challenges-in-post-training-quantization-of-vision-transformers">https://research.ibm.com/publications/challenges-in-post-training-quantization-of-vision-transformers</a></p>
<p>Vision Transformers recently showed outstanding performance in computer vision tasks. However, those models are compute and memory intensive that require accelerators with a large amount of memory like NVIDIA A100 graphic processing unit for training and even for inference. Post-training quantization is an appealing compression method, as it does not require retraining the models and labels to tune the model. In this paper, we look in depth at multiple models in terms of size, architecture, and training procedure and provide guidelines on how to quantize the model to an 8-bit integer, both weights and activations. We perform a well-rounded study on the effects of quantization and sensitivity to the quantization error. Moreover, we show that applying mixed-data precision quantization works well for most vision transformer models achieving up to 90% compression ratio within a 2% top-1 accuracy drop. This kind of quantization offers a trade-off between memory, compute, and performance of the models that are deployable with the current software and hardware stack.</p>
<ul>
<li>There’s a difference between Static vs Dynamic Quantization</li>
<li>Larger models are supposed to be easier to quantize, but not the case here</li>
<li>Signal to noise quantization ratio - SNQR</li>
<li>Partial Quantization: Some models that lost accuracy during dynamic quant, regained during 90% quant</li>
</ul>
</section>
<section id="generalization-in-graph-neural-networks-improved-pac-bayesian-bounds-on-graph-diffusion" class="level2">
<h2 class="anchored" data-anchor-id="generalization-in-graph-neural-networks-improved-pac-bayesian-bounds-on-graph-diffusion">Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion</h2>
<p>Paper: <a href="https://proceedings.mlr.press/v206/ju23a/ju23a.pdf">https://proceedings.mlr.press/v206/ju23a/ju23a.pdf</a></p>
<p>Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network’s feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works’ settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with observed generalization gaps of graph neural networks accurately; Optimizing noise stability properties for fine-tuning pretrained graph neural networks also improves the test performance on several graph-level classification tasks.</p>
<ul>
<li>Overfitting if there’s an imbalance between pretraining data and finetuning data size</li>
<li>Generalization gap
<ul>
<li>Not just cross validation loss</li>
<li>More detailed understanding - what networks are causing the overfitting</li>
<li>Generalization gap - measures the gap between training/test losses</li>
</ul></li>
</ul>
</section>
<section id="nlp-research-in-the-era-of-llms---unleashing-the-potential-of-llms-through-task-and-data-engineering" class="level2">
<h2 class="anchored" data-anchor-id="nlp-research-in-the-era-of-llms---unleashing-the-potential-of-llms-through-task-and-data-engineering">NLP Research in the Era of LLMs - Unleashing the Potential of LLMs through Task and Data Engineering</h2>
<p>Shafiq Joty gave this talk: https://raihanjoty.github.io/</p>
<section id="background-data-engineering" class="level3">
<h3 class="anchored" data-anchor-id="background-data-engineering">Background: Data Engineering</h3>
<ul>
<li>Hold the code fixed and invite research to improve the data (Andrew Ng)</li>
</ul>
</section>
<section id="background-rise-of-task-engineering" class="level3">
<h3 class="anchored" data-anchor-id="background-rise-of-task-engineering">Background: Rise of Task Engineering</h3>
<ul>
<li>Multi-task models with task prompts</li>
<li>Trained with many different instructions</li>
<li>Mentions prompt tuning again (soft tokens) ???</li>
</ul>
</section>
<section id="background-task-engineering" class="level3">
<h3 class="anchored" data-anchor-id="background-task-engineering">Background: Task Engineering</h3>
</section>
<section id="llm-lifecycle" class="level3">
<h3 class="anchored" data-anchor-id="llm-lifecycle">LLM Lifecycle</h3>
</section>
<section id="xgen-llm-june-2023" class="level3">
<h3 class="anchored" data-anchor-id="xgen-llm-june-2023"><strong>XGen LLM</strong>: June 2023</h3>
<ul>
<li><a href="https://github.com/salesforce/xgen">GitHub Link</a></li>
<li>Goal is to outperform LLaMA1</li>
</ul>
</section>
<section id="instructed-tuned" class="level3">
<h3 class="anchored" data-anchor-id="instructed-tuned">Instructed tuned</h3>
<ul>
<li>Instructional data: WizardLM. <a href="https://arxiv.org/abs/2304.12244">Paper Link</a></li>
</ul>
</section>
<section id="what-does-wizardlm-do-exactly-in-advancing-the-sota" class="level3">
<h3 class="anchored" data-anchor-id="what-does-wizardlm-do-exactly-in-advancing-the-sota">What does WizardLM do exactly in advancing the SoTA?</h3>
<ul>
<li><p><a href="https://arxiv.org/abs/2304.12244">Details on WizardLM</a></p></li>
<li><p>Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna’s testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at this https URL</p></li>
<li><p><strong>Verify and Edit CoT</strong> - Self-consistency</p></li>
<li><p>Knowledge adapting framework</p></li>
<li><p>Language diversity prompting</p></li>
<li><p>Standard vs Personalized Distillation from LLMs</p></li>
</ul>
</section>
</section>
<section id="modular-large-language-model-and-principle-driven-alignment-with-minimal-human-supervision" class="level2">
<h2 class="anchored" data-anchor-id="modular-large-language-model-and-principle-driven-alignment-with-minimal-human-supervision">Modular Large Language Model and Principle-Driven alignment with Minimal Human Supervision</h2>
<p>Yikang Shen from IBM gave this talk.</p>
<section id="foundation-model-types" class="level3">
<h3 class="anchored" data-anchor-id="foundation-model-types">Foundation model types</h3>
<section id="challenges-of-llm" class="level4">
<h4 class="anchored" data-anchor-id="challenges-of-llm">Challenges of LLM</h4>
<ul>
<li><strong>Efficiency</strong></li>
<li><strong>Extendability</strong></li>
<li><strong>Flexibility</strong></li>
</ul>
</section>
</section>
<section id="moduleformer---learning-modular-llm-from-uncurated-data" class="level3">
<h3 class="anchored" data-anchor-id="moduleformer---learning-modular-llm-from-uncurated-data">ModuleFormer - Learning Modular LLM from Uncurated Data</h3>
<ul>
<li>Previous modular models were based on already labeled data</li>
</ul>
</section>
<section id="mod-squad---designing-a-mixture-of-experts-as-modular-multi-task-learners" class="level3">
<h3 class="anchored" data-anchor-id="mod-squad---designing-a-mixture-of-experts-as-modular-multi-task-learners">Mod-Squad - designing a mixture of experts as modular multi-task learners</h3>
<ul>
<li>Can select the right experts for a task</li>
<li>Experts can share knowledge!?</li>
</ul>
</section>
<section id="dromedary---efficiently-teach-ai-to-follow-a-given-set-of-principles" class="level3">
<h3 class="anchored" data-anchor-id="dromedary---efficiently-teach-ai-to-follow-a-given-set-of-principles">Dromedary - efficiently teach AI to follow a given set of principles</h3>
<ul>
<li><a href="https://github.com/IBM/Dromedary">GitHub Link for Dromedary</a></li>
<li><strong>Principle Engraving</strong> -</li>
<li><strong>Verbose Cloning</strong> - refining the model to produce in-depth and detailed response</li>
<li>300 lines of annotations</li>
<li>Kind of similar to Evol-Instruct/WizardLM to produce annotations to fine-tune a model</li>
</ul>
</section>
</section>
<section id="autohint-automatic-prompt-optimization-with-hint-generation" class="level2">
<h2 class="anchored" data-anchor-id="autohint-automatic-prompt-optimization-with-hint-generation">AutoHint: Automatic Prompt Optimization with Hint Generation</h2>
<p>Paper: <a href="https://arxiv.org/pdf/2307.07415.pdf">https://arxiv.org/pdf/2307.07415.pdf</a></p>
<p>This paper presents AutoHint, a novel framework for automatic prompt engineering and optimization for Large Language Models (LLM). While LLMs have demonstrated remarkable ability in achieving high-quality annotation in various tasks, the key to applying this ability to specific tasks lies in developing high-quality prompts. Thus we propose a framework to inherit the merits of both in-context learning and zero-shot learning by incorporating enriched instructions derived from input-output demonstrations to optimize original prompt. We refer to the enrichment as the Hint and propose a framework to automatically generate the hint from labeled data. More concretely, starting from an initial prompt, our method first instructs a LLM to deduce new hints for selected samples from incorrect predictions, and then summarizes from per-sample hints and adds the results back to the initial prompt to form a new, enriched instruction. The proposed method is evaluated on the BIG-Bench Instruction Induction dataset for both zero-shot and few-short prompts, where experiments demonstrate our method is able to significantly boost accuracy for multiple tasks</p>
</section>
</section>
<section id="causal-inference-workshop-causal-inference-and-machine-learning-in-practice" class="level1">
<h1>Causal Inference Workshop: Causal Inference and Machine Learning in Practice</h1>
<p>The website for this workshop is here: https://causal-machine-learning.github.io/kdd2023-workshop/</p>
<section id="cog-creative-optimality-gap-for-video-advertising" class="level2">
<h2 class="anchored" data-anchor-id="cog-creative-optimality-gap-for-video-advertising">COG: Creative Optimality Gap for Video Advertising</h2>
<p>Raif Rustamov from Amazon gave this invited talk.</p>
<section id="video-ads-motivation" class="level3">
<h3 class="anchored" data-anchor-id="video-ads-motivation">Video ads motivation</h3>
<ul>
<li>How does a particular video affect shopper experience?</li>
</ul>
</section>
<section id="goal" class="level3">
<h3 class="anchored" data-anchor-id="goal">Goal</h3>
<ul>
<li>Driven by explicit hypotheses tied to quantifying value of the video</li>
</ul>
</section>
<section id="approach---creative-optimality-gap-cog" class="level3">
<h3 class="anchored" data-anchor-id="approach---creative-optimality-gap-cog">Approach - Creative Optimality Gap (COG)</h3>
<ul>
<li>If we were to replace the video of class 0 to video of class 1, what would be the improvement in the outcome for the ad?</li>
<li><strong>Uplift or Heterogenous Treatment Effect modeling</strong></li>
</ul>
</section>
<section id="benefits" class="level3">
<h3 class="anchored" data-anchor-id="benefits">Benefits</h3>
<ul>
<li>Differentiated at the level of video features vs.&nbsp;global ATE
<ul>
<li><strong>ATE</strong> - average treatment effect - videos are good</li>
<li><strong>ITE</strong> - individual treatment effect - noisy</li>
<li><strong>HTE</strong> - heterogeneous treatment effect - in the middle, denoising</li>
</ul></li>
<li>Handle cold start ads</li>
</ul>
</section>
<section id="preliminaries" class="level3">
<h3 class="anchored" data-anchor-id="preliminaries">Preliminaries</h3>
<ul>
<li><strong>Treatment indicator (T)</strong></li>
<li><strong>Video features</strong>
<ul>
<li>Computed using e.g.&nbsp;video embeddings</li>
<li>Can contain non</li>
</ul></li>
<li><strong>Ad features</strong>
<ul>
<li>Contains non-video related features like price, product category</li>
<li>Used as confounder/matching variables</li>
</ul></li>
<li><strong>Outcome = Y</strong></li>
</ul>
</section>
<section id="cog-modeling" class="level3">
<h3 class="anchored" data-anchor-id="cog-modeling">COG Modeling</h3>
<ul>
<li><strong>Step 1</strong></li>
<li><strong>Step 2</strong></li>
<li><strong>Step 3</strong> -
<ul>
<li>Used interpretable models in this step, why?</li>
</ul></li>
</ul>
</section>
<section id="cog-modeling-guardrails" class="level3">
<h3 class="anchored" data-anchor-id="cog-modeling-guardrails">COG Modeling: Guardrails</h3>
<section id="bias" class="level4">
<h4 class="anchored" data-anchor-id="bias">Bias</h4>
<ul>
<li>Bias comes from G model, comes from regularization or not enough capacity in the model</li>
<li>Bias is not constant but varies in the Z space</li>
<li>Double ML?</li>
</ul>
</section>
<section id="uncertaintyvariance" class="level4">
<h4 class="anchored" data-anchor-id="uncertaintyvariance">Uncertainty/Variance</h4>
</section>
</section>
<section id="solution" class="level3">
<h3 class="anchored" data-anchor-id="solution">Solution</h3>
<ul>
<li>Conservative COG = lower bound of confidence interval</li>
</ul>
</section>
</section>
<section id="the-value-of-last-mile-delivery-in-online-retail" class="level2">
<h2 class="anchored" data-anchor-id="the-value-of-last-mile-delivery-in-online-retail">The Value of Last-Mile Delivery in Online Retail</h2>
<p>Ruomeng Cui from Emory gave this talk.</p>
<section id="cainiao---chinese-company" class="level3">
<h3 class="anchored" data-anchor-id="cainiao---chinese-company">Cainiao - Chinese Company</h3>
<ul>
<li>Alibaba’s logistics platform</li>
<li>Largest logistics platform in China</li>
<li>If there are differences in preferences, there is an opportunity for optimization</li>
</ul>
</section>
<section id="use-causal-ml-estimating-ite" class="level3">
<h3 class="anchored" data-anchor-id="use-causal-ml-estimating-ite">Use Causal ML: Estimating ITE</h3>
<ul>
<li><strong>Data:</strong> Post-treatment data Q4 2021</li>
</ul>
</section>
<section id="models" class="level3">
<h3 class="anchored" data-anchor-id="models">Models</h3>
<ul>
<li>Partial Linear DML</li>
<li>First-difference DML</li>
<li>Others</li>
</ul>
</section>
<section id="account-for-knapsnack" class="level3">
<h3 class="anchored" data-anchor-id="account-for-knapsnack">Account for Knapsnack</h3>
<ul>
<li>Tau does not capture economic efficiency</li>
<li>Need to account for how much capacity a customer is using. A customer going from 0 to 1 unit sales is much more valuable than a customer going from 19 to 20 units sold because the latter is not using much capacity.</li>
</ul>
</section>
</section>
<section id="leveraging-causal-uplift-modeling-for-budget-constrained-benefits-allocation" class="level2">
<h2 class="anchored" data-anchor-id="leveraging-causal-uplift-modeling-for-budget-constrained-benefits-allocation">Leveraging Causal Uplift Modeling for Budget Constrained Benefits Allocation</h2>
<p>Dmitri Goldenberg from Booking.com gave this talk. It was a very good talk with virtually no words on his slides.</p>
</section>
<section id="ensemble-method-for-estimating-individualized-treatment-effects-kevin-wu-han-han-wu-stanford" class="level2">
<h2 class="anchored" data-anchor-id="ensemble-method-for-estimating-individualized-treatment-effects-kevin-wu-han-han-wu-stanford">Ensemble Method for Estimating Individualized Treatment Effects Kevin Wu Han, Han Wu (Stanford)</h2>
<ul>
<li>Paper: <a href="https://arxiv.org/abs/2202.12445">https://arxiv.org/abs/2202.12445</a></li>
<li>Ensemble methods almost always perform a validation-set model selection based method!</li>
</ul>
</section>
<section id="a-scalable-and-debiased-approach-to-dynamic-pricing-with-causal-machine-learning-and-optimization" class="level2">
<h2 class="anchored" data-anchor-id="a-scalable-and-debiased-approach-to-dynamic-pricing-with-causal-machine-learning-and-optimization">A Scalable and Debiased Approach to Dynamic Pricing with Causal Machine Learning and Optimization</h2>
<ul>
<li>Heard the term double machine learning for the second time which caused me to do to learn what it is.</li>
</ul>
</section>
<section id="an-ipw-based-unbiased-ranking-metric-in-two-sided-markets-keisho-oh-naoki-nishimura-recruit-co-minje-sung-ken-kobayashi-kazuhide-nakata-tokyo-institute-of-technology" class="level2">
<h2 class="anchored" data-anchor-id="an-ipw-based-unbiased-ranking-metric-in-two-sided-markets-keisho-oh-naoki-nishimura-recruit-co-minje-sung-ken-kobayashi-kazuhide-nakata-tokyo-institute-of-technology">An IPW-based Unbiased Ranking Metric in Two-sided Markets Keisho Oh, Naoki Nishimura (Recruit Co), Minje Sung, Ken Kobayashi, Kazuhide Nakata (Tokyo Institute of Technology)</h2>
<p>In two-sided markets like job-matching or dating-apps, need to use an unbiased ranking metric which they propose in their paper.</p>
</section>
<section id="unit-selection-based-on-counterfactual-logic" class="level2">
<h2 class="anchored" data-anchor-id="unit-selection-based-on-counterfactual-logic">Unit Selection Based on Counterfactual Logic</h2>
<p>This was an invited talk by Ang Li about this paper: <a href="https://ftp.cs.ucla.edu/pub/stat_ser/r488.pdf">https://ftp.cs.ucla.edu/pub/stat_ser/r488.pdf</a>.</p>
<p>My main takeaway was dividing a population into a typical A/B test where one group receives a treatment and the other group is the control is too simplistic. There are actually 4 groups we should be concerned about:</p>
<ul>
<li>Complier: Individuals who would respond positively if treated and negatively if not treated.</li>
<li>Always-taker: Individuals who always respond positively no matter whether they are treated or not.</li>
<li>Never-taker: Individuals who always respond negatively no matter whether they are treated or not.</li>
<li>Defier: Individuals who would respond negatively if treated and positively if not treated.</li>
</ul>
<p>Along with a benefit vector that assigns a positive or negative value to each of these 4 groups, we can use this to select the best treatment for each individual.</p>
<p>Ang also used the Pfizer Covid vaccine as a motivating example for why these 4 groups should be accounted for.</p>
</section>
<section id="towards-automating-the-causal-machine-learning-pipeline-vasilis-syrgkanis-stanfordeconml" class="level2">
<h2 class="anchored" data-anchor-id="towards-automating-the-causal-machine-learning-pipeline-vasilis-syrgkanis-stanfordeconml">Towards Automating the Causal Machine Learning Pipeline Vasilis Syrgkanis (Stanford/EconML)</h2>
<ul>
<li>A large variety of causal estimands that arise in complex static and longitudinal data analysis can be automatically de-biased when regularized machine learning algorithms are used to estimate nuisance models</li>
<li>Estimation of the de-biasing term itself can be performed with generic machine learning</li>
<li>Experimental results using neural nets and random forests for automated de-biasing provide examples superior performance to plug-in approaches and to prior automatically debasing approaches based solely on linear models</li>
</ul>


</section>
</section>

 ]]></description>
  <category>Conference</category>
  <category>KDD</category>
  <category>LLM</category>
  <category>Causal Inference</category>
  <guid>https://lawwu.github.io/blog.html/conferences/2023-08-07-kdd2023-day1/index.html</guid>
  <pubDate>Mon, 07 Aug 2023 07:00:00 GMT</pubDate>
</item>
</channel>
</rss>
