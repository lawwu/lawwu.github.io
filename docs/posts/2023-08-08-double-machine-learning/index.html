<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lawrence Wu">
<meta name="dcterms.date" content="2023-08-08">

<title>Lawrence Wu - Double Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-LN4GM4FVCJ"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-LN4GM4FVCJ', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  });
});
</script> 
  


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Lawrence Wu - Double Machine Learning">
<meta property="og:description" content="">
<meta property="og:site-name" content="Lawrence Wu">
<meta name="twitter:title" content="Lawrence Wu - Double Machine Learning">
<meta name="twitter:description" content="">
<meta name="twitter:creator" content="@law_wu">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Lawrence Wu</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../conferences.html" rel="" target="">
 <span class="menu-text">Conferences</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://lawwu.github.io/til/" rel="" target="">
 <span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/lawwu" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://twitter.com/law_wu" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text">Twitter</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:lawrencewu1+blog@gmail.com" rel="" target=""><i class="bi bi-envelope" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../blog.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#double-machine-learning-dml" id="toc-double-machine-learning-dml" class="nav-link active" data-scroll-target="#double-machine-learning-dml">Double Machine Learning (DML)</a>
  <ul class="collapse">
  <li><a href="#concept-and-its-benefits" id="toc-concept-and-its-benefits" class="nav-link" data-scroll-target="#concept-and-its-benefits">Concept and its Benefits</a>
  <ul class="collapse">
  <li><a href="#concept" id="toc-concept" class="nav-link" data-scroll-target="#concept">Concept</a></li>
  <li><a href="#benefits" id="toc-benefits" class="nav-link" data-scroll-target="#benefits">Benefits</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#source" id="toc-source" class="nav-link" data-scroll-target="#source">Source</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#how-is-the-dml-different-and-better-than-the-methods-that-were-used-before-this-method-was-introduced" id="toc-how-is-the-dml-different-and-better-than-the-methods-that-were-used-before-this-method-was-introduced" class="nav-link" data-scroll-target="#how-is-the-dml-different-and-better-than-the-methods-that-were-used-before-this-method-was-introduced">How is the DML different and better than the methods that were used before this method was introduced?</a>
  <ul class="collapse">
  <li><a href="#traditional-methods" id="toc-traditional-methods" class="nav-link" data-scroll-target="#traditional-methods">Traditional Methods</a>
  <ul class="collapse">
  <li><a href="#linear-regression-with-controls" id="toc-linear-regression-with-controls" class="nav-link" data-scroll-target="#linear-regression-with-controls">Linear Regression with Controls</a></li>
  <li><a href="#instrumental-variables-iv" id="toc-instrumental-variables-iv" class="nav-link" data-scroll-target="#instrumental-variables-iv">Instrumental Variables (IV)</a></li>
  <li><a href="#difference-in-differences-did" id="toc-difference-in-differences-did" class="nav-link" data-scroll-target="#difference-in-differences-did">Difference-in-Differences (DiD)</a></li>
  <li><a href="#matching-and-propensity-score-methods" id="toc-matching-and-propensity-score-methods" class="nav-link" data-scroll-target="#matching-and-propensity-score-methods">Matching and Propensity Score Methods</a></li>
  </ul></li>
  <li><a href="#advantages-of-dml" id="toc-advantages-of-dml" class="nav-link" data-scroll-target="#advantages-of-dml">Advantages of DML</a>
  <ul class="collapse">
  <li><a href="#flexibility-with-high-dimensional-controls" id="toc-flexibility-with-high-dimensional-controls" class="nav-link" data-scroll-target="#flexibility-with-high-dimensional-controls">Flexibility with High-Dimensional Controls</a></li>
  <li><a href="#robustness-to-model-misspecification" id="toc-robustness-to-model-misspecification" class="nav-link" data-scroll-target="#robustness-to-model-misspecification">Robustness to Model Misspecification</a></li>
  <li><a href="#leverages-machine-learning" id="toc-leverages-machine-learning" class="nav-link" data-scroll-target="#leverages-machine-learning">Leverages Machine Learning</a></li>
  <li><a href="#no-need-for-strong-instruments-or-parallel-trends" id="toc-no-need-for-strong-instruments-or-parallel-trends" class="nav-link" data-scroll-target="#no-need-for-strong-instruments-or-parallel-trends">No Need for Strong Instruments or Parallel Trends</a></li>
  <li><a href="#interoperability-with-other-techniques" id="toc-interoperability-with-other-techniques" class="nav-link" data-scroll-target="#interoperability-with-other-techniques">Interoperability with Other Techniques</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#can-you-give-me-2-3-examples-of-applying-dml-in-a-human-resources-setting-explain-it-using-the-variable-x-confounders-t-treatment-and-y-outcome" id="toc-can-you-give-me-2-3-examples-of-applying-dml-in-a-human-resources-setting-explain-it-using-the-variable-x-confounders-t-treatment-and-y-outcome" class="nav-link" data-scroll-target="#can-you-give-me-2-3-examples-of-applying-dml-in-a-human-resources-setting-explain-it-using-the-variable-x-confounders-t-treatment-and-y-outcome">Can you give me 2-3 examples of applying DML in a human resources setting? Explain it using the variable X (Confounders), T (Treatment) and Y (Outcome)</a>
  <ul class="collapse">
  <li><a href="#impact-of-training-programs-on-employee-productivity" id="toc-impact-of-training-programs-on-employee-productivity" class="nav-link" data-scroll-target="#impact-of-training-programs-on-employee-productivity">1. Impact of Training Programs on Employee Productivity</a></li>
  <li><a href="#evaluating-the-effects-of-flexible-work-hours-on-employee-satisfaction" id="toc-evaluating-the-effects-of-flexible-work-hours-on-employee-satisfaction" class="nav-link" data-scroll-target="#evaluating-the-effects-of-flexible-work-hours-on-employee-satisfaction">2. Evaluating the Effects of Flexible Work Hours on Employee Satisfaction</a></li>
  <li><a href="#understanding-the-causal-effect-of-health-benefits-on-employee-retention" id="toc-understanding-the-causal-effect-of-health-benefits-on-employee-retention" class="nav-link" data-scroll-target="#understanding-the-causal-effect-of-health-benefits-on-employee-retention">3. Understanding the Causal Effect of Health Benefits on Employee Retention</a></li>
  </ul></li>
  <li><a href="#can-you-give-me-some-code-in-python-to-implement-the-first-use-case-with-clear-comments-too" id="toc-can-you-give-me-some-code-in-python-to-implement-the-first-use-case-with-clear-comments-too" class="nav-link" data-scroll-target="#can-you-give-me-some-code-in-python-to-implement-the-first-use-case-with-clear-comments-too">Can you give me some code in Python to implement the first use case with clear comments too?</a></li>
  <li><a href="#can-you-implement-the-above-use-case-in-python-without-econml-for-pedagogical-purposes" id="toc-can-you-implement-the-above-use-case-in-python-without-econml-for-pedagogical-purposes" class="nav-link" data-scroll-target="#can-you-implement-the-above-use-case-in-python-without-econml-for-pedagogical-purposes">Can you implement the above use case in Python without econml for pedagogical purposes?</a></li>
  <li><a href="#do-the-ml-models-at-each-of-those-steps-need-to-be-the-same" id="toc-do-the-ml-models-at-each-of-those-steps-need-to-be-the-same" class="nav-link" data-scroll-target="#do-the-ml-models-at-each-of-those-steps-need-to-be-the-same">Do the ML models at each of those steps need to be the same?</a>
  <ul class="collapse">
  <li><a href="#heres-why-you-might-choose-different-models" id="toc-heres-why-you-might-choose-different-models" class="nav-link" data-scroll-target="#heres-why-you-might-choose-different-models">Here’s why you might choose different models:</a>
  <ul class="collapse">
  <li><a href="#different-natures-of-relationships" id="toc-different-natures-of-relationships" class="nav-link" data-scroll-target="#different-natures-of-relationships">Different Natures of Relationships</a></li>
  <li><a href="#model-performance-considerations" id="toc-model-performance-considerations" class="nav-link" data-scroll-target="#model-performance-considerations">Model Performance Considerations</a></li>
  <li><a href="#computational-considerations" id="toc-computational-considerations" class="nav-link" data-scroll-target="#computational-considerations">Computational Considerations</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#how-do-you-handle-training-and-test-data-splits-in-this-method" id="toc-how-do-you-handle-training-and-test-data-splits-in-this-method" class="nav-link" data-scroll-target="#how-do-you-handle-training-and-test-data-splits-in-this-method">How do you handle training and test data splits in this method?</a>
  <ul class="collapse">
  <li><a href="#heres-how-you-can-handle-training-and-test-data-splits-in-dml" id="toc-heres-how-you-can-handle-training-and-test-data-splits-in-dml" class="nav-link" data-scroll-target="#heres-how-you-can-handle-training-and-test-data-splits-in-dml">Here’s how you can handle training and test data splits in DML:</a>
  <ul class="collapse">
  <li><a href="#cross-fitting" id="toc-cross-fitting" class="nav-link" data-scroll-target="#cross-fitting">1. Cross-Fitting:</a></li>
  <li><a href="#train-test-split" id="toc-train-test-split" class="nav-link" data-scroll-target="#train-test-split">2. Train-Test Split:</a></li>
  </ul></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes">Notes:</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Double Machine Learning</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Causal Inference</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Lawrence Wu </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 8, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>At KDD2023 I was introduced to the term “Double Machine Learning” (DML). I was surprised I had never heard the term before so here are some notes on this important method in causal machine learning from a conversation with GPT-4. I am very impressed with GPT-4’s ability to help explain complex concepts especially in areas where there there are libraries and code involved.</p>
<p>The first 5 results from Google were not exactly what I was looking for:</p>
<ul>
<li><a href="https://towardsdatascience.com/double-machine-learning-for-causal-inference-78e0c6111f9d" class="uri">https://towardsdatascience.com/double-machine-learning-for-causal-inference-78e0c6111f9d</a> - I tend to avoid Medium articles because of the paywall</li>
<li><a href="https://econml.azurewebsites.net/spec/estimation/dml.html" class="uri">https://econml.azurewebsites.net/spec/estimation/dml.html</a> - not enough detail explaining the concept of DML</li>
<li><a href="https://docs.doubleml.org/stable/guide/basics.html" class="uri">https://docs.doubleml.org/stable/guide/basics.html</a> - It was difficult for me to follow this tutorial</li>
<li><a href="https://dm13450.github.io/2021/05/28/Double-ML.html" class="uri">https://dm13450.github.io/2021/05/28/Double-ML.html</a> - This was actually the best tutorial I read however the code is in R and I preferred a Python example</li>
<li><a href="https://arxiv.org/abs/1608.00060" class="uri">https://arxiv.org/abs/1608.00060</a> - Original paper proposing the method however I wanted something a tad more accessible</li>
</ul>
<p>Hence the need to have a conversation with GPT-4.</p>
<section id="double-machine-learning-dml" class="level1">
<h1>Double Machine Learning (DML)</h1>
<p>Double machine learning (DML) is a methodology developed to estimate causal effects or treatment effects in the presence of a high-dimensional set of controls. This technique is particularly useful in scenarios where there are a lot of control variables, and we want to adjust for confounding effects to accurately determine the impact of a particular treatment or variable of interest.</p>
<section id="concept-and-its-benefits" class="level2">
<h2 class="anchored" data-anchor-id="concept-and-its-benefits">Concept and its Benefits</h2>
<section id="concept" class="level3">
<h3 class="anchored" data-anchor-id="concept">Concept</h3>
<p><strong>Problem</strong>: Suppose you’re interested in estimating the causal effect of a treatment variable ( T ) on an outcome ( Y ), while controlling for a high-dimensional set of covariates ( X ). The relationship could be something like ( Y = + T + g(X) + ), where ( g(X) ) is a potentially complex and high-dimensional function, and ( ) is an error term.</p>
<p><strong>High-Dimensionality Issue</strong>: Directly estimating ( ) is challenging because of the high dimensionality of ( g(X) ). Traditional methods might suffer from overfitting or might not capture the nuanced relationship of ( X ) with ( Y ).</p>
<p><strong>Solution via DML</strong>: DML breaks down the problem into two stages:</p>
<ol type="1">
<li><strong>First Stage</strong>: Use machine learning methods to predict ( Y ) and ( T ) based on ( X ). Obtain the residuals from these predictions.</li>
<li><strong>Second Stage</strong>: Regress the residuals of ( Y ) on the residuals of ( T ) to estimate ( ). This step isolates the effect of ( T ) on ( Y ), controlling for ( X ), without directly modeling ( g(X) ).</li>
</ol>
</section>
<section id="benefits" class="level3">
<h3 class="anchored" data-anchor-id="benefits">Benefits</h3>
<ul>
<li><strong>Flexibility</strong>: DML doesn’t make strong assumptions about the functional forms of relationships, allowing for the use of flexible ML techniques.</li>
<li><strong>Robustness</strong>: By working with residuals, DML reduces concerns about model misspecification. Even if the first-stage models aren’t perfect, as long as they’re reasonably good, the second-stage estimate of ( ) can still be consistent and accurate.</li>
<li><strong>High-Dimensional Controls</strong>: DML is particularly suited for cases with many control variables. This is common in modern datasets where one might have a vast number of potential confounders.</li>
<li><strong>Interpretability</strong>: The final result, ( ), provides a clear and interpretable estimate of the causal effect of ( T ) on ( Y ).</li>
</ul>
</section>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>Imagine you want to estimate the effect of a training program (treatment ( T )) on employee performance (outcome ( Y )), controlling for a range of employee attributes ( X ) (like age, experience, past performance metrics, personality scores, and so on). Using DML, you would first predict performance and program participation based on all these controls using ML models. Then, you’d isolate the unexplained variations (residuals) of performance and program participation and estimate the effect of the training program using these residuals.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Double Machine Learning offers a robust and flexible way to estimate causal effects in high-dimensional settings, combining the strengths of traditional econometric methods with modern machine learning techniques.</p>
<section id="source" class="level3">
<h3 class="anchored" data-anchor-id="source">Source</h3>
<p>Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., &amp; Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1), C1-C68.</p>
</section>
</section>
</section>
<section id="how-is-the-dml-different-and-better-than-the-methods-that-were-used-before-this-method-was-introduced" class="level1">
<h1>How is the DML different and better than the methods that were used before this method was introduced?</h1>
<p>Double Machine Learning (DML) was introduced to address some of the limitations and challenges of traditional methods in econometrics, especially in settings with high-dimensional controls. Here’s a comparison of DML with conventional approaches and an overview of its advantages:</p>
<section id="traditional-methods" class="level2">
<h2 class="anchored" data-anchor-id="traditional-methods">Traditional Methods</h2>
<section id="linear-regression-with-controls" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression-with-controls">Linear Regression with Controls</h3>
<p>Traditionally, if researchers wanted to control for confounding variables, they would include these controls in a linear regression. But, with high-dimensional controls, this approach faces issues like overfitting or multicollinearity.</p>
</section>
<section id="instrumental-variables-iv" class="level3">
<h3 class="anchored" data-anchor-id="instrumental-variables-iv">Instrumental Variables (IV)</h3>
<p>IV methods are used when the treatment variable is endogenous (correlated with the error term). IV requires the identification and use of an instrument that affects the treatment but has no direct effect on the outcome. Finding such instruments can be challenging.</p>
</section>
<section id="difference-in-differences-did" class="level3">
<h3 class="anchored" data-anchor-id="difference-in-differences-did">Difference-in-Differences (DiD)</h3>
<p>DiD estimates causal effects by looking at changes in outcomes over time between a group that receives the treatment and a group that doesn’t. This method assumes that the two groups would have followed parallel trends in the absence of treatment, an assumption that’s sometimes hard to validate.</p>
</section>
<section id="matching-and-propensity-score-methods" class="level3">
<h3 class="anchored" data-anchor-id="matching-and-propensity-score-methods">Matching and Propensity Score Methods</h3>
<p>These involve pairing treated and untreated subjects that have similar observable characteristics. It’s often difficult to balance all covariates, especially in high dimensions, and unobserved confounding remains a challenge.</p>
</section>
</section>
<section id="advantages-of-dml" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-dml">Advantages of DML</h2>
<section id="flexibility-with-high-dimensional-controls" class="level3">
<h3 class="anchored" data-anchor-id="flexibility-with-high-dimensional-controls">Flexibility with High-Dimensional Controls</h3>
<p>Traditional methods struggle with many control variables due to overfitting, multicollinearity, or the curse of dimensionality. DML can handle high-dimensional controls efficiently by leveraging machine learning techniques.</p>
</section>
<section id="robustness-to-model-misspecification" class="level3">
<h3 class="anchored" data-anchor-id="robustness-to-model-misspecification">Robustness to Model Misspecification</h3>
<p>DML’s two-stage process (predicting using ML and then regressing the residuals) ensures that even if the first-stage predictions aren’t perfect, the causal estimate can still be consistent and accurate.</p>
</section>
<section id="leverages-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="leverages-machine-learning">Leverages Machine Learning</h3>
<p>By integrating machine learning into causal inference, DML can use complex, non-linear models to control for confounding, something that’s challenging with traditional linear methods.</p>
</section>
<section id="no-need-for-strong-instruments-or-parallel-trends" class="level3">
<h3 class="anchored" data-anchor-id="no-need-for-strong-instruments-or-parallel-trends">No Need for Strong Instruments or Parallel Trends</h3>
<p>Unlike IV or DiD, DML doesn’t rely on finding strong instruments or making assumptions about parallel trends, making it applicable in a broader range of scenarios.</p>
</section>
<section id="interoperability-with-other-techniques" class="level3">
<h3 class="anchored" data-anchor-id="interoperability-with-other-techniques">Interoperability with Other Techniques</h3>
<p>DML can be combined with other methods like instrumental variables (e.g., in the form of Double/Debiased Machine Learning with Instruments) to tackle more complex causal questions.</p>
<p>In essence, while traditional econometric methods have their value and are optimal in certain settings, DML provides a flexible and robust framework for causal inference in modern datasets with many covariates. It bridges the gap between the strengths of classical econometric methods and the power of machine learning.</p>
</section>
</section>
</section>
<section id="can-you-give-me-2-3-examples-of-applying-dml-in-a-human-resources-setting-explain-it-using-the-variable-x-confounders-t-treatment-and-y-outcome" class="level1">
<h1>Can you give me 2-3 examples of applying DML in a human resources setting? Explain it using the variable X (Confounders), T (Treatment) and Y (Outcome)</h1>
<p>Double Machine Learning (DML) can be an incredibly useful tool in the Human Resources (HR) context, particularly when one wants to infer causal relationships while adjusting for many potential confounding variables. Here are three examples applying DML in an HR setting:</p>
<section id="impact-of-training-programs-on-employee-productivity" class="level2">
<h2 class="anchored" data-anchor-id="impact-of-training-programs-on-employee-productivity">1. Impact of Training Programs on Employee Productivity</h2>
<ul>
<li><strong>T (Treatment):</strong> Participation in a specialized training program.</li>
<li><strong>Y (Outcome):</strong> Employee productivity (could be measured as tasks completed per day, quality scores, or any other relevant metric).</li>
<li><strong>X (Confounders):</strong>
<ul>
<li>Age of the employee.</li>
<li>Years of experience in the field.</li>
<li>Educational background.</li>
<li>Previous training programs attended.</li>
<li>Personality test scores.</li>
<li>Prior productivity metrics.</li>
</ul></li>
<li><strong>DML Application:</strong> HR wants to understand if the new training program truly increases productivity. By employing DML, they can control for a vast array of confounders (like age, experience, prior training, etc.) to ascertain the genuine causal effect of the training on productivity.</li>
</ul>
</section>
<section id="evaluating-the-effects-of-flexible-work-hours-on-employee-satisfaction" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-the-effects-of-flexible-work-hours-on-employee-satisfaction">2. Evaluating the Effects of Flexible Work Hours on Employee Satisfaction</h2>
<ul>
<li><strong>T (Treatment):</strong> Implementation of flexible work hours (e.g., 4-day work week, flexi-timings).</li>
<li><strong>Y (Outcome):</strong> Employee satisfaction scores (from regular employee surveys or feedback forms).</li>
<li><strong>X (Confounders):</strong>
<ul>
<li>Age of the employee.</li>
<li>Department or team (e.g., sales, IT, management).</li>
<li>Commute time to the office.</li>
<li>Number of dependents (like children).</li>
<li>Tenure at the company.</li>
<li>Previous satisfaction scores.</li>
</ul></li>
<li><strong>DML Application:</strong> HR is considering a policy change to offer more flexible working hours. They implement a pilot program in a few departments and then wish to measure its effect on employee satisfaction. With many potential confounders, DML can help isolate the true impact of the flexible hours policy on satisfaction.</li>
</ul>
</section>
<section id="understanding-the-causal-effect-of-health-benefits-on-employee-retention" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-causal-effect-of-health-benefits-on-employee-retention">3. Understanding the Causal Effect of Health Benefits on Employee Retention</h2>
<ul>
<li><strong>T (Treatment):</strong> Provision of a comprehensive health benefits package.</li>
<li><strong>Y (Outcome):</strong> Employee retention rate (measured as the number of years an employee stays with the company or a binary outcome of whether they stayed for a given period).</li>
<li><strong>X (Confounders):</strong>
<ul>
<li>Age of the employee.</li>
<li>Salary and other financial incentives.</li>
<li>Department or team.</li>
<li>Job role and responsibilities.</li>
<li>Previous job-switching history.</li>
<li>Size of the family or number of dependents.</li>
</ul></li>
<li><strong>DML Application:</strong> An organization introduces a new health benefits package, theorizing that it would lead to higher retention rates. However, before scaling it company-wide, they want to understand its true causal effect. Using DML, they can adjust for various confounding factors like salary, job role, and personal circumstances to determine the genuine influence of the health benefits on retention.</li>
</ul>
<p>In each of these scenarios, the objective is to tease out the causal relationship between a specific HR intervention (T) and an outcome of interest (Y), all while accounting for a myriad of confounding variables (X) that might distort this relationship. By integrating DML techniques, HR departments can make more informed decisions based on robust causal inferences.</p>
</section>
</section>
<section id="can-you-give-me-some-code-in-python-to-implement-the-first-use-case-with-clear-comments-too" class="level1">
<h1>Can you give me some code in Python to implement the first use case with clear comments too?</h1>
<p>Certainly! Below is a simplified Python code that applies Double Machine Learning (DML) to the first use case: evaluating the impact of a specialized training program on employee productivity. The code assumes you have data with relevant columns for treatment, outcome, and confounders.</p>
<p>For this example, I’m using the EconML library, which offers a robust implementation of DML among other causal inference tools.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Required Libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> econml.dml <span class="im">import</span> LinearDML</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load your dataset</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># df = pd.read_csv('your_data_file.csv')</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated dataset for the sake of this example</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Confounders</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>age <span class="op">=</span> np.random.randint(<span class="dv">20</span>, <span class="dv">60</span>, size<span class="op">=</span>n)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>years_of_experience <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">40</span>, size<span class="op">=</span>n)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>prior_productivity <span class="op">=</span> np.random.normal(<span class="dv">50</span>, <span class="dv">10</span>, n)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Treatment: whether they attended the training or not (1 for yes, 0 for no)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>treatment <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="fl">0.5</span>, n)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Outcome: productivity influenced by treatment, age, experience and some noise</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>productivity <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> treatment <span class="op">+</span> <span class="fl">0.05</span> <span class="op">*</span> age <span class="op">+</span> <span class="fl">0.3</span> <span class="op">*</span> years_of_experience <span class="op">+</span> </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>                <span class="fl">0.5</span> <span class="op">*</span> prior_productivity <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">5</span>, n))</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">'age'</span>: age, <span class="st">'experience'</span>: years_of_experience, <span class="st">'prior_productivity'</span>: prior_productivity,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">'treatment'</span>: treatment, <span class="st">'productivity'</span>: productivity</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the treatment, outcome, and confounders</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> df[<span class="st">'treatment'</span>]</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[<span class="st">'productivity'</span>]</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">'age'</span>, <span class="st">'experience'</span>, <span class="st">'prior_productivity'</span>]]</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the machine learning model to use; here we're using linear regression for simplicity</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>ml_model <span class="op">=</span> LinearRegression()</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the DML model</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>dml_model <span class="op">=</span> LinearDML(model_y<span class="op">=</span>ml_model, model_t<span class="op">=</span>ml_model)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>dml_model.fit(Y, T, X<span class="op">=</span>X)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Retrieve the causal effect (average treatment effect or ATE)</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>ate <span class="op">=</span> dml_model.ate()</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average Treatment Effect of the Training Program on Productivity: </span><span class="sc">{</span>ate<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here’s what the code does:</p>
<ol type="1">
<li>Imports required libraries.</li>
<li>Either loads or simulates a dataset with treatment, outcome, and confounders.</li>
<li>Defines the treatment, outcome, and confounders from the dataset.</li>
<li>Sets up the machine learning model to use. In this simple example, it’s linear regression, but more complex models like random forests or gradient boosting machines can be used.</li>
<li>Initializes and fits the DML model.</li>
<li>Retrieves the average treatment effect (ATE) of the training on productivity.</li>
</ol>
</section>
<section id="can-you-implement-the-above-use-case-in-python-without-econml-for-pedagogical-purposes" class="level1">
<h1>Can you implement the above use case in Python without econml for pedagogical purposes?</h1>
<p>Absolutely. Implementing Double Machine Learning (DML) without relying on specialized libraries will require a bit more manual work, but it’s quite educational.</p>
<p>Here’s a basic implementation of DML for the first use case:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate the dataset</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Confounders</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>age <span class="op">=</span> np.random.randint(<span class="dv">20</span>, <span class="dv">60</span>, size<span class="op">=</span>n)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>years_of_experience <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">40</span>, size<span class="op">=</span>n)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>prior_productivity <span class="op">=</span> np.random.normal(<span class="dv">50</span>, <span class="dv">10</span>, n)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Treatment: whether they attended the training or not (1 for yes, 0 for no)</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>treatment <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="fl">0.5</span>, n)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Outcome: productivity influenced by treatment, age, experience, and some noise</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>productivity <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> treatment <span class="op">+</span> <span class="fl">0.05</span> <span class="op">*</span> age <span class="op">+</span> <span class="fl">0.3</span> <span class="op">*</span> years_of_experience <span class="op">+</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>                <span class="fl">0.5</span> <span class="op">*</span> prior_productivity <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">5</span>, n))</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">'age'</span>: age, <span class="st">'experience'</span>: years_of_experience, <span class="st">'prior_productivity'</span>: prior_productivity,</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">'treatment'</span>: treatment, <span class="st">'productivity'</span>: productivity</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the treatment, outcome, and confounders</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> df[<span class="st">'treatment'</span>].values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> df[<span class="st">'productivity'</span>]</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">'age'</span>, <span class="st">'experience'</span>, <span class="st">'prior_productivity'</span>]]</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Partial out the effect of X on T and Y using Linear Regression</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>model_t <span class="op">=</span> LinearRegression().fit(X, T)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>residual_T <span class="op">=</span> T <span class="op">-</span> model_t.predict(X)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>model_y <span class="op">=</span> LinearRegression().fit(X, Y)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>residual_Y <span class="op">=</span> Y <span class="op">-</span> model_y.predict(X)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Regress the residuals of Y on the residuals of T to obtain the treatment effect</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>ate_model <span class="op">=</span> LinearRegression().fit(residual_T, residual_Y)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>ate <span class="op">=</span> ate_model.coef_[<span class="dv">0</span>]</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average Treatment Effect of the Training Program on Productivity: </span><span class="sc">{</span>ate<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here’s the basic workflow:</p>
<ol type="1">
<li><p><strong>Partial Out the Effects of Confounders (X)</strong>: We first “partial out” the effects of the confounders ( X ) on both the treatment ( T ) and outcome ( Y ) using linear regression. This leaves us with residuals for ( T ) and ( Y ) that represent the portions of ( T ) and ( Y ) that are not explained by ( X ).</p></li>
<li><p><strong>Regress the Residuals</strong>: We then regress the residual of ( Y ) on the residual of ( T ). The coefficient of this regression is our estimate of the average treatment effect.</p></li>
</ol>
<p>This method uses the concept that by “controlling” for ( X ) (through partialling out its effects), any remaining relationship between ( T ) and ( Y ) should be causal, under the assumption that there are no unobserved confounders.</p>
<p>This is a basic pedagogical introduction to DML. In practice, the methodology can get more intricate, especially when leveraging more complex models, handling various assumptions, and ensuring the robustness of the results.</p>
</section>
<section id="do-the-ml-models-at-each-of-those-steps-need-to-be-the-same" class="level1">
<h1>Do the ML models at each of those steps need to be the same?</h1>
<p>No, the machine learning models used to partial out the effects of the confounders on the treatment ( T ) and the outcome ( Y ) do not need to be the same in Double Machine Learning (DML). In fact, one of the strengths of DML is its flexibility in allowing different models for different parts of the process.</p>
<section id="heres-why-you-might-choose-different-models" class="level2">
<h2 class="anchored" data-anchor-id="heres-why-you-might-choose-different-models">Here’s why you might choose different models:</h2>
<section id="different-natures-of-relationships" class="level3">
<h3 class="anchored" data-anchor-id="different-natures-of-relationships">Different Natures of Relationships</h3>
<p>The relationship between the confounders and the treatment might be different from the relationship between the confounders and the outcome. For instance, a non-linear model like a decision tree or random forest might capture the relationship between the confounders and the treatment best, while a linear model might be most appropriate for the relationship between the confounders and the outcome.</p>
</section>
<section id="model-performance-considerations" class="level3">
<h3 class="anchored" data-anchor-id="model-performance-considerations">Model Performance Considerations</h3>
<p>In some settings, certain machine learning models might perform better (in terms of predictive accuracy) for the treatment assignment, while others might be better suited for modeling the outcome.</p>
</section>
<section id="computational-considerations" class="level3">
<h3 class="anchored" data-anchor-id="computational-considerations">Computational Considerations</h3>
<p>In some scenarios, especially with large datasets, computational efficiency might dictate the choice of models. Training a complex model might be computationally prohibitive for one task but feasible for another, depending on the data’s dimensionality and the relationships present.</p>
<p>While DML allows for such flexibility, it’s important to remember the foundational assumption: both models (for ( T ) and ( Y )) should be sufficiently rich to capture the true relationships in the data. If the models are misspecified or too simplistic, it can introduce bias in the estimated treatment effects.</p>
<p>In practice, using cross-validation or other model selection techniques can help determine which models are best suited for each step in the DML process.</p>
</section>
</section>
</section>
<section id="how-do-you-handle-training-and-test-data-splits-in-this-method" class="level1">
<h1>How do you handle training and test data splits in this method?</h1>
<p>Handling training and test data splits in Double Machine Learning (DML) is crucial to ensure the validity and robustness of the causal effect estimates. In DML, cross-fitting or cross-validation plays a central role in avoiding overfitting and ensuring that nuisance estimates do not inadvertently leak information about the treatment effect.</p>
<section id="heres-how-you-can-handle-training-and-test-data-splits-in-dml" class="level2">
<h2 class="anchored" data-anchor-id="heres-how-you-can-handle-training-and-test-data-splits-in-dml">Here’s how you can handle training and test data splits in DML:</h2>
<section id="cross-fitting" class="level3">
<h3 class="anchored" data-anchor-id="cross-fitting">1. Cross-Fitting:</h3>
<p>Cross-fitting involves splitting the dataset into ( K ) folds (similar to ( K )-fold cross-validation in standard machine learning). For each fold, the following steps are taken:</p>
<ul>
<li><strong>Training on ( K-1 ) folds:</strong> Train the ML models for the treatment and outcome on ( K-1 ) folds.</li>
<li><strong>Predicting on the left-out fold:</strong> Use the trained models to predict the treatment and outcome for the left-out fold, producing residuals.</li>
<li><strong>Repeat:</strong> Perform the same operation ( K ) times, each time leaving out a different fold.</li>
<li><strong>Finally, with the residuals produced from all the folds, you can estimate the treatment effect.</strong></li>
</ul>
</section>
<section id="train-test-split" class="level3">
<h3 class="anchored" data-anchor-id="train-test-split">2. Train-Test Split:</h3>
<p>A simpler approach involves just splitting the data into a training set and a test set.</p>
<ul>
<li><strong>Training on the training set:</strong> Train the ML models for the treatment and outcome on the training set.</li>
<li><strong>Predicting on the test set:</strong> Use the trained models to predict the treatment and outcome for the test set, producing residuals.</li>
<li><strong>Estimating the treatment effect:</strong> With the residuals from the test set, estimate the treatment effect.</li>
</ul>
<p>This method is less robust than cross-fitting but can be computationally less intensive.</p>
</section>
</section>
<section id="notes" class="level2">
<h2 class="anchored" data-anchor-id="notes">Notes:</h2>
<ul>
<li><strong>Overfitting Concerns:</strong> Without proper cross-fitting or a train-test split, DML can overfit. This is because the ML models for treatment and outcome might fit the noise in the data, leading to biased nuisance estimates and, subsequently, biased treatment effect estimates.</li>
<li><strong>Cross-Fitting is Preferred:</strong> Cross-fitting is typically preferred in DML because it helps ensure that the estimated causal effects are not driven by overfitting to the idiosyncrasies in the data. It also provides a more robust estimate of the treatment effect across different subsets of the data.</li>
<li><strong>Final Model Assessment:</strong> After obtaining the causal effect estimates using DML, it’s a good practice to assess the performance of the final model on a completely separate validation set, if available. This ensures that the DML procedure generalizes well to unseen data.</li>
</ul>
<p>To summarize, handling data splits in DML is integral to its methodology. Properly performed, it ensures that the estimated causal effects are both valid and generalizable.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="lawwu/blog_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center"><div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>